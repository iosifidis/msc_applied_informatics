# **Ταξινόμηση (Classification)**

---

## **1. Εισαγωγή & Πλαίσιο Προβλήματος (Slides 1-3)**

*   **Στόχος:** Να κατανοήσουμε τη διαδικασία της ταξινόμησης (classification) στην μηχανική μάθηση.
*   **Σενάριο:** Λαμβάνουμε ένα αρχείο Excel με δεδομένα από ασθενείς με όγκους στο μαστό.
    *   Κάθε γραμμή (παρατήρηση) αντιστοιχεί σε έναν ασθενή (~600 σύνολο).
    *   Κάθε παρατήρηση έχει 30 χαρακτηριστικά (features) - ιατρικοί δείκτες σχετικοί με τη φυσιολογία του όγκου.
    *   Για κάθε παρατήρηση, γνωρίζουμε την κατάσταση: **Κακοήθης (Malignant - καρκινικός)** ή **Καλοήθης (Benign - μη καρκινικός)**. Αυτή είναι η *μεταβλητή στόχος* (target variable) που θέλουμε να προβλέψουμε.
*   **Η Εργασία μας (Job):** Να δημιουργήσουμε ένα μοντέλο υποστήριξης απόφασης (decision support model).
    *   Είσοδος (Input): Οι 30 τιμές των δεικτών.
    *   Έξοδος (Output): Η πρόβλεψη για την κατάσταση του όγκου (Κακοήθης ή Καλοήθης).
*   **Σημαντικοί Περιορισμοί (Constraints):**
    *   Το μοντέλο πρέπει να ταξινομεί σωστά **τουλάχιστον το 90%** των κακοήθων όγκων (πολύ σημαντικό να μην χάσουμε καρκίνους!).
    *   Το μοντέλο πρέπει να ταξινομεί σωστά **τουλάχιστον το 80%** των καλοήθων όγκων.
*   **Προκλήσεις (Challenges):**
    *   Λαμβάνουμε τα δεδομένα "ως έχουν" (as is), χωρίς δυνατότητα παρέμβασης στη συλλογή ή αίτησης για επιπλέον πληροφορίες.
    *   Τα δεδομένα μπορεί να προέρχονται από πεδίο που δεν γνωρίζουμε (π.χ. ιατρική).
    *   Δεν μπορούμε εύκολα να κρίνουμε αν οι τιμές είναι λογικές ή τυχαίες.
    *   Δεν ξέρουμε εκ των προτέρων αν οι περιορισμοί (90%/80%) είναι εφικτοί με τα δεδομένα που έχουμε.

---

## **2. Μεθοδολογία Αντιμετώπισης (Slides 4-6)**

Προτείνεται μια δομημένη προσέγγιση σε δύο μέρη:

*   **Μέρος 1: Κατανόηση της Κατάστασης**
    1.  **Προεπεξεργασία Δεδομένων (Data Preprocessing):** Καθαρισμός, μορφοποίηση, χειρισμός ελλιπών τιμών κ.λπ. (Βλ. Slide 5 για τυπικά βήματα: profiling, cleansing, reduction, transformation, enrichment, validation).
    2.  **Οπτικοποίηση (Visualization):** Δημιουργία γραφημάτων για να κατανοήσουμε τα δεδομένα.
    3.  **Ορισμός Προβλήματος (Problem Definition):** Καθαρή διατύπωση του τι θέλουμε να πετύχουμε.
    4.  **Επιλογή Μοντέλου (Model Selection):** Επιλογή πιθανών αλγορίθμων ταξινόμησης.
*   **Μέρος 2: Έλεγχος και Αξιολόγηση**
    1.  **Πειραματικός Σχεδιασμός (Experimental Setup):** Πώς θα εκπαιδεύσουμε και θα δοκιμάσουμε τα μοντέλα (π.χ. διαχωρισμός train/test set, cross-validation).
    2.  **Ερμηνεία Αποτελεσμάτων (Results Interpretation):** Ανάλυση της απόδοσης των μοντέλων με βάση κατάλληλες μετρικές.
    3.  **Ενημερώσεις (Updates if necessary):** Βελτιώσεις στο μοντέλο ή την προεπεξεργασία αν τα αποτελέσματα δεν είναι ικανοποιητικά.
    4.  **Συμπεράσματα (Conclusions):** Τελική αξιολόγηση και επιλογή του καλύτερου μοντέλου.

---

## **3. Εξερεύνηση Δεδομένων (Data Exploration) (Slides 7-47)**

*   **Ελλιπείς Τιμές (Missing Values - Slide 7):**
    *   Ελέγχουμε αν λείπουν τιμές από κάποιο χαρακτηριστικό.
    *   **Καλά νέα:** Σε αυτό το dataset, δεν υπάρχουν ελλιπείς τιμές.
*   **Εύρος Τιμών (Range Check - Slide 8):**
    *   Ελέγχουμε τις ελάχιστες και μέγιστες τιμές για κάθε χαρακτηριστικό.
    *   **Κακά νέα:** Το εύρος τιμών διαφέρει *σημαντικά* μεταξύ των χαρακτηριστικών (π.χ. area_mean έχει πολύ μεγαλύτερες τιμές από smoothness_mean).
    *   **Συνέπεια:** Αυτό υποδηλώνει ότι πιθανότατα θα χρειαστεί **κανονικοποίηση (normalization)** ή **τυποποίηση (standardization)** των δεδομένων πριν την εκπαίδευση κάποιων μοντέλων (όπως KNN, SVM).
*   **Κατανομή Κλάσεων (Class Distribution - Slide 10):**
    *   Πόσα δείγματα έχουμε από κάθε κλάση (Καλοήθης vs. Κακοήθης);
    *   Έχουμε 357 Καλοήθεις και 212 Κακοήθεις (Λόγος ~1.7:1).
    *   **Καλά νέα:** Υπάρχει μια σχετική ισορροπία. Δεν είναι ακραία μη ισορροπημένο (imbalanced).
*   **Προειδοποίηση 1: Ανισορροπία Κλάσεων (Class Imbalance - Slide 11):**
    *   Αν μια κλάση είχε *πολύ* λιγότερα δείγματα, τα μοντέλα μπορεί να μάθαιναν να προβλέπουν κυρίως την πλειοψηφούσα κλάση.
    *   Η **Ακρίβεια (Accuracy)** δεν είναι καλή μετρική σε τέτοιες περιπτώσεις.
    *   Τεχνικές αντιμετώπισης (αναφέρονται, δεν εφαρμόζονται εδώ): Oversampling (δημιουργία νέων δειγμάτων της μειοψηφίας), Undersampling (αφαίρεση δειγμάτων της πλειοψηφίας), Class Weights, Ensemble Methods.
*   **Οπτικοποίηση Χαρακτηριστικών (Feature Visualization):**
    *   **Boxplots (Slides 12-27):**
        *   Το Boxplot δείχνει κατανομή: min, max, ενδοτεταρτημοριακό εύρος (IQR), διάμεσο (median), ακραίες τιμές (outliers). (Βλ. Slide 12 για επεξήγηση).
        *   Δημιουργούμε boxplots για *κάθε* χαρακτηριστικό, ξεχωριστά για τις κλάσεις Malignant και Benign.
        *   **Στόχος:** Να δούμε αν οι κατανομές των τιμών διαφέρουν σημαντικά μεταξύ των δύο κλάσεων για ένα δεδομένο χαρακτηριστικό.
        *   **Παρατήρηση:** Μερικά χαρακτηριστικά δείχνουν καλό διαχωρισμό (μικρή επικάλυψη - overlap - στα κουτιά, π.χ. Slide 16 - concave points_mean), ενώ άλλα έχουν μεγάλη επικάλυψη (π.χ. Slide 18 - texture_se). Χαρακτηριστικά με μικρή επικάλυψη είναι πιθανώς πιο χρήσιμα για την ταξινόμηση.
    *   **Pair Plots (aka Scatter Matrix - Slide 28):**
        *   Δείχνει scatter plots για ζεύγη χαρακτηριστικών. Χρήσιμο για να δούμε σχέσεις μεταξύ 2 χαρακτηριστικών ταυτόχρονα και πώς διαχωρίζονται οι κλάσεις στον 2D χώρο.
        *   Εδώ εστιάζουμε στα 'prominent' χαρακτηριστικά: `Symmetry_mean`, `Area_worst`, `Symmetry_worst`.
        *   **Παρατήρηση:** Φαίνεται να σχηματίζονται σχετικά εύκολα διαχωρίσιμες περιοχές στον 2D χώρο αυτών των χαρακτηριστικών.
        *   **Σημείωση:** Αγνοούμε τα ιστογράμματα στη διαγώνιο αυτού του τύπου γραφήματος.
    *   **Έννοια "Εύκολα Διαχωρίσιμο" (Easy-to-separate - Slide 29):** Όσο μικρότερη η "γκρίζα ζώνη" (περιοχή επικάλυψης) μεταξύ των κλάσεων, τόσο ευκολότερη η ταξινόμηση.
    *   **Προειδοποίηση 2: Μη Σαφής Διαφορά (No Clear Difference - Slides 30-31):** Αν οι τιμές των χαρακτηριστικών για τις δύο κλάσεις είναι πολύ ανακατεμένες (μεγάλη επικάλυψη), η ταξινόμηση είναι δύσκολη. Αυτό φαίνεται και στα scatter plots (Slide 30) και στα ιστογράμματα (Slide 31).
    *   **Ιστογράμματα (Histograms - Slides 31-46):**
        *   Δείχνουν την κατανομή συχνοτήτων των τιμών για ένα χαρακτηριστικό. Εδώ, βλέπουμε τα ιστογράμματα των δύο κλάσεων στο ίδιο γράφημα με επικάλυψη.
        *   **Στόχος:** Όπως και τα boxplots, να δούμε τον βαθμό επικάλυψης. Λιγότερη επικάλυψη = καλύτερο χαρακτηριστικό.
        *   **Παρατήρηση:** Επιβεβαιώνεται ότι μερικά χαρακτηριστικά έχουν καλύτερο διαχωρισμό (π.χ. Slide 33 - area_mean) από άλλα (π.χ. Slide 36 - fractal_dimension_mean).
    *   **3D Οπτικοποίηση (Slide 47):** Μερικές φορές, η οπτικοποίηση σε 3 διαστάσεις (χρησιμοποιώντας 3 χαρακτηριστικά) μπορεί να αποκαλύψει καλύτερα τον διαχωρισμό των κλάσεων.

---

## **4. Μοντέλα Μηχανικής Μάθησης για Ταξινόμηση (ML Approaches) (Slides 48-85)**

Εξετάζουμε μερικούς βασικούς αλγορίθμους:

*   **k-Nearest Neighbors (kNN - Slides 49-55)**
    *   **Ιδέα:** Για να ταξινομήσουμε ένα νέο, άγνωστο σημείο, βρίσκουμε τους `k` κοντινότερους γείτονές του στον χώρο των χαρακτηριστικών (με βάση τα δεδομένα εκπαίδευσης). Το νέο σημείο ταξινομείται στην κλάση που ανήκει η πλειοψηφία των `k` γειτόνων του. (Βλ. Slides 52-54 για k=1, 3, 5).
    *   **Πλεονεκτήματα:** Πολύ διαισθητικός και απλός στην κατανόηση.
    *   **Μειονεκτήματα:**
        *   Αργός στην πρόβλεψη (πρέπει να υπολογίσει αποστάσεις από *όλα* τα σημεία εκπαίδευσης).
        *   Ευαίσθητος στην "κατάρα της διαστατικότητας" (curse of dimensionality).
        *   Μπορεί να ευνοεί την πλειοψηφούσα κλάση σε ανισόρροπα σύνολα.
        *   Απαιτεί κανονικοποίηση/τυποποίηση των χαρακτηριστικών, καθώς βασίζεται σε αποστάσεις.
*   **Support Vector Machines (SVM) / Support Vector Classifier (SVC - Slides 56-70)**
    *   **Ιδέα:** Προσπαθεί να βρει το *υπερεπίπεδο (hyperplane)* που διαχωρίζει καλύτερα τις κλάσεις στον χώρο των χαρακτηριστικών. "Καλύτερα" σημαίνει αυτό που έχει το **μεγαλύτερο περιθώριο (margin)** από τα πλησιέστερα σημεία κάθε κλάσης.
    *   **Support Vectors (Διανύσματα Στήριξης):** Τα σημεία εκπαίδευσης που βρίσκονται ακριβώς πάνω στο περιθώριο (ή παραβιάζουν το περιθώριο σε μη-γραμμικά διαχωρίσιμες περιπτώσεις). Μόνο αυτά καθορίζουν το βέλτιστο υπερεπίπεδο. (Slide 61).
    *   **Linear SVM:** Όταν τα δεδομένα είναι γραμμικά διαχωρίσιμα (Slide 56). Η SVM βρίσκει τη γραμμή/επίπεδο με το μέγιστο περιθώριο (Slide 60). Μπορεί να ανεχτεί μερικά λάθη (soft margin) για καλύτερη γενίκευση (Slide 62).
    *   **Non-Linear SVM (Kernel Trick - Slides 67-70):** Όταν τα δεδομένα δεν είναι γραμμικά διαχωρίσιμα στον αρχικό χώρο. Η SVM χρησιμοποιεί μια **συνάρτηση πυρήνα (kernel function)** για να μετασχηματίσει τα δεδομένα σε έναν χώρο υψηλότερης διάστασης όπου μπορεί να είναι γραμμικά διαχωρίσιμα, *χωρίς* να υπολογίζει ρητά τις συντεταγμένες σε αυτόν τον χώρο. (Βλ. Slides 68, 69 για παραδείγματα). Το μόνο που χρειάζεται είναι ο υπολογισμός εσωτερικών γινομένων (inner products) στον χώρο υψηλής διάστασης, που γίνεται αποτελεσματικά μέσω του πυρήνα (Slide 70).
    *   **Ταξινόμηση Νέου Σημείου:** Ελέγχεται σε ποια πλευρά του υπερεπιπέδου-ορίου πέφτει το νέο σημείο (Slides 63-65).
*   **Decision Trees (Δέντρα Απόφασης - Slides 71-83)**
    *   **Ιδέα:** Δημιουργεί ένα μοντέλο που μοιάζει με διάγραμμα ροής (δέντρο). Κάθε εσωτερικός κόμβος αναπαριστά ένα "τεστ" σε ένα χαρακτηριστικό (π.χ. "είναι το `x1 <= 2`?"), κάθε κλάδος μια έκβαση του τεστ, και κάθε φύλλο μια πρόβλεψη κλάσης. Το δέντρο χωρίζει τον χώρο των χαρακτηριστικών σε ορθογώνιες περιοχές με διαδοχικές, άξονα-ευθυγραμμισμένες διαχωριστικές γραμμές (axis-aligned splits) (Slides 71-76).
    *   **Ορισμός (Slide 77):** Μη-παραμετρική μέθοδος επιβλεπόμενης μάθησης. Μαθαίνει απλούς κανόνες απόφασης (if-then-else) από τα δεδομένα. Βαθύτερο δέντρο = πιο πολύπλοκοι κανόνες, μπορεί να οδηγήσει σε υπερ-προσαρμογή (overfitting).
    *   **Παραμετρικά vs. Μη-Παραμετρικά Μοντέλα (Slide 78):** Παραμετρικά (π.χ. Γραμμική Παλινδρόμηση) έχουν σταθερό αριθμό παραμέτρων. Μη-παραμετρικά (π.χ. DT, kNN) έχουν αριθμό παραμέτρων που μπορεί να αυξάνεται με το μέγεθος των δεδομένων.
    *   **Ταξινόμηση Νέου Σημείου:** Το σημείο "διασχίζει" το δέντρο από τη ρίζα προς τα φύλλα, ακολουθώντας τους κανόνες σε κάθε κόμβο, μέχρι να φτάσει σε ένα φύλλο που δίνει την τελική κλάση (Slides 79-83).
*   **Naïve Bayes Classifier (Αφελής Ταξινομητής Bayes - Slides 84-85)**
    *   **Ιδέα:** Βασίζεται στο θεώρημα του Bayes. Υπολογίζει την πιθανότητα ένα σημείο να ανήκει σε κάθε κλάση, δεδομένων των τιμών των χαρακτηριστικών του, και επιλέγει την κλάση με την υψηλότερη πιθανότητα: `P(Class | Features) ∝ P(Features | Class) * P(Class)`.
    *   **Gaussian Naïve Bayes (Εδώ):** Υποθέτει ότι οι τιμές κάθε χαρακτηριστικού *μέσα σε κάθε κλάση* ακολουθούν Γκαουσιανή (κανονική) κατανομή (Slide 84, οι ελλείψεις).
    *   **"Naïve" (Αφελής) Υπόθεση:** Η *κρίσιμη* υπόθεση είναι ότι τα χαρακτηριστικά είναι **υπό συνθήκη ανεξάρτητα (conditionally independent)** δεδομένης της κλάσης. Δηλαδή, γνωρίζοντας την κλάση, η τιμή ενός χαρακτηριστικού δεν επηρεάζει την τιμή ενός άλλου. Αυτή η υπόθεση απλοποιεί πολύ τους υπολογισμούς `P(Features | Class)`, αλλά συχνά δεν ισχύει στην πράξη. (Γι' αυτό οι ελλείψεις στο Slide 84 είναι άξονα-ευθυγραμμισμένες).
    *   **Όροι:** `P(Class)` είναι η *εκ των προτέρων πιθανότητα (prior)* της κλάσης. `P(Features | Class)` είναι η *πιθανοφάνεια (likelihood)* των χαρακτηριστικών δεδομένης της κλάσης.

---

## **5. Αξιολόγηση Απόδοσης (Evaluating Performance) (Slides 86-95)**

Πώς κρίνουμε πόσο καλό είναι το μοντέλο μας; Ιδιαίτερα σημαντικό για το **test set** (δεδομένα που δεν είδε το μοντέλο κατά την εκπαίδευση).

*   **Confusion Matrix (Πίνακας Σύγχυσης - Slides 87, 89):** Ένας πίνακας 2x2 (για δυαδική ταξινόμηση) που δείχνει:
    *   **Ορισμός Θετικής Κλάσης (Positive Class):** Πολύ σημαντικό! Εδώ, ορίζουμε ως θετική κλάση την **Κακοήθη (Malignant)**.
    *   **True Positives (TP):** Αριθμός κακοήθων δειγμάτων που ταξινομήθηκαν σωστά ως κακοήθη.
    *   **True Negatives (TN):** Αριθμός καλοήθων δειγμάτων που ταξινομήθηκαν σωστά ως καλοήθη.
    *   **False Positives (FP) / Type I Error:** Αριθμός καλοήθων δειγμάτων που ταξινομήθηκαν *λάθος* ως κακοήθη (Slide 88).
    *   **False Negatives (FN) / Type II Error:** Αριθμός κακοήθων δειγμάτων που ταξινομήθηκαν *λάθος* ως καλοήθη (Slide 88). (Πολύ κρίσιμο λάθος στο ιατρικό σενάριο!).
    *   *Παράδειγμα Slide 89 (Test Set):* Το μοντέλο kNN έχασε 2 όγκους (FN=2).
*   **Μετρικές Απόδοσης (Performance Metrics - Slide 90):**
    *   **Accuracy (Ορθότητα):** `(TP + TN) / (TP + TN + FP + FN)`. Το ποσοστό των σωστών προβλέψεων συνολικά. **Προσοχή:** Μπορεί να είναι παραπλανητική σε ανισόρροπα σύνολα (Slides 91, 93).
    *   **Precision (Ακρίβεια):** `TP / (TP + FP)`. Από όλα όσα προβλέψαμε ως θετικά (κακοήθη), πόσα ήταν όντως θετικά; (Σημαντικό για να μην ανησυχούμε άδικα ασθενείς).
    *   **Recall (Ανάκληση) / Sensitivity (Ευαισθησία):** `TP / (TP + FN)`. Από όλα τα πραγματικά θετικά (κακοήθη), πόσα βρήκαμε; (Κρίσιμη μετρική εδώ, αντιστοιχεί στον 1ο περιορισμό του 90%).
    *   **F1-Score:** `2 * (Precision * Recall) / (Precision + Recall)`. Ο αρμονικός μέσος των Precision και Recall. Χρήσιμη μετρική όταν θέλουμε μια ισορροπία μεταξύ Precision και Recall (Slide 92).
*   **Σύγκριση Μοντέλων:**
    *   Slide 91: Η υψηλή Accuracy για όλα τα μοντέλα μπορεί να είναι παραπλανητική.
    *   Slide 94: Βλέπουμε τις τιμές Precision και Recall για διάφορα μοντέλα στο test set. Παρατηρούμε ότι κάποια μοντέλα μπορεί να έχουν υψηλό Recall αλλά χαμηλότερο Precision και το αντίστροφο. Η επιλογή εξαρτάται από τις απαιτήσεις του προβλήματος (εδώ, το Recall για τους κακοήθεις όγκους είναι πρωταρχικής σημασίας).
*   **Average Values Among Folds (Slide 95):** Τα αποτελέσματα συνήθως προκύπτουν από **διασταυρούμενη επικύρωση (Cross-Validation)**, όπου το πείραμα επαναλαμβάνεται πολλές φορές ("folds") και παίρνουμε τον μέσο όρο των μετρικών για πιο αξιόπιστη εκτίμηση της απόδοσης.

---

## **6. Συμπεράσματα (Conclusions - Slide 96)**

*   Τα μοντέλα ταξινόμησης είναι κρίσιμα εργαλεία υποστήριξης απόφασης, ειδικά σε ιατρικά σενάρια.
*   Η ανισορροπία των κλάσεων είναι μια συχνή πρόκληση.
*   Η αξιολόγηση απαιτεί μετρικές πέρα από την απλή Accuracy (Precision, Recall, F1). Η επιλογή των κατάλληλων μετρικών εξαρτάται από το πρόβλημα.
*   Διαφορετικά μοντέλα (kNN, SVM, DT, NB κ.ά.) έχουν διαφορετικά πλεονεκτήματα, μειονεκτήματα και υποθέσεις. Δεν υπάρχει ένα "καλύτερο" μοντέλο για όλα τα προβλήματα.
