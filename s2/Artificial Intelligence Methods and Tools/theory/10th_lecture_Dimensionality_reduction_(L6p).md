# **Τεχνικές Μείωσης Διαστατικότητας**

### **Εισαγωγή (Διαφάνεια 1)**

*   **Θέμα:** Τεχνικές μείωσης της διαστατικότητας των δεδομένων.
*   **Στόχος:** "Μειώνοντας τα Δεδομένα στο Κατάλληλο Μέγεθος!" – Κατανόηση του γιατί και πώς μειώνουμε τον αριθμό των χαρακτηριστικών (διαστάσεων) σε ένα σύνολο δεδομένων.
*   Η εικόνα δείχνει πώς ο αριθμός των πιθανών "θέσεων" ή "καταστάσεων" αυξάνεται εκθετικά με τον αριθμό των διαστάσεων (1 διάσταση: 10 θέσεις, 2 διαστάσεις: 100 θέσεις, 3 διαστάσεις: 1000 θέσεις). Αυτό υπονοεί την "κατάρα της διαστατικότητας".

### **Κίνητρο (Motivation) (Διαφάνεια 2)**

*   **Η Κατάρα της Διαστατικότητας (Curse of Dimensionality):**
    *   Έστω δύο σημεία **n** και **m** από d-διάστατες Γκαουσιανές κατανομές.
    *   Η αναμενόμενη τετραγωνική απόσταση μεταξύ τους: `E{||n - m||²} = d(σ₁² + σ₂²) + ||μ₁ - μ₂||²`
        *   `d(σ₁² + σ₂²)`: Όρος **θορύβου**, αυξάνεται γραμμικά με τον αριθμό των διαστάσεων `d`.
        *   `||μ₁ - μ₂||²`: Όρος **πληροφορίας**, η διαφορά μεταξύ των μέσων τιμών των κατανομών.
    *   **Συμπέρασμα:** Όταν το `d` (αριθμός διαστάσεων) είναι μεγάλο, ο όρος θορύβου κυριαρχεί έναντι του όρου πληροφορίας.
    *   **Συνέπεια:** Το μοντέλο μηχανικής μάθησης **δεν θα αποδίδει καλά** γιατί δυσκολεύεται να διακρίνει το χρήσιμο σήμα από τον θόρυβο. Τα σημεία φαίνονται όλα εξίσου μακριά μεταξύ τους.

### **Το Σύνολο Δεδομένων "Wine" (The Wine Dataset) (Διαφάνεια 3)**

*   **Προέλευση:** Μελέτη Ιταλικών κόκκινων κρασιών.
*   **Περιεχόμενο:** 178 δείγματα, καθένα με 13 χαρακτηριστικά που περιγράφουν χημικές ιδιότητες (π.χ., περιεκτικότητα σε αλκοόλ, ένταση χρώματος, φλαβονοειδή).
*   **Τιμές Χαρακτηριστικών (Ενδεικτικά):**
    *   Alcohol & malic_acid
    *   Ash & alcalinity_of_ash
    *   Magnesium & total_phenols
    *   Flavanoids and nonflavanoid_phenols
    *   Proanthocyanins
    *   color_intensity & hue
    *   od280/od315_of_diluted_wines & proline
*   Το γράφημα "Class Distribution" δείχνει τον αριθμό των δειγμάτων ανά κλάση (Class 0: ~71, Class 1: ~59, Class 2: ~48).

### **Οπτικοποιήσεις Τιμών Χαρακτηριστικών [1/13] - [13/13] (Διαφάνειες 4-16)**

*   Για κάθε ένα από τα 13 χαρακτηριστικά, παρουσιάζονται δύο γραφήματα:
    1.  **Box Plot (Θηκόγραμμα):** Δείχνει την κατανομή των τιμών του χαρακτηριστικού για κάθε μία από τις 3 κλάσεις (Class 0, Class 1, Class 2). Βοηθά στον εντοπισμό της διάμεσης τιμής, του ενδοτεταρτημοριακού εύρους και των ακραίων τιμών.
    2.  **Histogram (Ιστόγραμμα Πυκνότητας):** Δείχνει την πυκνότητα (ή συχνότητα) των τιμών του χαρακτηριστικού για κάθε κλάση. Βοηθά να δούμε πώς οι κατανομές των κλάσεων επικαλύπτονται ή διαχωρίζονται για το συγκεκριμένο χαρακτηριστικό.
*   **Στόχος αυτών των γραφημάτων:** Να εξεταστεί οπτικά η διαχωριστική ικανότητα κάθε χαρακτηριστικού μεμονωμένα. Αν οι κατανομές για τις διαφορετικές κλάσεις είναι καλά διαχωρισμένες σε ένα χαρακτηριστικό, τότε αυτό το χαρακτηριστικό είναι πιθανώς χρήσιμο για την ταξινόμηση. Αν υπάρχει μεγάλη επικάλυψη, το χαρακτηριστικό από μόνο του μπορεί να μην είναι πολύ πληροφοριακό.
*   **Γενική Παρατήρηση (από την προηγούμενη παρουσίαση και εδώ):** Πολλά χαρακτηριστικά δείχνουν σημαντική επικάλυψη μεταξύ των κλάσεων, ενώ λίγα (π.χ., flavanoids, color_intensity, proline) δείχνουν καλύτερη διαχωριστική ικανότητα.

### **Σύγκριση Προσεγγίσεων Μείωσης Διαστατικότητας (Διαφάνεια 17)**

*   Υπάρχουν πολλές διαθέσιμες τεχνικές.

**Τεχνικές προς Επίδειξη (Διαφάνεια 18)**

*   **Γραμμικές Τεχνικές (Linear Techniques):**
    *   Principal Component Analysis (PCA) - Ανάλυση Κύριων Συνιστωσών
    *   Linear Discriminant Analysis (LDA) - Γραμμική Διακριτική Ανάλυση
    *   Factor Analysis - Παραγοντική Ανάλυση
*   **Μη-Γραμμικές Τεχνικές (Non-Linear Techniques):**
    *   t-Distributed Stochastic Neighbor Embedding (t-SNE)
    *   Uniform Manifold Approximation and Projection (UMAP)
    *   Isomap
    *   Locally Linear Embedding (LLE) - Τοπικά Γραμμική Ενσωμάτωση
    *   Multidimensional Scaling (MDS) - Πολυδιάστατη Κλιμάκωση
*   **Μέθοδοι Βασισμένες στην Επιλογή Χαρακτηριστικών (Feature Selection-Based Methods):**
    *   Recursive Feature Elimination (RFE) - Αναδρομική Εξάλειψη Χαρακτηριστικών

### **Αξιολόγηση της Απόδοσης (Διαφάνεια 19)**

1.  **Απόδοση Ομαδοποίησης (Clustering Performance):** Ποσοτικοποίηση του πόσο καλά σχηματίζονται οι ομάδες (clusters).
    *   **Metrics Silhouette Score:** Μετρά πόσο παρόμοιο είναι ένα σημείο δεδομένων με τη δική του ομάδα σε σύγκριση με άλλες ομάδες. Τιμές από -1 (κακή ομαδοποίηση) έως 1 (καλή ομαδοποίηση).
    *   **Calinski-Harabasz (CH) Index:** Αξιολογεί τη συμπαγή δομή και τον διαχωρισμό των ομάδων. **Υψηλότερες τιμές** υποδεικνύουν καλύτερα ορισμένες ομάδες.
    *   **Davies-Bouldin (DB) Index:** Αξιολογεί τον διαχωρισμό και τη συμπαγή δομή των ομάδων. **Χαμηλότερες τιμές** υποδεικνύουν καλύτερη ομαδοποίηση.
2.  **Απόδοση Ταξινόμησης (Classification Performance):** Χρήση ενός απλού ταξινομητή (π.χ., Logistic Regression) για πρόβλεψη ετικετών κλάσης. Χρήση 5-fold cross-validation για την αξιολόγηση της ακρίβειας (accuracy score).
3.  **Χρόνος Εκτέλεσης (Execution Time):** Μέτρηση του υπολογιστικού χρόνου για κάθε μέθοδο. Σημαντικό για την αποδοτικότητα, ειδικά σε μεγάλα σύνολα δεδομένων.
4.  **Οπτικοποίηση (Visualization):** Προβολές 2D και 3D. Οπτική επιθεώρηση του πόσο καλά οι μέθοδοι διαχωρίζουν τις κλάσεις ή τις ομάδες. Χρήση διαγραμμάτων διασποράς (scatter plots) με υπομνήματα.

---

## **Γραμμικές Τεχνικές (Διαφάνεια 20)**

### **Α. Ανάλυση Κύριων Συνιστωσών (Principal Component Analysis - PCA) (Διαφάνειες 21-25)**

*   **Βασική Ιδέα (Διαφ. 21-23):**
    *   Μια μη επιβλεπόμενη τεχνική.
    *   Βρίσκει νέους άξονες (κύριες συνιστώσες) που είναι γραμμικοί συνδυασμοί των αρχικών χαρακτηριστικών.
    *   Η πρώτη κύρια συνιστώσα είναι ο άξονας της **μεγαλύτερης διακύμανσης** στα δεδομένα (η γραμμή καλύτερης προσαρμογής).
    *   Κάθε επόμενη συνιστώσα είναι ορθογώνια προς τις προηγούμενες και συλλαμβάνει τη μέγιστη εναπομένουσα διακύμανση.
    *   Τα δεδομένα "προβάλλονται" πάνω σε αυτούς τους νέους άξονες. Για παράδειγμα, από 2 διαστάσεις (ύψος, βάρος) σε 1 διάσταση (πόσο μακριά είναι ένα σημείο κατά μήκος του "χάρακα" της πρώτης κύριας συνιστώσας).
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 24-25):**
    *   **Οπτικοποίηση (2D & 3D):** Οι προβολές δείχνουν πώς οι κλάσεις διαχωρίζονται μετά την PCA. Η κλάση 1 (πορτοκαλί) φαίνεται να διαχωρίζεται καλά, ενώ οι κλάσεις 0 (μπλε) και 2 (πράσινο) έχουν κάποια επικάλυψη.
    *   **Μετρικές:**
        *   PCA 2D: Χρόνος 0.0132s, Silhouette 0.54, CH 327.22, DB 0.62, Ακρίβεια Ταξ. 0.98.
        *   PCA 3D: Χρόνος 0.0017s, Silhouette 0.45, CH 195.07, DB 0.81, Ακρίβεια Ταξ. 0.97.
    *   *Παρατήρηση:* Η PCA 2D φαίνεται να δίνει ελαφρώς καλύτερους δείκτες ομαδοποίησης και ακρίβεια ταξινόμησης για αυτό το σύνολο δεδομένων, αν και η PCA 3D αναφέρεται ως ταχύτερη.

### **Β. Γραμμική Διακριτική Ανάλυση (Linear Discriminant Analysis - LDA) (Διαφάνειες 26-31)**

*   **Βασική Ιδέα (Διαφ. 26-28):**
    *   Μια **επιβλεπόμενη** τεχνική (χρησιμοποιεί τις ετικέτες των κλάσεων).
    *   Στόχος: Να βρεθεί ένας νέος χώρος χαρακτηριστικών (άξονες) που **μεγιστοποιεί τον διαχωρισμό μεταξύ των κλάσεων**.
    *   Κριτήρια για τη δημιουργία του νέου άξονα:
        1.  **Μεγιστοποίηση της απόστασης μεταξύ των μέσων όρων των κλάσεων.**
        2.  **Ελαχιστοποίηση της διακύμανσης εντός κάθε κλάσης.**
    *   Παράδειγμα με τους τροχούς: Αν ένα χαρακτηριστικό (καμπυλότητα) δεν διαχωρίζει καλά, η προσθήκη ενός άλλου (διάμετρος) και η χρήση LDA μπορεί να βρει έναν καλύτερο διαχωριστικό άξονα.
*   **LDA vs PCA (Διαφάνεια 30):**
    *   **PCA:** Μεγιστοποιεί τη διακύμανση των δεδομένων (μη επιβλεπόμενη).
    *   **LDA:** Μεγιστοποιεί τον διαχωρισμό μεταξύ γνωστών κλάσεων (επιβλεπόμενη).
*   **Αξιολόγηση Απόδοσης (Διαφάνεια 31):**
    *   **Οπτικοποίηση (2D):** Η προβολή LDA 2D δείχνει έναν πολύ καλό διαχωρισμό των τριών κλάσεων.
    *   **Μετρικές (LDA 2D):**
        *   Χρόνος: 0.0101s
        *   Silhouette: 0.66
        *   CH Index: 577.95
        *   DB Index: 0.45
        *   Ακρίβεια Ταξ.: **1.00** (τέλεια ταξινόμηση μετά την LDA)
    *   *Παρατήρηση:* Η LDA επιτυγχάνει εξαιρετική ακρίβεια ταξινόμησης και πολύ καλούς δείκτες ομαδοποίησης, δεδομένου ότι είναι επιβλεπόμενη μέθοδος και στοχεύει στον διαχωρισμό κλάσεων.

### **Γ. Παραγοντική Ανάλυση (Factor Analysis - FA) (Διαφάνειες 32-34)**

*   **Σκοπός (Διαφ. 32):** Στατιστική τεχνική για τον εντοπισμό και τη μοντελοποίηση **λανθανόντων μεταβλητών (παραγόντων)** που εξηγούν τις παρατηρούμενες συσχετίσεις μεταξύ ενός συνόλου μεταβλητών. Στόχος η μείωση της διαστατικότητας με την καταγραφή της υποκείμενης δομής.
*   **Υπόθεση:** Τα παρατηρούμενα δεδομένα παράγονται από έναν γραμμικό συνδυασμό λίγων υποκείμενων παραγόντων συν θόρυβο. Οι παράγοντες αυτοί αντιπροσωπεύουν κρυφές ή μη παρατηρήσιμες μεταβλητές.
*   **Διαφορά από PCA:** Η PCA εστιάζει στη μεγιστοποίηση της διακύμανσης (data-driven). Η FA είναι **μοντελο-βασισμένη** και εστιάζει στην εξήγηση της **δομής συνδιακύμανσης**. Ενσωματώνει ένα πιθανοτικό μοντέλο για τον θόρυβο.
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 33-34):**
    *   **Οπτικοποίηση (2D & 3D):** Δείχνει κάποιο διαχωρισμό των κλάσεων.
    *   **Μετρικές:**
        *   FA 2D: Χρόνος 0.0773s, Silhouette 0.50, CH 231.26, DB 0.66, Ακρίβεια Ταξ. 0.94.
        *   FA 3D: Χρόνος 0.0902s, Silhouette 0.40, CH 99.02, DB 1.01, Ακρίβεια Ταξ. 0.96.
    *   *Παρατήρηση:* Η FA δίνει αξιοπρεπή αποτελέσματα, αλλά όχι τόσο ισχυρά όσο η LDA για ταξινόμηση σε αυτό το παράδειγμα. Ο χρόνος εκτέλεσης είναι υψηλότερος από PCA/LDA.

---

## **Μη-Γραμμικές Τεχνικές (Διαφάνεια 35)**

### **Δ. t-Distributed Stochastic Neighbor Embedding (t-SNE) (Διαφάνειες 36-38)**

*   **Σκοπός (Διαφ. 36):** Μη-γραμμική τεχνική μείωσης διαστατικότητας σχεδιασμένη για την **οπτικοποίηση δεδομένων υψηλής διάστασης** σε 2D ή 3D, διατηρώντας την **τοπική δομή** των δεδομένων (π.χ., ομάδες, γειτονιές).
*   **Υποθέσεις:** Η διατήρηση της τοπικής δομής (σχέσεις μεταξύ πλησιέστερων γειτόνων) είναι πιο σημαντική από τη διατήρηση των παγκόσμιων σχέσεων.
*   **Διαφορά από PCA:** Η PCA διατηρεί την παγκόσμια διακύμανση. Η t-SNE διατηρεί τις τοπικές δομές, εξασφαλίζοντας ότι σημεία που είναι κοντά στον αρχικό χώρο παραμένουν κοντά στον χώρο χαμηλότερης διάστασης. Εστιάζει σε ομάδες και γειτονιές.
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 37-38):**
    *   **Οπτικοποίηση (2D & 3D):** Οι προβολές δείχνουν **εξαιρετικό διαχωρισμό** των τριών κλάσεων, σχηματίζοντας ευδιάκριτες ομάδες.
    *   **Μετρικές:**
        *   t-SNE 2D: Χρόνος 1.6281s, Silhouette 0.58, CH 613.06, DB 0.52, Ακρίβεια Ταξ. 0.97.
        *   t-SNE 3D: Χρόνος 5.8140s, Silhouette 0.36, CH 146.63, DB 1.00, Ακρίβεια Ταξ. 0.94.
    *   *Παρατήρηση:* Η t-SNE είναι πολύ αποτελεσματική για οπτικοποίηση και διατήρηση ομαδοποιήσεων. Είναι υπολογιστικά πιο απαιτητική από τις γραμμικές μεθόδους, ειδικά για μεγαλύτερα σύνολα δεδομένων.

### **Ε. Uniform Manifold Approximation and Projection (UMAP) (Διαφάνειες 39-41)**

*   **Σκοπός (Διαφ. 39):** Μη-γραμμική τεχνική για προβολή δεδομένων υψηλής διάστασης σε χώρο χαμηλότερης διάστασης (συνήθως 2D ή 3D) διατηρώντας τόσο **τοπικές όσο και κάποιες παγκόσμιες δομές**.
*   **Υποθέσεις:**
    *   **Manifold Hypothesis:** Τα δεδομένα βρίσκονται σε μια πολλαπλότητα (manifold) χαμηλής διάστασης ενσωματωμένη στον χώρο υψηλής διάστασης, και η δομή αυτής της πολλαπλότητας είναι σημαντική.
    *   **Metric Space Compatibility:** Οι αποστάσεις στον χώρο υψηλής διάστασης αντικατοπτρίζουν σημαντικές σχέσεις.
*   **Διαφορές από t-SNE (Διαφ. 39):**
    1.  Η UMAP είναι **ταχύτερη** και χειρίζεται καλύτερα μεγάλα σύνολα δεδομένων.
    2.  Η UMAP διατηρεί **περισσότερη παγκόσμια δομή** σε σύγκριση με την t-SNE.
    3.  Η UMAP προσφέρει πιο **ερμηνεύσιμες παραμέτρους** (π.χ., `n_neighbors`, `min_dist`) ενώ η `perplexity` της t-SNE απαιτεί περισσότερο πειραματισμό.
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 40-41):**
    *   **Οπτικοποίηση (2D & 3D):** Παρόμοια εξαιρετικός διαχωρισμός κλάσεων με την t-SNE.
    *   **Μετρικές:**
        *   UMAP 2D: Χρόνος 10.6274s, Silhouette 0.58, CH 615.65, DB 0.52, Ακρίβεια Ταξ. 0.97.
        *   UMAP 3D: Χρόνος 0.6258s, Silhouette 0.62, CH 739.44, DB 0.47, Ακρίβεια Ταξ. 0.97.
    *   *Παρατήρηση:* Εξαιρετική απόδοση. Ο χρόνος εκτέλεσης της UMAP 2D είναι υψηλότερος από της t-SNE 2D σε αυτό το παράδειγμα, κάτι που μπορεί να οφείλεται σε παραμέτρους ή στην υλοποίηση της βιβλιοθήκης. Γενικά, η UMAP θεωρείται ταχύτερη. Η UMAP 3D είναι εδώ ταχύτερη από την t-SNE 3D και δίνει καλύτερους δείκτες ομαδοποίησης.

### **ΣΤ. Isometric Mapping (Isomap) (Διαφάνειες 42-44)**

*   **Σκοπός (Διαφ. 42):** Μη-γραμμική τεχνική που διατηρεί την **παγκόσμια γεωμετρία** των δεδομένων καταγράφοντας **γεωδαισιακές αποστάσεις** (αποστάσεις πάνω στην πολλαπλότητα).
*   **Κύριες Υποθέσεις:**
    *   **Manifold Hypothesis:** Τα δεδομένα βρίσκονται σε μια ομαλή πολλαπλότητα χαμηλής διάστασης, και οι συντομότερες διαδρομές πάνω στην πολλαπλότητα (γεωδαισιακές αποστάσεις) αντιπροσωπεύουν σημαντικές σχέσεις.
    *   **Connectivity:** Τα δεδομένα σχηματίζουν ένα συνεκτικό γράφημα, όπου οι αποστάσεις μπορούν να προσεγγιστούν μέσω ενός γραφήματος γειτονίας.
*   **Διαφορές από t-SNE (Διαφ. 42):**
    *   **Διατήρηση Δομής:** Isomap (παγκόσμια δομή μέσω γεωδαισιακών αποστάσεων). t-SNE (κυρίως τοπικές γειτονιές).
    *   **Ντετερμινισμός:** Η Isomap είναι ντετερμινιστική (ίδια είσοδος, ίδια έξοδος). Η t-SNE είναι στοχαστική.
    *   **Κλιμάκωση:** Η Isomap μπορεί να δυσκολευτεί με πολύ μεγάλα σύνολα δεδομένων λόγω του υπολογισμού των κατά ζεύγη αποστάσεων.
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 43-44):**
    *   **Οπτικοποίηση (2D & 3D):** Καλός διαχωρισμός των κλάσεων.
    *   **Μετρικές:**
        *   Isomap 2D: Χρόνος 0.0243s, Silhouette 0.54, CH 448.49, DB 0.60, Ακρίβεια Ταξ. 0.94.
        *   Isomap 3D: Χρόνος 0.0259s, Silhouette 0.47, CH 302.48, DB 0.76, Ακρίβεια Ταξ. 0.94.
    *   *Παρατήρηση:* Καλή απόδοση και γρήγορη για αυτό το μέγεθος δεδομένων. Οι δείκτες ομαδοποίησης ελαφρώς χαμηλότεροι από t-SNE/UMAP.

### **Ζ. Locally Linear Embedding (LLE) (Διαφάνειες 45-48)**

*   **Σκοπός (Διαφ. 45):** Μη-γραμμική τεχνική που στοχεύει στη διατήρηση των **τοπικών γραμμικών σχέσεων** εντός δεδομένων υψηλής διάστασης. Προβάλλει τα σημεία διατηρώντας την τοπική γεωμετρία, χρήσιμη για δεδομένα σε μη-γραμμικές πολλαπλότητες.
*   **Κύριες Υποθέσεις (Διαφ. 45):**
    *   **Τοπική Γραμμικότητα:** Κάθε σημείο και οι γείτονές του βρίσκονται σε (ή κοντά σε) ένα τοπικά γραμμικό "κομμάτι" της πολλαπλότητας.
    *   **Παγκόσμια Μη-Γραμμικότητα:** Η συνολική δομή είναι μη-γραμμική.
    *   **Σχέσεις Γειτονίας:** Κάθε σημείο μπορεί να ανακατασκευαστεί γραμμικά από τους γείτονές του.
*   **Εικόνα (Διαφ. 46):** Δείχνει πώς η LLE "ξετυλίγει" μια πολλαπλότητα σχήματος 'S'.
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 47-48):**
    *   **Οπτικοποίηση (2D & 3D):** Οι κλάσεις σχηματίζουν διακριτές, αν και κάπως παραμορφωμένες, ομάδες.
    *   **Μετρικές:**
        *   LLE 2D: Χρόνος 0.0661s, Silhouette 0.61, CH 302.31, DB 0.55, Ακρίβεια Ταξ. 0.89.
        *   LLE 3D: Χρόνος 0.1741s, Silhouette 0.47, CH 93.70, DB 0.89, Ακρίβεια Ταξ. 0.87.
    *   *Παρατήρηση:* Καλοί δείκτες ομαδοποίησης (ειδικά Silhouette για 2D). Η ακρίβεια ταξινόμησης είναι χαμηλότερη από άλλες μεθόδους.

### **Η. Multidimensional Scaling (MDS) (Διαφάνειες 49-51)**

*   **Σκοπός (Διαφ. 49):** Μείωση διαστατικότητας και οπτικοποίηση, με κύριο στόχο την εύρεση μιας αναπαράστασης χαμηλής διάστασης που **διατηρεί τις κατά ζεύγη αποστάσεις (dissimilarities)** μεταξύ των σημείων όσο το δυνατόν περισσότερο.
*   **Υποθέσεις (Διαφ. 49):**
    *   **Διατήρηση Ανισοτήτων (Dissimilarity Preservation):** Οι αποστάσεις στον αρχικό χώρο αντικατοπτρίζουν σημαντικές σχέσεις που πρέπει να διατηρηθούν.
    *   **Ευκλείδεια Γεωμετρία (στην κλασική MDS):** Τα δεδομένα βρίσκονται σε Ευκλείδειο χώρο.
    *   **Όχι Υποθέσεις Γραμμικότητας:** Λειτουργεί καλά και για δεδομένα χωρίς απλή γραμμική δομή.
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 50-51):**
    *   **Οπτικοποίηση (2D & 3D):** Οι κλάσεις είναι σχετικά διαχωρισμένες.
    *   **Μετρικές:**
        *   MDS 2D: Χρόνος 0.3670s, Silhouette 0.46, CH 174.65, DB 0.80, Ακρίβεια Ταξ. 0.97.
        *   MDS 3D: Χρόνος 0.5358s, Silhouette 0.39, CH 123.95, DB 0.99, Ακρίβεια Ταξ. 0.97.
    *   *Παρατήρηση:* Καλή ακρίβεια ταξινόμησης. Οι δείκτες ομαδοποίησης είναι μέτριοι. Είναι πιο χρονοβόρα από κάποιες άλλες μη-γραμμικές μεθόδους για αυτό το σύνολο.

---

## **Μέθοδοι Επιλογής Χαρακτηριστικών (Διαφάνεια 52)**

### **Θ. Recursive Feature Elimination (RFE) (Διαφάνειες 53-55)**

*   **Σκοποί (Διαφ. 53):**
    *   **Μείωση Διαστατικότητας:** Εντοπίζει τα πιο σημαντικά χαρακτηριστικά αφαιρώντας αναδρομικά τα λιγότερο σημαντικά.
    *   **Επιλογή Χαρακτηριστικών:** Κατατάσσει τα χαρακτηριστικά βάσει της προβλεπτικής τους σημασίας για ένα δεδομένο μοντέλο, διατηρώντας μόνο τα πιο σχετικά.
    *   **Βελτιωμένη Απόδοση Μοντέλου:** Μειώνει την υπερπροσαρμογή (overfitting) και βελτιώνει τη γενίκευση.
*   **Υποθέσεις (Διαφ. 53):**
    *   **Εξάρτηση από Μοντέλο:** Η σημασία των χαρακτηριστικών καθορίζεται από τους συντελεστές/βάρη ενός προβλεπτικού μοντέλου (π.χ., γραμμικά μοντέλα, δέντρα απόφασης, SVM).
    *   **Σχέσεις Χαρακτηριστικών:** Το μοντέλο μπορεί να συλλάβει αποτελεσματικά τη σχέση μεταξύ χαρακτηριστικών και μεταβλητής στόχου.
    *   **Προεπεξεργασία Δεδομένων:** Τα δεδομένα είναι προεπεξεργασμένα (π.χ., κανονικοποιημένα) αν απαιτείται από το μοντέλο.
    *   **Προβλεπτική Σχετικότητα:** Τα πιο σημαντικά χαρακτηριστικά που εντοπίζονται είναι αυτά που συμβάλλουν σημαντικά στην πρόβλεψη.
*   **Αξιολόγηση Απόδοσης (Διαφάνειες 54-55):**
    *   **Οπτικοποίηση (Top 2 & Top 3 Features):** Οι προβολές χρησιμοποιούν μόνο τα 2 ή 3 πιο σημαντικά χαρακτηριστικά που επιλέχθηκαν από την RFE.
    *   **Μετρικές:**
        *   RFE (Top 2): Χαρακτηριστικά 2, Χρόνος 0.1025s, Silhouette 0.41, CH 197.37, DB 0.81, Ακρίβεια Ταξ. 0.90.
        *   RFE (Top 3): Χαρακτηριστικά 3, Χρόνος 0.1114s, Silhouette 0.42, CH 206.51, DB 0.84, Ακρίβεια Ταξ. 0.93.
    *   *Παρατήρηση:* Η RFE με τα 3 κορυφαία χαρακτηριστικά δίνει καλύτερη ακρίβεια ταξινόμησης και ελαφρώς καλύτερους δείκτες ομαδοποίησης από ό,τι με τα 2. Δείχνει ότι μερικά, καλά επιλεγμένα, χαρακτηριστικά μπορούν να διατηρήσουν καλή προβλεπτική ικανότητα.

---

## **Συμπεράσματα (Διαφάνειες 56-57)**

*   **Οφέλη Μείωσης Διαστατικότητας (Διαφ. 56):**
    *   Απλοποιεί τα σύνολα δεδομένων, καθιστώντας τα ευκολότερα στην οπτικοποίηση και ανάλυση.
    *   Μειώνει το υπολογιστικό κόστος και τον χρόνο εκπαίδευσης για μοντέλα μηχανικής μάθησης.
    *   Βοηθά στον μετριασμό της υπερπροσαρμογής αφαιρώντας πλεονάζοντα ή άσχετα χαρακτηριστικά.
*   **Συμβιβασμοί Απόδοσης (Performance Trade-Offs) (Διαφ. 56):**
    *   **Γραμμικές μέθοδοι (π.χ., PCA, LDA):** Υπολογιστικά αποδοτικές, αλλά μπορεί να δυσκολευτούν με μη-γραμμικές σχέσεις.
    *   **Μη-γραμμικές μέθοδοι (π.χ., t-SNE, UMAP):** Συλλαμβάνουν πολύπλοκα μοτίβα, αλλά μπορεί να είναι πιο χρονοβόρες.
*   **Εισήγηση από την Επιλογή Χαρακτηριστικών (Feature Selection Insights) (Διαφ. 57):**
    *   Τεχνικές όπως η RFE εστιάζουν στη διατήρηση των πιο σχετικών χαρακτηριστικών για τη μεταβλητή στόχο.
    *   Η επιλογή χαρακτηριστικών εξασφαλίζει ερμηνευσιμότητα και διατηρεί την προβλεπτική απόδοση.
*   **Τελική Σύσταση (Final Recommendation) (Διαφ. 57):**
    *   **Επιλέξτε τη μέθοδο** με βάση την πολυπλοκότητα του προβλήματος, το μέγεθος του συνόλου δεδομένων και την ανάγκη για ερμηνευσιμότητα.
    *   **Συνδυάστε ποσοτικές μετρικές και οπτικοποιήσεις** για μια ολοκληρωμένη αξιολόγηση.

