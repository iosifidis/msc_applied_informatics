# Clustering

---

## 🔍 Τι είναι το Clustering;

**Clustering** είναι μία από τις βασικές τεχνικές της μη εποπτευόμενης μάθησης (unsupervised learning). Στόχος του είναι η ομαδοποίηση δεδομένων σε υποσύνολα (clusters) με τέτοιο τρόπο ώστε τα σημεία κάθε ομάδας να είναι παρόμοια μεταξύ τους και διαφορετικά από σημεία άλλων ομάδων.

---

## 📐 Ορισμός Ομοιότητας & Απόστασης

Για να μετρηθεί η ομοιότητα, χρησιμοποιούνται συναρτήσεις απόστασης που πρέπει να πληρούν:
- **Συμμετρία**: D(A,B) = D(B,A)
- **Μηδενική απόσταση με τον εαυτό**: D(A,A) = 0
- **Θετικότητα**: D(A,B) = 0 μόνο αν A = B
- **Τριγωνική ανισότητα**: D(A,B) ≤ D(A,C) + D(B,C)

---

## 📊 Είδη Clustering

1. **Partitional Clustering**: Διαχωρίζει τα δεδομένα σε μη επικαλυπτόμενα clusters (π.χ. K-means).
2. **Hierarchical Clustering**: Δημιουργεί ιεραρχικά δέντρα (dendrograms) από nested clusters.

---

## 🧠 Αλγόριθμοι Clustering

### 1. **K-Means**

Ο αλγόριθμος K-Means είναι μία από τις πιο δημοφιλείς μεθόδους συσταδοποίησης (clustering) και ανήκει στις partitional τεχνικές. Ο χρήστης πρέπει να ορίσει εκ των προτέρων τον αριθμό των συστάδων K. Ο αλγόριθμος λειτουργεί επαναληπτικά: αρχικά επιλέγονται τυχαία K κέντρα (centroids), και κάθε σημείο ανατίθεται στο πλησιέστερο κέντρο με βάση κάποια απόσταση (συνήθως την ευκλείδεια). Έπειτα, τα centroids ενημερώνονται ως το μέσο όρο των σημείων κάθε συστάδας και η διαδικασία επαναλαμβάνεται έως ότου υπάρξει σύγκλιση. Παρότι είναι απλός και αποδοτικός, ο K-Means παρουσιάζει προβλήματα όταν τα δεδομένα περιέχουν outliers, όταν οι συστάδες έχουν άνισα μεγέθη, πυκνότητες ή μη σφαιρικά σχήματα. Για καλύτερα αποτελέσματα απαιτείται προεπεξεργασία των δεδομένων (π.χ. κανονικοποίηση) και ενδεχομένως πολλαπλές αρχικοποιήσεις.

- Απλό και δημοφιλές.
- Απαιτεί προκαθορισμένο αριθμό clusters.
- Χρησιμοποιεί ευκλείδεια απόσταση και υπολογίζει κέντρα (centroids).

#### ➖ Προβλήματα:
- Δεν λειτουργεί καλά με clusters διαφορετικών μεγεθών/πυκνοτήτων/μορφών.
- Ευαίσθητο σε outliers.

---

### 2. **Affinity Propagation**

Ο αλγόριθμος **Affinity Propagation** είναι μια εξελιγμένη τεχνική συσταδοποίησης που δεν απαιτεί να ορίσουμε εκ των προτέρων τον αριθμό των clusters, σε αντίθεση με τον K-Means. Αντί να στηρίζεται σε προκαθορισμένα centroids, ο αλγόριθμος εντοπίζει **“εξέχοντα σημεία” (exemplars)**, δηλαδή αντιπροσωπευτικά σημεία που λειτουργούν ως κέντρα για τις υπόλοιπες ομάδες. Η βασική του λογική στηρίζεται στην ανταλλαγή δύο ειδών μηνυμάτων μεταξύ των σημείων: *responsibility* (πόσο κατάλληλο είναι ένα σημείο για να είναι το exemplar ενός άλλου) και *availability* (πόσο κατάλληλο είναι για ένα σημείο να δεχθεί κάποιο άλλο ως exemplar). Αυτή η επικοινωνία συνεχίζεται μέχρι να προκύψει μια σταθερή κατανομή συστάδων. Το μοντέλο απαιτεί έναν **πίνακα ομοιοτήτων** (συνήθως αρνητικά τετράγωνα ευκλείδειας απόστασης) και μια τιμή *προτίμησης* για κάθε σημείο (που επηρεάζει πόσο πιθανό είναι να γίνει exemplar). Αν και ο Affinity Propagation μπορεί να εντοπίσει clusters με ποικίλα σχήματα και μεγέθη και έχει θεωρητικά ισχυρά πλεονεκτήματα, εντούτοις είναι **υπολογιστικά απαιτητικός** και **ευαίσθητος στην επιλογή των παραμέτρων**, ιδιαίτερα στο similarity metric και στην τιμή προτίμησης, ενώ μπορεί να παράγει πολλά μικρά clusters ή περισσότερα από ό,τι επιθυμεί ο χρήστης.

- Δεν απαιτεί προκαθορισμένο αριθμό clusters.
- Βασίζεται σε “exemplars” (αντιπροσωπευτικά σημεία).

#### ➕ Πλεονεκτήματα:
- Καλύτερο σε σύνθετα δεδομένα.
#### ➖ Μειονεκτήματα:
- Πολύπλοκο και αργό για μεγάλα σύνολα.

---

### 3. **Mean Shift**

Ο αλγόριθμος **Mean Shift** είναι μια μη παραμετρική μέθοδος συσταδοποίησης που βασίζεται στην **εκτίμηση πυκνότητας (density estimation)** των δεδομένων. Σε αντίθεση με αλγορίθμους όπως ο K-Means, δεν απαιτεί τον ορισμό του αριθμού των clusters εκ των προτέρων. Η βασική του ιδέα είναι να εντοπίζει τα **τοπικά μέγιστα της πυκνότητας** – τις περιοχές δηλαδή όπου συγκεντρώνονται τα περισσότερα δεδομένα. Αυτό επιτυγχάνεται μέσω ενός **παραθύρου (kernel ή bandwidth)**, εντός του οποίου υπολογίζεται το **κέντρο μάζας** των σημείων, και το κέντρο αυτό μετακινείται επαναληπτικά μέχρι να σταθεροποιηθεί στο πιο πυκνό σημείο. Κάθε cluster σχηματίζεται γύρω από ένα τέτοιο τοπικό μέγιστο. Το Mean Shift είναι ιδιαίτερα χρήσιμο για δεδομένα με **πολύπλοκα ή μη σφαιρικά σχήματα** και μπορεί να εντοπίσει **αυθαίρετο αριθμό συστάδων**, ανάλογα με την κατανομή των δεδομένων. Ωστόσο, ο καθορισμός της παραμέτρου **bandwidth** είναι κρίσιμος: αν είναι πολύ μικρό, δημιουργούνται πολλά μικρά clusters, ενώ αν είναι μεγάλο, μπορεί να συγχωνευτούν σημαντικές δομές. Παρά την υψηλή υπολογιστική του πολυπλοκότητα, ο Mean Shift είναι ισχυρός για **ανάλυση δεδομένων πραγματικού κόσμου** όπου δεν γνωρίζουμε εκ των προτέρων τη δομή τους.

- Βασίζεται στην εκτίμηση πυκνότητας.
- Εντοπίζει περιοχές υψηλής πυκνότητας.
- Δεν απαιτεί καθορισμό αριθμού clusters.

#### ➖ Μειονεκτήματα:
- Η επιλογή παραμέτρου “bandwidth” είναι κρίσιμη.

---

### 4. **Spectral Clustering**

Ο **Spectral Clustering** είναι μια προηγμένη τεχνική συσταδοποίησης που βασίζεται στις ιδιότητες της **γραμμικής άλγεβρας και της φασματικής ανάλυσης** γράφων, και είναι ιδιαίτερα αποτελεσματική σε δεδομένα με **μη γραμμικά διαχωρίσιμες** ή **μη σφαιρικές** συστάδες. Η βασική του ιδέα είναι να κατασκευάσει έναν **γράφο συγγένειας (similarity graph)**, όπου οι κορυφές αντιστοιχούν στα σημεία δεδομένων και οι ακμές συνδέουν τα πιο “κοντινά” σημεία μεταξύ τους (με βάση κάποιο metric, π.χ. ευκλείδεια απόσταση, k-nearest neighbors ή ε-neighborhood). Στη συνέχεια, υπολογίζεται ο **Λαπλασιανός πίνακας (Laplacian matrix)** του γράφου και εξάγονται τα **k πρώτα ιδιοδιανύσματα** του, που σχηματίζουν ένα νέο χώρο χαρακτηριστικών μειωμένης διάστασης. Εκεί εφαρμόζεται αλγόριθμος όπως ο **K-Means** για την τελική συσταδοποίηση. Το Spectral Clustering είναι πολύ ισχυρό στη **διάκριση περίπλοκων δομών** και μπορεί να αποκαλύψει εσωτερικά πρότυπα των δεδομένων, όμως είναι **ευαίσθητο στην επιλογή των υπερπαραμέτρων** (όπως ο τρόπος κατασκευής του γράφου ή το πλήθος των clusters) και μπορεί να είναι **υπολογιστικά δαπανηρό**, ειδικά σε μεγάλα σύνολα δεδομένων λόγω της ιδιοτιμητικής αποσύνθεσης. Παρόλα αυτά, αποτελεί μια από τις πιο ευέλικτες τεχνικές για clustering σε πραγματικά προβλήματα.

- Μετασχηματίζει τα δεδομένα σε χαμηλότερες διαστάσεις μέσω του γραφήματος συγγένειας (similarity graph).
- Εκτελεί K-means στις ιδιοδιανύσματα του λαπλασιανού πίνακα.

#### ➕ Πλεονεκτήματα:
- Κατάλληλο για μη σφαιρικά clusters.
- Ανθεκτικό στον θόρυβο.
#### ➖ Μειονεκτήματα:
- Υπολογιστικά απαιτητικό.

---

### 5. **Agglomerative (Ιεραρχικό) Clustering**

Ο **Agglomerative Clustering** (ή ιεραρχική συσταδοποίηση από κάτω προς τα πάνω) είναι μια **ιεραρχική μέθοδος clustering** που δεν απαιτεί εκ των προτέρων να ορίσουμε τον αριθμό των συστάδων. Ξεκινάει θεωρώντας ότι κάθε σημείο αποτελεί το δικό του ξεχωριστό cluster και σε κάθε βήμα **συγχωνεύει τα δύο πλησιέστερα clusters** με βάση κάποιο μέτρο απόστασης, μέχρι όλα τα σημεία να ανήκουν σε μία μόνο συστάδα ή να ικανοποιείται ένα κριτήριο διακοπής. Το αποτέλεσμα απεικονίζεται σε ένα **δενδρόγραμμα (dendrogram)**, το οποίο μπορεί να “κοπεί” σε οποιοδήποτε επίπεδο για να επιλεγεί ο τελικός αριθμός συστάδων. Υπάρχουν διάφορες στρατηγικές συγχώνευσης (linkage methods):  
- **Single-link (ελάχιστη απόσταση μεταξύ σημείων)**,  
- **Complete-link (μέγιστη απόσταση)**,  
- **Average-link (μέση απόσταση)** και  
- **Ward's method (ελαχιστοποίηση της αύξησης της ενδοομαδικής διακύμανσης)**.  
Ο Agglomerative Clustering είναι ιδανικός για **δεδομένα με ιεραρχική ή πολυεπίπεδη δομή**, προσφέρει οπτική κατανόηση της συσταδοποίησης και δεν επηρεάζεται από την αρχική τοποθέτηση centroids. Ωστόσο, **είναι υπολογιστικά απαιτητικός** (ειδικά για μεγάλα σύνολα δεδομένων), και **δεν μπορεί να αναιρέσει συγχωνεύσεις**, κάτι που σημαίνει ότι λάθη στα πρώτα στάδια επηρεάζουν το τελικό αποτέλεσμα. Παρ’ όλα αυτά, είναι ισχυρό εργαλείο όταν η αναπαράσταση των σχέσεων των δεδομένων έχει σημασία.

- Ξεκινά από κάθε σημείο ως ξεχωριστό cluster και συγχωνεύει βάσει απόστασης.

#### ➕ Πλεονεκτήματα:
- Δεν απαιτεί αριθμό clusters εκ των προτέρων.
- Παράγει δενδρογράμματα.
#### ➖ Μέθοδοι μέτρησης απόστασης μεταξύ clusters:
  - **Single-link (MIN)**: πιο κοντινά σημεία
  - **Complete-link (MAX)**: πιο μακρινά σημεία
  - **Average-link**: μέση απόσταση όλων
  - **Ward’s method**: αύξηση σφάλματος

---

### 6. **DBSCAN**

Ο αλγόριθμος **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** είναι μια ισχυρή μέθοδος συσταδοποίησης που βασίζεται στην έννοια της **πυκνότητας των δεδομένων** και μπορεί να αναγνωρίσει clusters με **αυθαίρετα σχήματα και μεγέθη**, ενώ ταυτόχρονα είναι **ανθεκτικός σε θόρυβο και outliers**. Δεν απαιτεί τον καθορισμό του αριθμού των clusters εκ των προτέρων. Η λειτουργία του βασίζεται σε δύο βασικές παραμέτρους:  
- το **ε (epsilon)**, δηλαδή η ακτίνα γύρω από ένα σημείο, και  
- το **minPts**, ο ελάχιστος αριθμός γειτονικών σημείων εντός της ακτίνας για να θεωρηθεί ένα σημείο ως "πυκνό" (core point).  
Τα σημεία κατηγοριοποιούνται σε **core points** (πυκνά σημεία), **border points** (γειτονικά σε πυκνά αλλά όχι πυκνά από μόνα τους) και **noise points** (απομονωμένα). Οι συστάδες σχηματίζονται μέσω **συνδεσιμότητας βάσει πυκνότητας** μεταξύ των core points και των border points. Ο DBSCAN έχει εξαιρετικά πλεονεκτήματα, καθώς μπορεί να εντοπίσει φυσικές δομές χωρίς να υποθέτει σφαιρικότητα ή ομοιομορφία. Ωστόσο, η **σωστή επιλογή των παραμέτρων ε και minPts είναι κρίσιμη** και μπορεί να είναι δύσκολη σε δεδομένα μεταβλητής πυκνότητας, καθώς διαφορετικές περιοχές μπορεί να απαιτούν διαφορετικές ρυθμίσεις. Παρόλα αυτά, ο DBSCAN αποτελεί εξαιρετική επιλογή για δεδομένα με **σύνθετες δομές** και παρουσία **θορύβου**.

- Βασίζεται στην πυκνότητα σημείων.
- Κατηγοριοποιεί σε core, border, και noise.

#### ➕ Ικανότητες:
- Ανιχνεύει αυθαίρετα σχήματα.
- Ανθεκτικό σε θόρυβο.
#### ➖ Αδυναμίες:
- Δεν λειτουργεί καλά σε δεδομένα διαφορετικής πυκνότητας.

---

### 7. **OPTICS**

Ο **OPTICS (Ordering Points To Identify the Clustering Structure)** είναι μια μέθοδος συσταδοποίησης βασισμένη στην πυκνότητα, παρόμοια με τον DBSCAN, αλλά σχεδιασμένη ώστε να αντιμετωπίζει καλύτερα δεδομένα με **μεταβαλλόμενη πυκνότητα**. Σε αντίθεση με τον DBSCAN, ο οποίος επιστρέφει μια συγκεκριμένη κατάτμηση σε clusters, ο OPTICS **δεν παράγει ρητά clusters**, αλλά δημιουργεί μια **σειριοποίηση των σημείων** και έναν **διάγραμμα προσβασιμότητας (reachability plot)**, το οποίο απεικονίζει την εγγύτητα κάθε σημείου προς τα πυκνά σημεία της περιοχής του. Μελετώντας το διάγραμμα, μπορούμε να **εντοπίσουμε περιοχές χαμηλής προσβασιμότητας** (χαμηλές τιμές), οι οποίες υποδεικνύουν καλά σχηματισμένα clusters. Ο αλγόριθμος χρησιμοποιεί επίσης παραμέτρους όπως το **ε (μέγιστη ακτίνα)** και το **minPts**, αλλά σε αντίθεση με τον DBSCAN, είναι **λιγότερο ευαίσθητος σε αυτές**. Το βασικό του πλεονέκτημα είναι ότι **εντοπίζει clusters διαφορετικής πυκνότητας** χωρίς να απαιτεί καθορισμό ενός παγκόσμιου ορίου, ενώ επιπλέον διατηρεί πληροφορία για την **ιεραρχική δομή των clusters**. Ωστόσο, είναι **πιο αργός από τον DBSCAN**, ιδιαίτερα σε μεγάλα datasets, και απαιτεί **μεταγενέστερη ανάλυση** του διαγράμματος για την εξαγωγή των τελικών clusters. Παρόλα αυτά, αποτελεί εξαιρετικό εργαλείο όταν απαιτείται **ευελιξία και λεπτομέρεια στην ανάλυση πυκνοτήτων**.

- Παρόμοιο με DBSCAN αλλά πιο ευέλικτο.
- Παράγει reachability plot αντί για τελικά clusters.
- Χρήσιμο σε μεταβαλλόμενη πυκνότητα.

---

### 8. **BIRCH**

Ο αλγόριθμος **BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)** είναι μια μέθοδος συσταδοποίησης σχεδιασμένη ειδικά για **μεγάλα σύνολα δεδομένων** και χαρακτηρίζεται από την **αποδοτικότητά του στη μνήμη και στον χρόνο υπολογισμού**. Η βασική ιδέα του BIRCH είναι η δημιουργία μιας **συμπαγούς ιεραρχικής δομής (CF tree – Clustering Feature tree)**, η οποία συνοψίζει τα δεδομένα με τρόπο που επιτρέπει ταχύτατη και επαναλαμβανόμενη συσταδοποίηση. Κάθε κόμβος του CF tree περιέχει στατιστικά (πλήθος σημείων, άθροισμα διανυσμάτων, άθροισμα τετραγώνων) που επιτρέπουν υπολογισμό αποστάσεων και χαρακτηριστικών χωρίς να χρειάζεται αναφορά σε όλα τα σημεία. Ο BIRCH ακολουθεί μια **πολυφασική διαδικασία**: αρχικά δημιουργείται το CF tree με βάση τις παραμέτρους *threshold* και *branching factor*, και στη συνέχεια εφαρμόζεται ένας παραδοσιακός αλγόριθμος (συνήθως K-Means) στα συνοψισμένα δεδομένα. Ο αλγόριθμος είναι εξαιρετικός για **streaming data** ή **πολύ μεγάλα datasets**, ωστόσο η απόδοσή του εξαρτάται έντονα από τις **αρχικές παραμέτρους** και **δεν αποδίδει καλά σε δεδομένα υψηλών διαστάσεων** (curse of dimensionality). Παρόλα αυτά, είναι μια πολύ αποδοτική λύση όταν ο χρόνος και οι πόροι είναι περιορισμένοι.

- Χρησιμοποιεί δέντρο CF (Clustering Feature) για μεγάλες βάσεις δεδομένων.
- Πολυφασικό clustering (με αρχική σύνοψη + τελικό refinement).

#### ➕ Πολύ αποδοτικό σε μεγάλα σύνολα.
#### ➖ Όχι καλό για υψηλές διαστάσεις.

---

### 9. **Gaussian Mixture Models (GMM)**

Τα **Gaussian Mixture Models (GMM)** είναι ένα μοντέλο πιθανοτήτων που χρησιμοποιείται για το **clustering** και αναπαριστά την κατανομή των δεδομένων ως ένα μείγμα από **κανονικές κατανομές (Gaussians)**. Κάθε κατανομή έχει τη δική της μέση τιμή (mean) και διακύμανση (covariance), και το μοντέλο συνδυάζει πολλές τέτοιες κατανομές για να μοντελοποιήσει πιο περίπλοκες κατανομές δεδομένων. Κάθε παρατήρηση σε ένα GMM έχει μια πιθανότητα να ανήκει σε μια από τις κατανομές, και η διαδικασία **εκτίμησης παραμέτρων** βασίζεται στην **Αλγόριθμο EM (Expectation-Maximization)**, που προσδιορίζει τις παραμέτρους των Gaussians, όπως τις μέσες τιμές, τις διακυμάνσεις και τις αναλογίες κάθε κατανομής στο μείγμα. Το GMM έχει το πλεονέκτημα ότι, σε αντίθεση με άλλες μεθόδους clustering όπως το K-means, επιτρέπει τη χρήση διακυμάνσεων διαφορετικού μεγέθους και σχήματος για κάθε κλάση, προσφέροντας μεγαλύτερη ευχέρεια στην κατανομή των δεδομένων. Η εκτίμηση των παραμέτρων μπορεί να γίνει μέσω του αλγορίθμου EM, ο οποίος επαναλαμβάνει δύο βήματα: το **E-step** (εκτίμηση των πιθανοτήτων συμμετοχής των δεδομένων σε κάθε Gaussian) και το **M-step** (ενημέρωση των παραμέτρων των Gaussians).

- Υποθέτει ότι τα δεδομένα προέρχονται από μείγμα Gaussians.
- Δίνει πιθανότητες συμμετοχής κάθε σημείου σε κάθε cluster (soft clustering).
- Χρησιμοποιεί Expectation-Maximization (EM).

#### ➖ Απαιτεί καθορισμό αριθμού clusters.

---

## 📌 Συμπεράσματα

- **Δεν υπάρχει ένας “καλύτερος” αλγόριθμος**. Η επιλογή εξαρτάται από:
  - Σχήμα και μέγεθος clusters
  - Θόρυβο στα δεδομένα
  - Υπολογιστική πολυπλοκότητα
- Ορισμένοι αλγόριθμοι (π.χ. DBSCAN, Mean Shift) **δεν απαιτούν** αριθμό clusters εκ των προτέρων.
- Άλλοι (π.χ. K-means, GMM) είναι **απλοί αλλά περιορισμένοι**.

---

## 📚 Extra Πηγές

- [GeeksForGeeks - Clustering Guide](https://www.geeksforgeeks.org/)
- [LazyProgrammer ML Compendium](https://lazyprogrammer.me/mlcompendium/)
- [Spectral Clustering Intro](https://towardsdatascience.com/spectral-clustering-for-beginners-d08b7d25b4d8)
- [DBSCAN vs OPTICS](https://www.atlantbh.com/clustering-algorithms-dbscan-vs-optics/)

