# **Αξιολόγηση Απόδοσης (Performance Assessment)** 

---

## **1. Διάγραμμα Περιεχομένων (Outline - Slide 2)**

*   Πρακτικές Διασταυρούμενης Επικύρωσης (Cross-validation).
*   Μέτρηση απόδοσης: Προβλήματα Παλινδρόμησης (Regression).
*   Μέτρηση απόδοσης: Προβλήματα Ταξινόμησης (Classification).
*   Μέτρηση απόδοσης: Προβλήματα Συσταδοποίησης (Clustering).
*   Μελέτες περίπτωσης (Case studies) για παλινδρόμηση και ταξινόμηση.
*   Βελτιστοποίηση Υπερπαραμέτρων (αναφέρεται στον τίτλο, slide 1).

---

##**2. Πρακτικές Διασταυρούμενης Επικύρωσης (Cross-Validation Practices) (Slides 3-8, 18-40)**

*   **Κίνητρο (Slides 4-8):** Έστω ένα πρόβλημα παλινδρόμησης (`y = f(x) + noise`). Δοκιμάζουμε διάφορα μοντέλα (π.χ., Γραμμική Παλινδρόμηση, Τετραγωνική, Join-the-dots / Piecewise Linear). Ποιο είναι το καλύτερο; Το μοντέλο που ταιριάζει *τέλεια* στα δεδομένα εκπαίδευσης (π.χ., Join-the-dots) δεν είναι εγγυημένο ότι θα αποδώσει καλά σε **νέα, αόρατα δεδομένα (overfitting)**. Πρέπει να εκτιμήσουμε την ικανότητα **γενίκευσης (generalization)** του μοντέλου.
*   **Μέθοδος Test Set (Slides 18-23):**
    1.  Χωρίζουμε τυχαία τα δεδομένα σε **σύνολο εκπαίδευσης (training set)** (π.χ., 70%) και **σύνολο ελέγχου (test set)** (π.χ., 30%).
    2.  Εκπαιδεύουμε το μοντέλο *μόνο* στο training set.
    3.  Αξιολογούμε την απόδοσή του στο test set (χρησιμοποιώντας κατάλληλες μετρικές, π.χ., MSE).
    *   **Καλά νέα:** Απλή μέθοδος. Επιλέγουμε το μοντέλο με την καλύτερη απόδοση στο test set.
    *   **Κακά νέα:**
        *   "Σπαταλάμε" δεδομένα (το test set δεν χρησιμοποιείται για εκπαίδευση).
        *   Η εκτίμηση απόδοσης βασίζεται σε ένα *συγκεκριμένο*, τυχαίο διαχωρισμό. Αν έχουμε λίγα δεδομένα, το αποτέλεσμα μπορεί να είναι θέμα τύχης (lucky/unlucky split).
        *   **Δεν μπορούμε να ισχυριστούμε με σιγουριά ότι ένα μοντέλο είναι γενικά καλύτερο βασιζόμενοι σε ένα μόνο πείραμα.**
*   **Leave-One-Out Cross-Validation (LOOCV) (Slides 24-32):**
    *   Μια προσπάθεια να χρησιμοποιηθούν τα δεδομένα πιο αποδοτικά.
    *   **Διαδικασία (για σύνολο δεδομένων μεγέθους R):**
        1.  Για k = 1 έως R:
        2.  Κράτα το k-οστό δείγμα `(xk, yk)` στην άκρη (ως test set ενός στοιχείου).
        3.  Εκπαίδευσε το μοντέλο στα υπόλοιπα R-1 δείγματα.
        4.  Υπολόγισε το σφάλμα πρόβλεψης για το k-οστό δείγμα που άφησες εκτός.
        5.  Επανάλαβε για όλα τα k.
        6.  Η τελική εκτίμηση σφάλματος είναι ο **μέσος όρος** των R ατομικών σφαλμάτων.
    *   **Παραδείγματα (Slides 29-31):** Εφαρμογή LOOCV για Γραμμική, Τετραγωνική και Join-the-dots παλινδρόμηση. Η Τετραγωνική δίνει το χαμηλότερο MSE (0.962).
    *   **Σύγκριση (Slide 32):**
        *   **Test Set:** (+) Φθηνό (μια εκπαίδευση). (-) Αναξιόπιστη εκτίμηση (υψηλή διακύμανση στην εκτίμηση σφάλματος).
        *   **LOOCV:** (+) Δεν σπαταλά δεδομένα (κάθε σημείο χρησιμοποιείται για έλεγχο). (+) Λιγότερη μεροληψία στην εκτίμηση σφάλματος. (-) Πολύ ακριβό υπολογιστικά (R εκπαιδεύσεις). (-) Μπορεί να έχει υψηλή διακύμανση (οι R εκπαιδεύσεις είναι πολύ όμοιες μεταξύ τους).
*   **k-Fold Cross-Validation (Slides 33-40):** Ένας συμβιβασμός μεταξύ Test Set και LOOCV.
    *   **Διαδικασία:**
        1.  Χώρισε τυχαία το σύνολο δεδομένων σε `k` ισομεγέθη, μη επικαλυπτόμενα υποσύνολα (folds) (π.χ., k=3 στο slide 33).
        2.  Για κάθε fold `i` (από 1 έως k):
        3.  Χρησιμοποίησε το fold `i` ως **σύνολο ελέγχου (validation set)**.
        4.  Χρησιμοποίησε τα υπόλοιπα `k-1` folds ως **σύνολο εκπαίδευσης**.
        5.  Εκπαίδευσε το μοντέλο και υπολόγισε το σφάλμα στο validation fold `i`.
        6.  Η τελική εκτίμηση σφάλματος είναι ο **μέσος όρος** των `k` σφαλμάτων.
    *   **Παραδείγματα (Slides 37-39):** Εφαρμογή 3-fold CV για Γραμμική, Τετραγωνική και Join-the-dots παλινδρόμηση. Η Τετραγωνική δίνει πάλι το χαμηλότερο μέσο MSE (1.11).
    *   **Σύγκριση (Slide 40):**
        *   **k-fold (π.χ., 10-fold):** (+) Πιο αξιόπιστη εκτίμηση σφάλματος από το test set. (+) Χρησιμοποιεί τα δεδομένα πιο αποδοτικά. (-) Πιο ακριβό από το test set (k εκπαιδεύσεις). (-) Λίγο λιγότερο αποδοτικό από LOOCV στη χρήση δεδομένων. (k=R δίνει LOOCV).
        *   Συνήθεις τιμές για k: 5 ή 10.

---

##**3. Μετρικές Απόδοσης για Παλινδρόμηση (Regression Performance Metrics) (Slides 9-16)**

Πώς μετράμε το "σφάλμα" σε προβλήματα παλινδρόμησης;

*   **Mean Absolute Error (MAE - Slide 10):** Ο μέσος όρος των **απόλυτων διαφορών** μεταξύ πραγματικών (`y`) και προβλεπόμενων (`ŷ`) τιμών: `MAE = (1/n) * Σ |yi - ŷi|`.
    *   Εύκολο στην ερμηνεία (μέσο απόλυτο σφάλμα).
    *   Πιο ανθεκτικό σε outliers σε σχέση με το MSE.
    *   Μη παραγωγίσιμο στο 0, που το καθιστά λιγότερο βολικό για κάποιες μεθόδους βελτιστοποίησης (π.χ. Gradient Descent).
*   **Mean Squared Error (MSE - Slide 11):** Ο μέσος όρος των **τετραγώνων των διαφορών**: `MSE = (1/n) * Σ (yi - ŷi)²`. (Αυτό χρησιμοποιήθηκε και στην OLS).
    *   "Τιμωρεί" τα μεγάλα σφάλματα περισσότερο λόγω του τετραγώνου.
    *   Ευαίσθητο σε outliers.
    *   Παραγωγίσιμο, βολικό για βελτιστοποίηση.
    *   Οι μονάδες του είναι το τετράγωνο των μονάδων του Υ.
*   **Root Mean Squared Error (RMSE - Slide 12):** Η **τετραγωνική ρίζα του MSE**: `RMSE = sqrt(MSE)`.
    *   Επαναφέρει τη μετρική στις αρχικές μονάδες του Υ, κάνοντας την ερμηνεία ευκολότερη.
    *   Ίδιες ιδιότητες ευαισθησίας σε outliers με το MSE.
    *   **Normalized RMSE (NRMSE):** Μερικές φορές το RMSE κανονικοποιείται (π.χ., διαιρώντας με την τυπική απόκλιση ή το εύρος των Υ) για ευκολότερη σύγκριση μεταξύ datasets με διαφορετικές κλίμακες.
*   **R-squared (R²) / Συντελεστής Προσδιορισμού (Coefficient of Determination - Slide 13):** Μετρά το **ποσοστό της διακύμανσης** της μεταβλητής απόκρισης Υ που **εξηγείται** από το μοντέλο.
    *   `R² = 1 - (RSS / TSS)`
        *   `RSS (Residual Sum of Squares) = Σ (yi - ŷi)²` (Άθροισμα Τετραγώνων Υπολοίπων).
        *   `TSS (Total Sum of Squares) = Σ (yi - ȳ)²` (Συνολικό Άθροισμα Τετραγώνων - διακύμανση γύρω από τον μέσο όρο).
    *   Τιμές: Συνήθως μεταξύ 0 και 1. Τιμή κοντά στο 1 σημαίνει ότι το μοντέλο εξηγεί μεγάλο μέρος της μεταβλητότητας του Υ. Τιμή κοντά στο 0 σημαίνει ότι το μοντέλο δεν εξηγεί καλά τη μεταβλητότητα. *Μπορεί* να πάρει αρνητικές τιμές αν το μοντέλο είναι χειρότερο από την πρόβλεψη απλά με τον μέσο όρο `ȳ`.
    *   Αυξάνεται (ή παραμένει ίδια) πάντα όταν προσθέτουμε νέες μεταβλητές στο μοντέλο, ακόμα κι αν είναι άσχετες (γι' αυτό υπάρχει και το Adjusted R²).
*   **Σύνοψη Ιδιοτήτων (Slide 14):** Πότε προτιμάμε MAE vs MSE/RMSE, ευαισθησία σε outliers, χρησιμότητα R².
*   **Mean Absolute Percentage Error (MAPE - Slide 15):** Μέσο απόλυτο ποσοστιαίο σφάλμα: `MAPE = (1/n) * Σ |(yi - ŷi) / yi| * 100%`.
    *   Εκφράζει το σφάλμα ως ποσοστό της πραγματικής τιμής.
    *   (+) Καλή ερμηνεία. Ανθεκτικό σε outliers.
    *   (-) Δεν ορίζεται αν κάποιο `yi = 0`.
    *   (-) Μπορεί να πάρει τεράστιες τιμές αν κάποιο `yi` είναι πολύ κοντά στο 0.
    *   (-) Μεροληπτεί προς προβλέψεις που είναι *μικρότερες* από τις πραγματικές τιμές.
*   **Mean Percentage Error (MPE - Slide 16):** Όπως το MAPE, αλλά *χωρίς* την απόλυτη τιμή: `MPE = (1/n) * Σ [(yi - ŷi) / yi] * 100%`.
    *   Δεν μετρά το μέγεθος του σφάλματος (θετικά και αρνητικά σφάλματα αλληλοαναιρούνται).
    *   Χρήσιμο για να δείξει αν το μοντέλο έχει **συστηματική τάση (bias)** να υπερεκτιμά (MPE > 0) ή να υποεκτιμά (MPE < 0).

---

##**4. Μετρικές Απόδοσης για Ταξινόμηση (Classification Performance Metrics) (Slides 41-58)**

Εστιάζουμε σε δυαδική ταξινόμηση (positive/negative class).

*   **Γιατί Μετρικές; (Slide 42):** Η συνάρτηση κόστους εκπαίδευσης δεν είναι πάντα ο τελικός στόχος. Οι μετρικές βοηθούν να μετρήσουμε επιχειρησιακούς στόχους, να οργανώσουμε την προσπάθεια, να δούμε την πρόοδο και να κάνουμε debugging (π.χ. bias vs variance).
*   **Score-Based Models (Slides 43-44):** Πολλά μοντέλα (π.χ. Logistic Regression) δίνουν ένα "σκορ" (συνήθως στο [0,1]) που εκφράζει την πεποίθηση ότι ένα δείγμα ανήκει στη θετική κλάση.
    *   Για να μετατρέψουμε το σκορ σε απόφαση (κατηγορία), χρειαζόμαστε ένα **κατώφλι (threshold)**.
*   **Κατώφλι & Πίνακας Σύγχυσης (Threshold & Confusion Matrix - Slides 45-51):**
    *   Επιλέγοντας ένα κατώφλι (π.χ., 0.5), ταξινομούμε τα δείγματα σε Positive/Negative.
    *   Συγκρίνοντας τις προβλέψεις με τις πραγματικές ετικέτες, φτιάχνουμε τον **Πίνακα Σύγχυσης (Confusion Matrix)**:
        *   **True Positives (TP):** Πραγματικά Θετικά που προβλέφθηκαν ως Θετικά.
        *   **True Negatives (TN):** Πραγματικά Αρνητικά που προβλέφθηκαν ως Αρνητικά.
        *   **False Positives (FP) / Type I Error:** Πραγματικά Αρνητικά που προβλέφθηκαν ως Θετικά.
        *   **False Negatives (FN) / Type II Error:** Πραγματικά Θετικά που προβλέφθηκαν ως Αρνητικά.
    *   Ιδιότητες: Τα σύνολα των στηλών (πραγματικές κλάσεις) είναι σταθερά. Οι διαγώνιοι (TP, TN) θέλουμε να είναι "βαριές", οι εκτός διαγωνίου (FP, FN) "ελαφριές".
*   **Βασικές Μετρικές (Slides 52-56):** (Υπολογίζονται από τον πίνακα σύγχυσης για ένα *συγκεκριμένο* κατώφλι).
    *   **Accuracy:** `(TP + TN) / Total`. Συνολική ορθότητα.
    *   **Precision:** `TP / (TP + FP)`. Από όσα προβλέφθηκαν Θετικά, πόσα ήταν όντως Θετικά;
    *   **Recall / Sensitivity:** `TP / (TP + FN)`. Από όσα ήταν όντως Θετικά, πόσα βρήκαμε;
    *   **Specificity / Negative Recall:** `TN / (TN + FP)`. Από όσα ήταν όντως Αρνητικά, πόσα βρήκαμε;
    *   **F1-Score:** `2 * (Precision * Recall) / (Precision + Recall)`. Αρμονικός μέσος Precision και Recall.
*   **Αλλαγή Κατωφλίου (Changing Threshold - Slides 57-58):**
    *   Αλλάζοντας το κατώφλι, αλλάζουν οι τιμές TP, TN, FP, FN και άρα όλες οι μετρικές.
    *   Μπορούμε να δοκιμάσουμε πολλά πιθανά κατώφλια ("Threshold Scanning", Slide 58) και να δούμε πώς μεταβάλλονται οι μετρικές. (Αυτό οδηγεί σε καμπύλες όπως η ROC και η Precision-Recall).
    *   Ο αριθμός των ουσιαστικά διαφορετικών κατωφλίων είναι (# δείγματα + 1).

---

##**5. Μετρικές Απόδοσης για Συσταδοποίηση (Clustering Performance Metrics) (Slides 59-65)**

Η αξιολόγηση της συσταδοποίησης είναι πιο δύσκολη γιατί δεν έχουμε "σωστές" ετικέτες (ground truth), εκτός αν χρησιμοποιούμε τεχνητά δεδομένα ή δεδομένα όπου οι πραγματικές ομάδες είναι γνωστές (κάτι που σπανίζει στην πράξη).

*   **Χωρίς Ground Truth (Internal Evaluation - Slide 61):** Αξιολογούμε πόσο "καλές" είναι οι συστάδες με βάση εγγενή κριτήρια συνοχής και διαχωρισμού.
    *   **Inertia (Within-Cluster Sum of Squares - WCSS):** Το άθροισμα των τετραγωνικών αποστάσεων κάθε σημείου από το κεντροειδές της συστάδας του. *Χαμηλότερη* τιμή υποδηλώνει πιο συμπαγείς συστάδες. (Προσοχή: Η τιμή τείνει στο 0 καθώς ο αριθμός των clusters αυξάνεται, μπορεί να οδηγήσει σε overfitting).
    *   **Silhouette Coefficient:** Μετρά πόσο καλά ταιριάζει κάθε σημείο στη συστάδα του σε σύγκριση με τις γειτονικές συστάδες. Τιμές από -1 (κακή) έως +1 (εξαιρετική). *Υψηλότερες* τιμές είναι καλύτερες.
    *   **Davies-Bouldin Index (DBI):** Μετρά την ομοιότητα (π.χ. απόσταση κεντροειδών, διασπορά) κάθε συστάδας με την πιο "όμοιά" της άλλη συστάδα. *Χαμηλότερες* τιμές είναι καλύτερες (καλύτερος διαχωρισμός, συμπάγεια).
    *   **Calinski-Harabasz Index (Variance Ratio Criterion):** Συγκρίνει τη διακύμανση *μεταξύ* των συστάδων με τη διακύμανση *εντός* των συστάδων. *Υψηλότερες* τιμές υποδηλώνουν πιο διακριτές, καλά διαχωρισμένες συστάδες.
*   **Παραδείγματα Αξιολόγησης (Slides 62-65):** Εμφανίζονται οι τιμές των παραπάνω μετρικών για αποτελέσματα K-Means και DBSCAN σε ένα σύνολο δεδομένων, για διαφορετικές παραμέτρους των αλγορίθμων. Παρατηρούμε πώς αλλάζουν οι μετρικές ανάλογα με τον αριθμό clusters (K-Means) ή τις παραμέτρους eps/min_samples (DBSCAN).

---

##**6. Μελέτες Περίπτωσης (Case Studies)**

*   **Παλινδρόμηση: Μοντελοποίηση Εισχώρησης Θάλασσας στην Κάλυμνο (Slides 66-73):**
    *   **Πρόβλημα:** Πρόβλεψη της αλατότητας (ή άλλου δείκτη εισχώρησης) σε πηγάδια με βάση γεωλογικά/υδρολογικά δεδομένα, πιθανώς και χρονοσειρές.
    *   **Μοντέλα:** Χρησιμοποιούνται προηγμένα μοντέλα για χρονοσειρές: RNN, LSTM, GRU (slide 69).
    *   **Αξιολόγηση:** Μετριέται το RMSE.
        *   Slide 70: Απόδοση διαφορετικών αρχιτεκτονικών (LSTM, GRU, κλπ.) κατά μέσο όρο σε 6 folds στο test set.
        *   Slide 71: Μέση απόδοση ανά πηγάδι (well id).
        *   Slide 72: Αναλυτική απόδοση (Train/Val/Test) για 2 πηγάδια. (Ερώτηση: Τι πρόβλημα έχουν τα plots; Πιθανώς ο άξονας Υ να μην είναι ίδιος, κάνοντας τη σύγκριση δύσκολη, ή τα χρώματα Train/Val/Test να μην είναι συνεπή).
        *   Slide 73: Επίδραση του "παραθύρου" εισόδου (πλήθος προηγούμενων χρονικών βημάτων) στην απόδοση. **Συμπέρασμα:** Δεν υπάρχει σαφής νικητής ή τάση, είναι πολύπλοκο.
*   **Ταξινόμηση: Σημασιολογική Κατάτμηση Υπερφασματικών Εικόνων (Slides 74-80):**
    *   **Πρόβλημα:** Να ταξινομηθεί *κάθε pixel* μιας υπερφασματικής εικόνας σε μια κατηγορία (π.χ., δρόμος, κτίριο, δέντρο, νερό). (Slide 75).
    *   **Προσέγγιση (Slide 76):**
        1.  Λίγα εικονοστοιχεία έχουν ετικέτες (annotations).
        2.  Χρησιμοποιείται ένα Νευρωνικό Δίκτυο (NN) για να μετατρέψει "κομμάτια" (patches) της εικόνας γύρω από κάθε pixel σε διανύσματα χαρακτηριστικών (embeddings).
        3.  Δημιουργείται ένας γράφος όπου οι κόμβοι είναι τα pixels (με τα embeddings τους).
        4.  Χρησιμοποιείται μια μέθοδος διάδοσης ετικετών (label propagation) στον γράφο για να ταξινομηθούν και τα pixels χωρίς αρχική ετικέτα, βασιζόμενοι στις σχέσεις τους με τα ετικετοποιημένα.
    *   **Προεπεξεργασία (Slide 77):** Βήματα όπως δημιουργία patches, κανονικοποίηση, δημιουργία train/test, εκπαίδευση NN, προβολή σε χώρο χαμηλότερης διάστασης.
    *   **Datasets (Slide 78):** Παραδείγματα (Botswana, Pavia, Salinas).
    *   **Αξιολόγηση:** Μετριέται η Accuracy και το F1 score.
        *   Slide 79: Επίδραση του πλήθους των αρχικών δειγμάτων εκπαίδευσης ανά κατηγορία. **Συμπέρασμα:** Περισσότερα δεδομένα δεν οδηγούν *πάντα* σε καλύτερη απόδοση (π.χ., στο PaviaU η απόδοση πέφτει από 20 σε 30 δείγματα).
        *   Slide 80: Επίδραση συνδυασμών παραμέτρων (διάσταση προβολής, rank για label propagation). **Συμπέρασμα:** Κανένας συνδυασμός δεν υπερτερεί *πάντα* σε όλα τα datasets.

---

##**7. Βελτιστοποίηση Υπερπαραμέτρων (Hyperparameter Tuning) (Slides 82, 102, 109-125)**

*   **Διαφορά Παραμέτρων & Υπερπαραμέτρων (Slides 102, 110, 111):**
    *   **Παράμετροι Μοντέλου:** Οι τιμές που μαθαίνει το μοντέλο *από τα δεδομένα* κατά την εκπαίδευση (π.χ., βάρη σε NN, συντελεστές `β` σε παλινδρόμηση).
    *   **Υπερπαράμετροι:** Οι ρυθμίσεις που ορίζουν την **αρχιτεκτονική** ή τη **διαδικασία εκπαίδευσης** του μοντέλου και *ορίζονται από τον χρήστη πριν την εκπαίδευση*. Δεν μαθαίνονται απευθείας από τα δεδομένα. (π.χ., το `k` στο kNN, το βάθος ενός decision tree, ο ρυθμός μάθησης, η παράμετρος `λ` στην κανονικοποίηση).
*   **Ανάγκη Βελτιστοποίησης:** Πρέπει να βρούμε τις "καλύτερες" τιμές για τις υπερπαραμέτρους για να πετύχουμε τη βέλτιστη απόδοση του μοντέλου.
*   **Διαδικασία (Slide 111):** Ορισμός μοντέλου -> Ορισμός εύρους τιμών για υπερπαραμέτρους -> Ορισμός μεθόδου δειγματοληψίας τιμών -> Ορισμός κριτηρίου αξιολόγησης -> Ορισμός μεθόδου cross-validation.
*   **Παραδείγματα Υπερπαραμέτρων (Slide 112):** Για Decision Tree, kNN, Logistic Regression, SVM.
*   **Μέθοδοι Βελτιστοποίησης (Slide 113):**
    *   **Manual Search (Slide 114):** Δοκιμή με βάση την εμπειρία. Απλό, αλλά χρονοβόρο και όχι απαραίτητα βέλτιστο.
    *   **Grid Search (Slide 115):** Δοκιμάζει *όλους* τους συνδυασμούς από ένα προκαθορισμένο πλέγμα τιμών. Εγγυάται ότι θα βρει τον καλύτερο συνδυασμό *μέσα στο πλέγμα*, αλλά γίνεται υπολογιστικά πανάκριβο αν υπάρχουν πολλές υπερπαράμετροι ή μεγάλο εύρος τιμών.
    *   **Randomized Search (Slides 116-117):** Δοκιμάζει *τυχαίους* συνδυασμούς από το εύρος τιμών. Συνήθως βρίσκει καλά αποτελέσματα πολύ πιο γρήγορα από το Grid Search, ειδικά όταν λίγες υπερπαράμετροι είναι πραγματικά σημαντικές (Slide 117). Δεν εγγυάται το βέλτιστο.
    *   **Bayesian Optimization (Slides 118-125):** Πιο "έξυπνη" προσέγγιση.
        1.  Ξεκινά με λίγες τυχαίες δοκιμές.
        2.  Χτίζει ένα **πιθανοτικό μοντέλο (surrogate model)** (π.χ., Gaussian Process) που προσπαθεί να προσεγγίσει την (άγνωστη) πραγματική συνάρτηση απόδοσης (objective function) με βάση τις δοκιμές που έχουν γίνει (Slide 121).
        3.  Χρησιμοποιεί μια **συνάρτηση απόκτησης (acquisition function)** για να αποφασίσει ποιον συνδυασμό υπερπαραμέτρων να δοκιμάσει στη συνέχεια, ισορροπώντας μεταξύ εξερεύνησης (περιοχές με μεγάλη αβεβαιότητα) και εκμετάλλευσης (περιοχές που φαίνονται καλές) (Slide 122).
        4.  Ενημερώνει το surrogate model με το νέο αποτέλεσμα και επαναλαμβάνει (Slide 123).
        5.  (+) Πιο αποδοτικό σε δείγματα (χρειάζεται λιγότερες δοκιμές) από Grid/Random Search.
        6.  (-) Πιο πολύπλοκο στην υλοποίηση και κατανόηση. Μπορεί να είναι ακριβό αν το χτίσιμο του surrogate model είναι δύσκολο.
*   **Παράδειγμα: Επιλογή k στο kNN (Slides 107-108):** Δείχνει πώς το σφάλμα στο training και validation set αλλάζει καθώς αλλάζει το `k`. Βρίσκουμε το "γόνατο" (elbow) της καμπύλης validation error για να επιλέξουμε το καλύτερο `k` (που ισορροπεί bias/variance).

---

##**8. Τελικές Σκέψεις (Slide 81)**

*   Υπάρχουν πολλές εναλλακτικές μετρικές απόδοσης για κάθε τύπο προβλήματος.
*   Η επιλογή της κατάλληλης μετρικής εξαρτάται από τον στόχο του προβλήματος.
*   Διαφορετικές μετρικές μπορεί να υποδεικνύουν διαφορετικά μοντέλα ως "καλύτερα".
*   Οι υπερπαράμετροι επηρεάζουν σημαντικά τα αποτελέσματα. Η επιλογή τους πρέπει να γίνεται προσεκτικά (tuning).
