# **Προβλήματα Παλινδρόμησης**

## **Εισαγωγή & Παράδειγμα: Δεδομένα Διαφήμισης (Διαφάνειες 1-3)**

- **Θέμα:** Προβλήματα Παλινδρόμησης (Regression problems).
- **Υπότιτλος:** "Πέρα από τους Αριθμούς: Κατανόηση της Δυναμικής της Παλινδρόμησης".
- **Στόχος:** Κατανόηση του πώς μπορούμε να προβλέψουμε μια συνεχή μεταβλητή.
- **Παράδειγμα (Διαφ. 2): Πρόβλεψη Πωλήσεων (Sales) βάσει Διαφημιστικού Προϋπολογισμού.**
  - Δεδομένα από 200 διαφορετικές αγορές.
  - Μεταβλητές: Προϋπολογισμός TV, Radio, Newspaper (σε χιλιάδες δολλάρια) και Πωλήσεις (σε εκατομμύρια δολλάρια).
  - **Το πρόβλημα:** Πώς μπορούμε να προβλέψουμε την μεταβλητή "Πωλήσεις" χρησιμοποιώντας τις μεταβλητές προϋπολογισμού;
- **Μεταβλητές Απόκρισης (Response) vs. Προβλεπτικές (Predictor) (Διαφ. 3):**
  - **Μεταβλητή Απόκρισης (response variable):** Η μεταβλητή που προσπαθούμε να προβλέψουμε (εδώ: "sales").
    - Άλλες ονομασίες: Outcome, Εξαρτημένη μεταβλητή (Dependent variable).
    - Σύμβολο: `Y`.
  - **Προβλεπτικές Μεταβλητές (p predictors):** Οι μεταβλητές που χρησιμοποιούμε για να κάνουμε την πρόβλεψη (εδώ: "TV", "radio", "newspaper").
    - Άλλες ονομασίες: Χαρακτηριστικά (Features), Συμμεταβλητές (Covariates).
    - Σύμβολο: `X`.
  - `n` παρατηρήσεις (observations): Ο αριθμός των δειγμάτων (εδώ: 200 αγορές).

## **Βασικές Έννοιες (Διαφάνειες 4-6)**

- **Πραγματική vs. Στατιστικό Μοντέλο (True vs. Statistical Model) (Διαφ. 5):**
  - Υποθέτουμε ότι η μεταβλητή απόκρισης `Y` σχετίζεται με τις προβλεπτικές μεταβλητές `X` μέσω μιας άγνωστης συνάρτησης `f` της μορφής:
    `Y = f(X) + ε`
    - `f`: Η άγνωστη συνάρτηση που εκφράζει τον υποκείμενο κανόνα που συνδέει το `Y` με το `X`.
    - `ε`: Ο τυχαίος όρος (άσχετος με το `X`) που αντιπροσωπεύει τη διαφορά του `Y` από τον κανόνα `f(X)` (θόρυβος, μη παρατηρούμενοι παράγοντες).
  - Ένα **στατιστικό μοντέλο** είναι οποιοσδήποτε αλγόριθμος που εκτιμά την `f`.
  - Συμβολίζουμε την εκτιμώμενη συνάρτηση ως `f̂` (f-hat).
- **Διευκρινίσεις (Διαφ. 6):**
  - Σε ένα πραγματικό σενάριο (π.χ., `f(x) = 0.25x + 0.4`), **δεν έχουμε αυτή την πληροφορία**.
  - **Δεν γνωρίζουμε καν αν έχουμε ένα γραμμικό μοντέλο** στην πραγματικότητα.
  - Οι **πραγματικές τιμές (actual values)** που παράγονται από την `f(X)` (χωρίς το `ε`) **δεν είναι παρατηρήσιμες**. Ακόμα και ο καλύτερος εξοπλισμός θα έχει κάποιο θόρυβο.
  - Παρατηρούμε μόνο τα σημεία δεδομένων (π.χ., τα πορτοκαλί σημεία στο γράφημα) που περιλαμβάνουν τον θόρυβο `ε`.
  - Βάσει αυτών των παρατηρήσεων, πρέπει να κάνουμε κάποιες **υποθέσεις** και να βρούμε έναν κατάλληλο προσεγγιστή `f̂`.

## **Εστίαση στο Πρόβλημα & Απλές Προσεγγίσεις (Διαφάνειες 7-11)**

- **Εστίαση στον Προϋπολογισμό TV (Διαφ. 8):** Για απλοποίηση, εξετάζουμε μόνο τη σχέση μεταξύ προϋπολογισμού TV (`x`) και πωλήσεων (`y`).
- **Πώς Βρίσκουμε το `f̂(x)`; (Διαφ. 9-10):** Ποια είναι η τιμή του `y` για ένα δεδομένο `x`;
- **Απλή Ιδέα: Εκτίμηση με τη Μέση Τιμή (Διαφ. 11):**
  - `f̂(x) = (1/n) Σ yᵢ` (η μέση τιμή όλων των παρατηρούμενων πωλήσεων).
  - Αυτό σημαίνει ότι ανεξάρτητα από την τιμή του `x` (προϋπολογισμός TV), η πρόβλεψη για το `y` (πωλήσεις) θα είναι πάντα η ίδια (η μέση τιμή).
  - **Αυτό δεν είναι χρήσιμο!** Δεν λαμβάνει υπόψη τη σχέση μεταξύ `x` και `y`.

## **Ορολογία: Πρόβλεψη vs. Συμπερασματολογία (Prediction/Estimation) (Διαφάνεια 12)**

- Για κάποια προβλήματα, αυτό που έχει σημασία είναι η εύρεση του `f̂`, της εκτίμησής μας για την `f`. Αυτά ονομάζονται προβλήματα **συμπερασματολογίας (inference problems)** (δηλ., θέλουμε να καταλάβουμε τη σχέση).
- Όταν θέλουμε να **προβλέψουμε (predict)** μια τιμή για τη μεταβλητή απόκρισης, συμβολίζουμε την προβλεπόμενη τιμή ως:
  `ŷᵢ = f̂(xᵢ)`
- Πρακτικά, προσπαθούμε το `ŷᵢ ≈ yᵢ`. Αυτό μπορεί να επιτευχθεί **χωρίς να έχουμε μια συγκεκριμένη (κλειστή) μορφή για το `f̂`**.

## **Ένα Απλό Μοντέλο Πρόβλεψης: Πλησιέστερος Γείτονας (Διαφάνειες 13-14)**

- **Για ένα δεδομένο `x_q` (σημείο ερωτήματος) (Διαφ. 13):**
  1.  Βρίσκουμε τις αποστάσεις `D(x_q, xᵢ)` προς όλα τα άλλα (παρατηρούμενα) σημεία `xᵢ`.
  2.  Βρίσκουμε τον **πλησιέστερο γείτονα (nearest neighbor)**, έστω `(x_p, y_p)`.
  3.  Προβλέπουμε `ŷ_q = y_p` (η πρόβλεψη για το `x_q` είναι η τιμή `y` του πλησιέστερου γείτονά του).
- **Εφαρμογή για "όλα" τα `x` (Διαφ. 14):** Το γράφημα δείχνει μια βηματική συνάρτηση, όπου η πρόβλεψη για ένα `x` είναι η τιμή `y` του πλησιέστερου παρατηρούμενου σημείου.

## **Επέκταση του Μοντέλου Πρόβλεψης: k-Πλησιέστεροι Γείτονες (k-NN) (Διαφάνειες 15-18)**

- **Για ένα δεδομένο `x_q` (Διαφ. 15):**
  1.  Βρίσκουμε τις αποστάσεις προς όλα τα άλλα σημεία.
  2.  Βρίσκουμε τους **k-πλησιέστερους γείτονες**.
  3.  Προβλέπουμε `ŷ_q = (1/k) Σ yᵢ` (η πρόβλεψη είναι ο **μέσος όρος** των τιμών `y` των k-πλησιέστερων γειτόνων).
- **Οπτικοποίηση για διαφορετικά `k` (Διαφ. 16-17):**
  - `k=1`: Ίδιο με τον πλησιέστερο γείτονα (πολύ "θορυβώδες").
  - `k=3`, `k=8`: Η γραμμή πρόβλεψης γίνεται πιο ομαλή.
  - Καθώς το `k` αυξάνεται, οι προβλέψεις τείνουν προς τη μέση τιμή του `y` (Διαφ. 17). **Αυτό δεν είναι χρήσιμο** αν το `k` γίνει πολύ μεγάλο.
- **Χρήση k-NN για Παλινδρόμηση (Διαφ. 18):**
  - **Ναι, μόλις χρησιμοποιήσαμε k-NN για παλινδρόμηση!** Είναι ένας διαισθητικός τρόπος πρόβλεψης μιας ποσοτικής μεταβλητής απόκρισης.
  - Άλλες τεχνικές για παλινδρόμηση: Δέντρα απόφασης, παλινδρόμηση υποστήριξης διανυσμάτων (SVR), Naïve Bayes παλινδρόμηση.

## **Ανάγκη για Κλειστή Μορφή του `f̂`: Γραμμικά Μοντέλα (Διαφάνειες 19-21)**

- **Τι γίνεται αν χρειαζόμαστε μια κλειστή (μαθηματική) μορφή για το `f̂`; (Διαφ. 19):** Ας ξεκινήσουμε με Γραμμικά Μοντέλα.
- **Περιορισμοί των Μη-Παραμετρικών Προσεγγίσεων (όπως k-NN) (Διαφ. 20):**
  - Δύσκολο να απαντηθούν ερωτήσεις όπως: "πόσο περισσότερες πωλήσεις αναμένουμε αν διπλασιάσουμε τον προϋπολογισμό διαφήμισης TV;"
  - Η προσέγγιση k-NN **δεν είναι πολύ χρήσιμη** για την απάντηση τέτοιων ερωτήσεων συμπερασματολογίας.
- **Γραμμικά Μοντέλα (Διαφ. 21):**
  - Υποθέτουμε μια **γραμμική μορφή** για την `f`:
    `Y = f(X) + ε = β₁X + β₀ + ε`
  - Τότε η εκτίμησή μας είναι:
    `Ŷ = f̂(X) = β̂₁X + β̂₀`
  - όπου `β̂₁` και `β̂₀` είναι εκτιμήσεις των πραγματικών συντελεστών `β₁` και `β₀`.
  - Και οι `β̂₁`, `β̂₀` μπορούν να υπολογιστούν χρησιμοποιώντας τις παρατηρήσεις.
  - **Προειδοποίηση:** Συνήθως ένα γραμμικό μοντέλο **δεν είναι** καλή επιλογή, αλλά βοηθά στην κατανόηση βασικών εννοιών.

## **Εκτίμηση των Συντελεστών Παλινδρόμησης (Διαφάνειες 22-30)**

- **Δεδομένο Σύνολο Δεδομένων (Διαφ. 22):** Έχουμε ένα σύνολο παρατηρήσεων (x, y).
- **Ερώτηση: Ποια Γραμμή Είναι η Καλύτερη; (Διαφ. 23-26):**
  - Δοκιμάζουμε διάφορες γραμμές.
  - Για να αποφασίσουμε, πρώτα υπολογίζουμε τα **υπόλοιπα (residuals)**: οι κάθετες αποστάσεις μεταξύ των παρατηρούμενων σημείων και της γραμμής.
- **Χρήση Μέσου Τετραγωνικού Σφάλματος (MSE) ως Συνάρτηση Απώλειας (Διαφ. 27):**
  - `L(β₀, β₁) = (1/n) Σ (yᵢ - ŷᵢ)² = (1/n) Σ [yᵢ - (β₁xᵢ + β₀)]²`
  - Επιλέγουμε `β̂₀` και `β̂₁` ώστε να **ελαχιστοποιούν** τα προβλεπτικά σφάλματα, δηλαδή να ελαχιστοποιούν τη συνάρτηση απώλειας.
  - `β̂₀, β̂₁ = argmin_{β₀,β₁} L(β₀, β₁)`
- **Εκτίμηση με "Ωμή Βία" (Brute Force) (Διαφ. 28):**
  - Υπολογισμός της συνάρτησης απώλειας για κάθε πιθανό `β₀` και `β₁`. Επιλογή του ζεύγους που δίνει την ελάχιστη απώλεια.
  - Παράδειγμα: Γράφημα της απώλειας για διάφορα `β₁` όταν το `β₀` είναι σταθερό στο 6. Το ελάχιστο είναι στο `β₁ = 0.044`.
  - **Αυτό δεν είναι καλή πρακτική** (υπολογιστικά ακριβό, μη πρακτικό).
- **Εκτίμηση με Ακριβή Μέθοδο (Μέθοδος Ελαχίστων Τετραγώνων - Least Squares) (Διαφ. 29):**
  - Παίρνουμε τις μερικές παραγώγους της `L` ως προς `β₀` και `β₁`, τις θέτουμε μηδέν, και λύνουμε το σύστημα εξισώσεων.
  - Αυτό δίνει **ρητούς τύπους (explicit formulae)** για τους `β̂₀` και `β̂₁`:
    - `β̂₁ = Σ(xᵢ - x̄)(yᵢ - ȳ) / Σ(xᵢ - x̄)²`
    - `β̂₀ = ȳ - β̂₁x̄`
    - όπου `x̄` και `ȳ` είναι οι μέσοι όροι των δειγμάτων.
  - Η γραμμή `Ŷ = β̂₁X + β̂₀` ονομάζεται **γραμμή παλινδρόμησης (regression line)**.
  - **Δυστυχώς, οι ρητές λύσεις είναι σπάνιες περιπτώσεις** για πιο πολύπλοκα μοντέλα.
- **Αναλυτική Παραγώγιση των Συντελεστών Ελαχίστων Τετραγώνων (Διαφ. 30):**
  - Δείχνει τα μαθηματικά βήματα για την παραγώγιση των τύπων για `β̂₀` και `β̂₁`.

## **Εκτίμηση των Συντελεστών Παλινδρόμησης: Κατάβαση Κλίσης (Gradient Descent) (Διαφάνειες 31-35)**

- **Μια πιο ευέλικτη μέθοδος (Διαφ. 31):**
  1.  Ξεκίνα από ένα τυχαίο σημείο (τυχαίες τιμές για `β₀`, `β₁`).
  2.  Καθόρισε ποια κατεύθυνση να κινηθείς για να μειώσεις την απώλεια.
  3.  Υπολόγισε την κλίση της συνάρτησης σε αυτό το σημείο. Κινήσου δεξιά αν η κλίση είναι αρνητική, αριστερά αν είναι θετική (δηλ., κινήσου αντίθετα στην κλίση).
  4.  Επανάλαβε.
- **Ερωτήσεις (Διαφ. 32):**
  - Ποια μαθηματική συνάρτηση περιγράφει την κλίση; **Η παράγωγος (Derivative)**.
  - Πώς λέμε στο μοντέλο πόσο να αλλάξει (μέγεθος βήματος); **Αν το βήμα είναι ανάλογο της κλίσης, τότε αποφεύγουμε την υπέρβαση του ελαχίστου (overshooting)**.
  - Πώς γενικεύουμε για περισσότερους από έναν προβλεπτικούς παράγοντες; **Παίρνουμε την παράγωγο ως προς κάθε συντελεστή και κάνουμε το ίδιο διαδοχικά (ή ταυτόχρονα για όλους).**
- **Τυπική Διαδικασία (Διαφ. 33):**
  - Συμβολισμός: `w = [β₀, β₁]` (διάνυσμα βαρών/συντελεστών).
  - Κίνηση σε ένα βήμα: `w_new = w_old + step`.
  - Κίνηση στην αντίθετη κατεύθυνση της παραγώγου και ανάλογα με την παράγωγο:
    `w_new = w_old - λ (dL/dw)` (όπου `λ` είναι ο ρυθμός μάθησης - learning rate).
  - Συμβατικός συμβολισμός: `w⁽ⁱ⁺¹⁾ = w⁽ⁱ⁾ - λ (dL/dw)`.
- **Σύνοψη Κατάβασης Κλίσης (Διαφ. 34):**
  - Αλγόριθμος βελτιστοποίησης πρώτης τάξης για εύρεση ελαχίστου.
  - Επαναληπτική μέθοδος.
  - Η απώλεια `L` μειώνεται στην κατεύθυνση της αρνητικής παραγώγου.
  - Ο ρυθμός μάθησης `λ` ελέγχει το μέγεθος του βήματος.
- **Θέματα προς Εξέταση (Διαφ. 35):**
  - Πρέπει ακόμα να υπολογίσουμε τις παραγώγους.
  - Πρέπει να ξέρουμε ποιος είναι ο ρυθμός μάθησης ή πώς να τον ορίσουμε.
  - Πρέπει να αποφύγουμε τα τοπικά ελάχιστα.
  - Η πλήρης συνάρτηση απώλειας περιλαμβάνει άθροιση όλων των ατομικών σφαλμάτων (μπορεί να είναι χιλιάδες παραδείγματα).

## **Προσοχή: Επίδραση των Δεδομένων στις Τιμές των Παραμέτρων (Διαφάνεια 36)**

- Οι παράμετροι του μοντέλου (π.χ., `β̂₁`, `β̂₀`) **εξαρτώνται από τα δεδομένα εκπαίδευσης**.
- Διαφορετικά δεδομένα εκπαίδευσης (π.χ., διαφορετικά "folds" σε cross-validation) οδηγούν σε διαφορετικές τιμές για τις παραμέτρους του μοντέλου.

**Πολλαπλή Παλινδρόμηση (Διαφάνειες 37-47)**

- **Τι γίνεται αν έχουμε πολλαπλές εισόδους και μία έξοδο; (Διαφ. 37):** Μιλάμε για προβλήματα πολλαπλής παλινδρόμησης. Ξεκινάμε με γραμμικά μοντέλα.
- **Δημιουργία ενός Ψεύτικου Συνόλου Δεδομένων (Dummy Dataset) (Διαφ. 38):**
  - Ένα γραμμικό μοντέλο της μορφής:
    `y = β₀ + β₁x₁ + β₂x₂ + ... + β₉x₉` (χωρίς δυνάμεις, εκθετικά κ.λπ. πάνω στα `xᵢ`).
  - Ορίζουμε αυθαίρετες τιμές για τους συντελεστές `βᵢ`. Κάποιοι `βᵢ` μπορεί να είναι 0, που σημαίνει ότι το αντίστοιχο `xᵢ` δεν συμβάλλει στην έξοδο `y`.
  - Οι πραγματικές τιμές που παίρνουμε είναι:
    `y = β₀ + β₁x₁ + ... + β₉x₉ + ε`
  - **Δεν γνωρίζουμε** ότι οι τιμές προέρχονται από ένα γραμμικό μοντέλο. Απλώς το **υποθέσαμε**.
- **Η Ευκολότερη Λύση και Σχετικά Προβλήματα (OLS) (Διαφ. 39):**
  - Υποθέτοντας ένα γραμμικό μοντέλο `Y = βX + ε` (όπου `X` είναι τώρα ένας πίνακας και `β` ένα διάνυσμα), με `ε ~ N(0, σ)`, σχηματίζουμε τη συνάρτηση απώλειας (συνήθως Άθροισμα Τετραγώνων των Υπολοίπων - RSS, που οδηγεί στο ίδιο αποτέλεσμα με το MSE για ελαχιστοποίηση):
    `L_OLS(β̂) = Σ ||yᵢ - xᵢᵀβ̂||²`
  - Θέλουμε να ελαχιστοποιήσουμε το `L_OLS(β̂)`. Υπάρχει μια **ρητή λύση**:
    `β̂_OLS = (XᵀX)⁻¹XᵀY`
  - Αυτό είναι γνωστό ως η λύση των **Συνήθων Ελαχίστων Τετραγώνων (Ordinary Least Squares - OLS)**.
  - Το OLS έχει πολλούς **περιορισμούς** που αποδίδονται στον **συμβιβασμό μεροληψίας/διακύμανσης (bias/variance trade-off)**.
- **Κατανόηση Μεροληψίας, Διακύμανσης και της Επίδρασής τους (Διαφ. 40-42):**
  - **Μεροληψία (Bias) (Διαφ. 40):** Οι υποκείμενες υποθέσεις που γίνονται από τα δεδομένα για την απλοποίηση της συνάρτησης στόχου. Υψηλή μεροληψία μπορεί να οδηγήσει σε **υποπροσαρμογή (underfitting)**.
  - **Διακύμανση (Variance) (Διαφ. 40):** Η ευαισθησία ενός μοντέλου σε μικρές διακυμάνσεις στο σύνολο δεδομένων. Υψηλή διακύμανση μπορεί να οδηγήσει σε **υπερπροσαρμογή (overfitting)**.
  - **Μεροληψία και Διακύμανση στο OLS (Διαφ. 41):**
    - `Bias(β̂) = E(β̂) - β`
    - `Var(β̂) = σ²(XᵀX)⁻¹`
    - Το σφάλμα του OLS μπορεί να αναλυθεί σε: `Bias² + Variance + Μη αναγώγιμο σφάλμα (σ²)`.
  - **Οπτικοποίηση Μεροληψίας-Διακύμανσης (Διαφ. 42):** Στόχοι που δείχνουν: Χαμηλή Διακύμανση/Χαμηλή Μεροληψία (ιδανικό), Χαμηλή Διακύμανση/Υψηλή Μεροληψία, Υψηλή Διακύμανση/Χαμηλή Μεροληψία, Υψηλή Διακύμανση/Υψηλή Μεροληψία.
- **Περιορισμοί της Λύσης OLS (Διαφ. 43):**
  - Η γραμμική παλινδρόμηση εστιάζει στη μεροληψία και υποφέρει από υψηλή διακύμανση.
  - Ασταθής, **πληθωρισμός διακύμανσης (variance inflation)** με μεγάλο αριθμό προβλεπτικών μεταβλητών, ειδικά αν είναι συσχετισμένες (multicollinearity).
  - Η αντιστροφή του `XᵀX` γίνεται προβληματική (αν δεν είναι πλήρους βαθμού).
  - **Πιθανή Λύση: Κανονικοποίηση (Regularization).**
    - Συρρίκνωση των συντελεστών, έλεγχος της πολυπλοκότητας.
    - "Δαμάστε" τη διακύμανση και σταθεροποιήστε το μοντέλο με Lasso, Ridge, Elastic Net.
- **Κανονικοποίηση [1/3]: Παλινδρόμηση Ridge (Ridge Regression) (Διαφ. 44):**
  - Ελαχιστοποιεί μια συνάρτηση απώλειας + **όρος ποινής L₂**:
    `L_Ridge = argmin_β (||Y - βX||² + λ||β||₂²)`
  - Προσθέτει `λI` (όπου `I` ο μοναδιαίος πίνακας) στις διαγωνίους του `XᵀX` για να καταστήσει δυνατή την αντιστροφή.
  - Ρητή λύση: `β̂ = (XᵀX + λI)⁻¹XᵀY`.
  - Συρρικνώνει τους συντελεστές κοντά στο μηδέν, αλλά **δεν τους μηδενίζει**.
- **Κανονικοποίηση [2/3]: Παλινδρόμηση Lasso (Lasso Regression) (Διαφ. 45):**
  - Ελαχιστοποιεί μια συνάρτηση απώλειας + **όρος ποινής L₁**:
    `L_Lasso = argmin_β (||Y - βX||² + λ||β||₁)`
  - **Δεν έχει ρητή λύση.** Μόνο προσεγγιστικοί αλγόριθμοι. Πιο υπολογιστικά εντατική.
  - Αν υπάρχουν δύο ή περισσότερες ισχυρά συσχετισμένες μεταβλητές, το LASSO επιλέγει μία από αυτές τυχαία (όχι καλό για ερμηνεία).
  - **Μπορεί να θέσει κάποιους συντελεστές ακριβώς ίσους με μηδέν** (κάνει επιλογή χαρακτηριστικών).
- **Κανονικοποίηση [3/3]: Παλινδρόμηση Elastic Net (Διαφ. 46):**
  - Ελαχιστοποιεί μια συνάρτηση απώλειας + **δύο όρους ποινής (L₁ και L₂)**:
    `L_ElasticNet = argmin_β (||Y - βX||² + λ₁||β||₂² + λ₂||β||₁)`
  - Όχι ρητή λύση.
  - Πιο υπολογιστικά εντατική.
  - Μπορεί να χειριστεί την **πολυσυγγραμμικότητα (multicollinearity)** καλύτερα από το Lasso.
  - Μειώνει την πολυπλοκότητα του μοντέλου καλύτερα από το Ridge.
- **Επίδραση της Προσέγγισης Παλινδρόμησης στους Συντελεστές του Μοντέλου (Διαφ. 47):**
  - Γράφημα που δείχνει τους πραγματικούς συντελεστές και τους εκτιμώμενους συντελεστές από OLS, Ridge, Lasso, Elastic Net.
  - Το Lasso μηδενίζει κάποιους συντελεστές. Το Ridge τους συρρικνώνει.
- **Γρήγορος Έλεγχος της Απόδοσης του Μοντέλου (Διαφ. 48):**
  - Διαγράμματα προβλεπόμενων vs πραγματικών τιμών για OLS, Ridge, Lasso, Elastic Net, τόσο για το σύνολο εκπαίδευσης (Train Set) όσο και για το σύνολο ελέγχου (Test Set).
  - Αναφέρεται το **RMSE (Root Mean Squared Error)**: χαμηλότερη τιμή = καλύτερο μοντέλο.

## **Μη-Γραμμικά Προβλήματα (Διαφάνειες 49-55)**

- **Τι γίνεται αν έχω ένα μη-γραμμικό πρόβλημα; (Διαφ. 49):** Υπάρχουν μερικά κόλπα.
- **Δημιουργία Νέου Ψεύτικου Συνόλου Δεδομένων (Μη-Γραμμικό) (Διαφ. 50):**
  - Ένα μη-γραμμικό μοντέλο της μορφής (π.χ., πολυωνυμικό):
    `y = β₀ + β₁x₁² + β₂x₂² + β₃x₁x₂ + β₄x₁ + β₅x₂` (περιέχει δυνάμεις και γινόμενα των `xᵢ`).
  - Ορίζουμε αυθαίρετες τιμές για τους `βᵢ`.
  - **Δεν γνωρίζουμε** ότι οι τιμές προέρχονται από ένα μη-γραμμικό μοντέλο. Απλώς **υποθέσαμε ότι έχουμε ένα πολυωνυμικό μοντέλο!**
- **Οπτικοποίηση των Νέων Δεδομένων (Διαφ. 51):** Δείχνει μια μη-γραμμική επιφάνεια.
- **Υποθέσεις Προβλήματος (Διαφ. 52):**
  - Σε αυτό το σενάριο, υποθέτουμε δύο (2) μοντέλα:
    1.  Ένα γραμμικό: `y = β₀ + β₁x₁ + β₂x₂`
    2.  Ένα πολυωνυμικό 2ου βαθμού: `y = β₀ + β₁x₁² + β₂x₂² + β₃x₁x₂ + β₄x₁ + β₅x₂`
  - **Εξαιρετικά σημαντικό:** Λύνουμε για τους `βᵢ`, `i = 1, ..., m`. Ανεξάρτητα από το μέγεθος του `m` (αριθμός όρων), η προσέγγιση είναι η ίδια (π.χ., μπορούμε να χρησιμοποιήσουμε OLS, Ridge κ.λπ. θεωρώντας τους πολυωνυμικούς όρους ως νέα χαρακτηριστικά).
- **Προσαρμοσμένες Έξοδοι Μοντέλου (Y), Σύνολα Εκπαίδευσης & Ελέγχου (Διαφ. 53-54):**
  - Σύγκριση της απόδοσης (RMSE) του Γραμμικού Μοντέλου (OLS, Ridge, Lasso, ElasticNet) με το Πολυωνυμικό Μοντέλο (OLS, Ridge, Lasso, ElasticNet) στα σύνολα εκπαίδευσης και ελέγχου.
  - **Γενική παρατήρηση:** Το πολυωνυμικό μοντέλο τείνει να έχει χαμηλότερο RMSE (καλύτερη απόδοση), ειδικά όταν τα δεδομένα έχουν πράγματι μη-γραμμική δομή.
- **Επίδραση της Προσέγγισης Παλινδρόμησης στους Συντελεστές του Πολυωνυμικού Μοντέλου (Διαφ. 55):**
  - Παρόμοια με τη διαφάνεια 47, αλλά για τους συντελεστές του πολυωνυμικού μοντέλου. Δείχνει πώς οι διαφορετικές μέθοδοι κανονικοποίησης επηρεάζουν τους εκτιμώμενους συντελεστές.

## **Συμπεράσματα (Διαφάνεια 56)**

- Η ανάλυση παλινδρόμησης είναι ένα ισχυρό εργαλείο για την κατανόηση και μοντελοποίηση σχέσεων μεταξύ μεταβλητών, επιτρέποντας τεκμηριωμένες προβλέψεις.
- Διακρίνοντας μεταξύ διαφορετικών τύπων παλινδρόμησης (γραμμική, πολυωνυμική, κανονικοποιημένη), μπορούμε να επιλέξουμε την καταλληλότερη τεχνική.
- Η προσεκτική ερμηνεία των αποτελεσμάτων (συντελεστές, μετρικές σφάλματος, διαγνωστικά μοντέλου) παρέχει πολύτιμες πληροφορίες.
- Αν η συγκεκριμένη προσέγγιση μοντέλου αποτύχει, μπορούμε πάντα να χρησιμοποιήσουμε πολλαπλούς άλλους προσεγγιστές (π.χ., kNN, SVR, Δέντρα).
