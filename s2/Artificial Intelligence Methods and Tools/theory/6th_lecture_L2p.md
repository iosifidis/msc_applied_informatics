# **Προβλήματα Παλινδρόμησης (Regression Problems)**.

---

## **1. Εισαγωγή & Πλαίσιο Προβλήματος (Slides 1-3)**

*   **Τι είναι η Παλινδρόμηση (Regression);** Είναι ένας τύπος προβλήματος μηχανικής μάθησης όπου ο στόχος είναι να προβλέψουμε μια **συνεχή ποσοτική τιμή** (όχι μια κατηγορία όπως στην ταξινόμηση).
*   **Παράδειγμα: Δεδομένα Διαφήμισης (Advertising Data Set - Slide 2)**
    *   Έχουμε δεδομένα για τις πωλήσεις (Sales) ενός προϊόντος σε 200 διαφορετικές αγορές.
    *   Για κάθε αγορά, γνωρίζουμε τα ποσά που δαπανήθηκαν για διαφήμιση σε Τηλεόραση (TV), Ραδιόφωνο (Radio) και Εφημερίδες (Newspaper).
    *   **Στόχος:** Να προβλέψουμε την τιμή της μεταβλητής `Sales` χρησιμοποιώντας τις τιμές των `TV`, `Radio`, και `Newspaper`.
*   **Ορολογία (Slide 3):**
    *   **Μεταβλητή Απόκρισης (Response Variable):** Η μεταβλητή που θέλουμε να προβλέψουμε (εδώ: `Sales`). Άλλες ονομασίες: Outcome, Dependent variable. Συνηθισμένο σύμβολο: `Y`.
    *   **Μεταβλητές Πρόβλεψης (Predictor Variables):** Οι μεταβλητές που χρησιμοποιούμε για να κάνουμε την πρόβλεψη (εδώ: `TV`, `Radio`, `Newspaper`). Άλλες ονομασίες: Features, Covariates, Independent variables. Συνηθισμένο σύμβολο: `X`.
    *   `n`: Αριθμός παρατηρήσεων (γραμμές δεδομένων).
    *   `p`: Αριθμός μεταβλητών πρόβλεψης (features).

---

## **2. Βασικές Έννοιες: Πραγματικότητα vs. Στατιστικό Μοντέλο (Slides 5-6)**

*   **Υπόθεση:** Υποθέτουμε ότι υπάρχει μια (άγνωστη) σχέση `f` που συνδέει τις μεταβλητές πρόβλεψης `X` με τη μεταβλητή απόκρισης `Y`, αλλά η σχέση αυτή επηρεάζεται από τυχαίο **θόρυβο (noise)** `ε`:
    `Y = f(X) + ε`
    *   `f`: Η πραγματική, υποκείμενη συνάρτηση που περιγράφει τη σχέση.
    *   `ε`: Ο τυχαίος όρος σφάλματος, που αντιπροσωπεύει παράγοντες που δεν μετράμε ή τυχαίες διακυμάνσεις. Είναι ασυσχέτιστος με το `X`.
*   **Στατιστικό Μοντέλο:** Είναι οποιοσδήποτε αλγόριθμος που προσπαθεί να **εκτιμήσει (estimate)** την άγνωστη συνάρτηση `f`. Συμβολίζουμε την εκτιμώμενη συνάρτηση ως `f̂` (f-hat).
*   **Σημαντική Διευκρίνιση (Slide 6):**
    *   Στην πράξη, **δεν γνωρίζουμε** την πραγματική `f`. (Στο παράδειγμα του slide 6, η `f(x) = 0.25x + 0.4` δίνεται μόνο για επεξήγηση, κανονικά είναι άγνωστη).
    *   Δεν γνωρίζουμε καν *αν* η σχέση είναι γραμμική ή κάποιας άλλης μορφής.
    *   Οι **πραγματικές τιμές `Y`** (που προκύπτουν από την `f` χωρίς θόρυβο) δεν είναι παρατηρήσιμες λόγω του θορύβου `ε`.
    *   Παρατηρούμε μόνο τα **ζεύγη `(X, Y)`** (τα πορτοκαλί σημεία στα γραφήματα).
    *   Βάσει αυτών των παρατηρήσεων, πρέπει να κάνουμε υποθέσεις και να βρούμε έναν κατάλληλο "προσεγγιστή" `f̂`.

---

## **3. Εκτίμηση `f̂`: Απλές Προσεγγίσεις (Slides 8-18)**

Εστιάζουμε προσωρινά στη σχέση `TV` -> `Sales`. Πώς θα προβλέψουμε τις πωλήσεις `y` για μια δεδομένη τιμή `x` του TV budget;

*   **Πρόβλεψη με τον Μέσο Όρο (Slide 11):** Η πιο απλή ιδέα: `f̂(x) = μέση τιμή όλων των παρατηρούμενων y`.
    *   **Μειονέκτημα:** Η πρόβλεψη είναι ίδια για *όλα* τα `x`. Δεν λαμβάνει υπόψη την τιμή του `x`, άρα **δεν είναι χρήσιμο**.
*   **Πρόβλεψη vs. Συμπερασματολογία (Prediction vs. Inference - Slide 12):**
    *   **Συμπερασματολογία (Inference):** Ο στόχος είναι να κατανοήσουμε τη σχέση μεταξύ `X` και `Y`. Θέλουμε να βρούμε την ίδια την `f̂` (π.χ., πόσο αυξάνονται οι πωλήσεις για κάθε $1000 αύξηση στο TV budget;).
    *   **Πρόβλεψη (Prediction):** Ο στόχος είναι να προβλέψουμε την τιμή του `Y` για νέες τιμές του `X`. Δεν μας ενδιαφέρει απαραίτητα η ακριβής μορφή της `f̂`, αρκεί οι προβλέψεις `ŷ = f̂(x)` να είναι κοντά στις πραγματικές `y` (`ŷ ≈ y`).
*   **k-Nearest Neighbors (kNN) για Παλινδρόμηση (Slides 13-18):**
    *   **Ναι, το kNN χρησιμοποιείται και για παλινδρόμηση! (Slide 18)**
    *   **Ιδέα:** Για να προβλέψουμε την τιμή `ŷq` για ένα νέο σημείο `xq`:
        1.  Βρίσκουμε την απόσταση του `xq` από όλα τα σημεία εκπαίδευσης `xi`.
        2.  Εντοπίζουμε τους `k` κοντινότερους γείτονες του `xq` στα δεδομένα εκπαίδευσης.
        3.  Η πρόβλεψη `ŷq` είναι ο **μέσος όρος** των τιμών `y` αυτών των `k` γειτόνων. (Slide 15).
        *   Η περίπτωση `k=1` σημαίνει ότι η πρόβλεψη είναι απλά η τιμή `y` του μοναδικού πλησιέστερου γείτονα (Slides 13-14).
    *   **Παρατηρήσεις (Slides 16-17):**
        *   Για `k=1`, η γραμμή πρόβλεψης ακολουθεί πολύ στενά τα δεδομένα (υψηλή διακύμανση, πιθανό overfitting).
        *   Καθώς το `k` αυξάνεται, η γραμμή πρόβλεψης γίνεται πιο ομαλή.
        *   Αν το `k` γίνει πολύ μεγάλο (π.χ., `k=n`), η πρόβλεψη τείνει στον γενικό μέσο όρο (όπως στο Slide 11), κάτι που πάλι δεν είναι χρήσιμο (υψηλή μεροληψία, underfitting).

---

## **4. Γραμμικά Μοντέλα (Linear Models) (Slides 19-30)**

*   **Κίνητρο:** Το kNN δεν μας δίνει μια σαφή, κλειστή μαθηματική μορφή (closed form) για την `f̂`. Αν θέλουμε να απαντήσουμε ερωτήματα όπως "πόσο αυξάνονται οι πωλήσεις αν διπλασιάσουμε το budget της TV;", χρειαζόμαστε ένα μοντέλο με σαφή δομή (Slide 20).
*   **Υπόθεση Απλού Γραμμικού Μοντέλου (Simple Linear Model - Slide 21):** Υποθέτουμε ότι η `f` έχει γραμμική μορφή:
    `Y = f(X) + ε = β₁X + β₀ + ε`
    *   `β₀`: Το σημείο τομής με τον άξονα Υ (intercept) - η αναμενόμενη τιμή του Υ όταν Χ=0.
    *   `β₁`: Η κλίση (slope) - η αλλαγή στην αναμενόμενη τιμή του Υ για μοναδιαία αύξηση του Χ.
*   **Εκτίμηση:** Στόχος είναι να εκτιμήσουμε τους συντελεστές `β₀` και `β₁` χρησιμοποιώντας τα παρατηρούμενα δεδομένα. Συμβολίζουμε τις εκτιμήσεις ως `β̂₀` και `β̂₁`. Η εκτιμώμενη γραμμή (γραμμή παλινδρόμησης) είναι:
    `Ŷ = f̂(X) = β̂₁X + β̂₀`
*   **Πώς βρίσκουμε τους βέλτιστους `β̂₀` και `β̂₁`; (Slides 22-27)**
    *   Ποια είναι η "καλύτερη" γραμμή που ταιριάζει στα δεδομένα; (Slides 23-25)
    *   **Υπόλοιπα (Residuals - Slide 26):** Η διαφορά μεταξύ της παρατηρούμενης τιμής `yi` και της προβλεπόμενης τιμής `ŷi` για κάθε σημείο: `ei = yi - ŷi`.
    *   **Συνάρτηση Απώλειας (Loss Function):** Θέλουμε να ελαχιστοποιήσουμε τα σφάλματα πρόβλεψης. Μια κοινή συνάρτηση απώλειας είναι το **Μέσο Τετραγωνικό Σφάλμα (Mean Squared Error - MSE)** (Slide 27):
        `MSE = L(β₀, β₁) = (1/n) * Σ(yi - ŷi)² = (1/n) * Σ(yi - (β̂₁Xi + β̂₀))²`
    *   Επιλέγουμε τα `β̂₀` και `β̂₁` που ελαχιστοποιούν το MSE.
*   **Μέθοδοι Ελαχιστοποίησης MSE:**
    1.  **Brute Force (Slide 28):** Δοκιμάζουμε όλους τους πιθανούς συνδυασμούς `β₀`, `β₁`, υπολογίζουμε το MSE για καθέναν και διαλέγουμε τον συνδυασμό με το μικρότερο MSE. **Μη πρακτικό!**
    2.  **Αναλυτική (Ακριβής) Μέθοδος: Ελάχιστα Τετράγωνα (Least Squares - Slides 29-30):**
        *   Παίρνουμε τις μερικές παραγώγους της συνάρτησης απώλειας `L` ως προς `β₀` και `β₁`.
        *   Θέτουμε τις παραγώγους ίσες με μηδέν.
        *   Λύνουμε το σύστημα εξισώσεων για να βρούμε τους τύπους για `β̂₀` και `β̂₁`.
        *   Οι τελικοί τύποι (Ordinary Least Squares - OLS formulas) είναι (Slide 29):
            `β̂₁ = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²]`
            `β̂₀ = ȳ - β̂₁x̄` (όπου `x̄` και `ȳ` είναι οι μέσοι όροι των `x` και `y`).
        *   Η γραμμή `Ŷ = β̂₁X + β̂₀` ονομάζεται **γραμμή παλινδρόμησης (regression line)**.
        *   **Προσοχή:** Αναλυτικές/ακριβείς λύσεις υπάρχουν σπάνια σε πιο σύνθετα προβλήματα.

---

## **5. Μέθοδος Κλίσης (Gradient Descent) (Slides 31-35)**

Μια πιο γενική, **επαναληπτική (iterative)** μέθοδος βελτιστοποίησης για την εύρεση των συντελεστών που ελαχιστοποιούν τη συνάρτηση απώλειας, ειδικά χρήσιμη όταν δεν υπάρχει αναλυτική λύση.

*   **Ιδέα (Slide 31):**
    1.  Ξεκινάμε με τυχαίες τιμές για τους συντελεστές (π.χ., `β₁`).
    2.  Υπολογίζουμε την κλίση (παράγωγο - derivative) της συνάρτησης απώλειας στο τρέχον σημείο. Η κλίση δείχνει την κατεύθυνση της πιο απότομης ανόδου.
    3.  Κάνουμε ένα μικρό βήμα στην **αντίθετη** κατεύθυνση της κλίσης (για να κατέβουμε προς το ελάχιστο).
    4.  Επαναλαμβάνουμε τα βήματα 2 και 3 μέχρι να συγκλίνουμε σε ένα ελάχιστο.
*   **Μέγεθος Βήματος (Step Size) / Ρυθμός Μάθησης (Learning Rate - Slides 32-33):**
    *   Το μέγεθος του βήματος σε κάθε επανάληψη καθορίζεται από τον **ρυθμό μάθησης (learning rate)**, `λ`.
    *   Ένα καλό μέγεθος βήματος είναι ανάλογο της απόλυτης τιμής της κλίσης (μικρότερα βήματα κοντά στο ελάχιστο για να μην το προσπεράσουμε).
    *   Τυπικός κανόνας ενημέρωσης για έναν συντελεστή `w` (όπου `w` μπορεί να είναι `β₀` ή `β₁`):
        `w_new = w_old - λ * (∂L / ∂w)` (Κινούμαστε αντίθετα από την κλίση `∂L / ∂w`).
*   **Πολλαπλοί Συντελεστές (Slide 32):** Για μοντέλα με πολλούς συντελεστές, υπολογίζουμε την παράγωγο ως προς *κάθε* συντελεστή και τους ενημερώνουμε όλους ταυτόχρονα (ή διαδοχικά).
*   **Περίληψη & Προβληματισμοί (Slides 34-35):**
    *   Είναι επαναληπτική μέθοδος που κινείται προς την κατεύθυνση της αρνητικής παραγώγου.
    *   Ο ρυθμός μάθησης `λ` είναι κρίσιμη παράμετρος.
    *   Πρέπει να υπολογίζουμε (αναλυτικά ή αριθμητικά) τις παραγώγους.
    *   Κίνδυνος εγκλωβισμού σε **τοπικά ελάχιστα (local minima)** αν η συνάρτηση απώλειας δεν είναι κυρτή (convex).
    *   Η πλήρης συνάρτηση απώλειας απαιτεί άθροιση πάνω από όλα τα δείγματα, που μπορεί να είναι υπολογιστικά ακριβό (υπάρχουν παραλλαγές όπως Stochastic Gradient Descent - SGD).

---

## **6. Επίδραση Δεδομένων & Πολλαπλή Παλινδρόμηση (Slides 36-39)**

*   **Ευαισθησία στα Δεδομένα Εκπαίδευσης (Slide 36):** Οι εκτιμώμενοι συντελεστές (`β̂₀`, `β̂₁`) εξαρτώνται από το συγκεκριμένο υποσύνολο δεδομένων που χρησιμοποιείται για την εκπαίδευση. Διαφορετικά δείγματα οδηγούν σε ελαφρώς διαφορετικές γραμμές παλινδρόμησης.
*   **Πολλαπλή Παλινδρόμηση (Multiple Regression - Slide 37):** Τι γίνεται αν έχουμε *πολλές* μεταβλητές πρόβλεψης (π.χ., TV, Radio, Newspaper);
    *   Υποθέτουμε ένα **πολλαπλό γραμμικό μοντέλο**:
        `Y = β₀ + β₁X₁ + β₂X₂ + ... + βpXp + ε`
    *   Ή σε μορφή πινάκων: `Y = Xβ + ε`
*   **Δημιουργία Ψευδο-Δεδομένων (Dummy Dataset - Slide 38):** Ένα παράδειγμα όπου ορίζουμε εμείς τις τιμές των συντελεστών `β` (κάποιες μπορεί να είναι 0, άρα οι αντίστοιχες μεταβλητές δεν επηρεάζουν το `Y`), δημιουργούμε τις τιμές `Y` βάσει του μοντέλου και προσθέτουμε θόρυβο `ε`. Στην πράξη, **δεν ξέρουμε** αν τα δεδομένα προήλθαν από γραμμικό μοντέλο, απλά το υποθέτουμε.
*   **Λύση Ελαχίστων Τετραγώνων για Πολλαπλή Παλινδρόμηση (OLS - Slide 39):**
    *   Η συνάρτηση απώλειας (MSE) παραμένει ο στόχος προς ελαχιστοποίηση.
    *   Υπάρχει αναλυτική λύση σε μορφή πινάκων:
        `β̂_OLS = argmin ||Y - Xβ||² = (XᵀX)⁻¹ XᵀY`
    *   Αυτή είναι η **λύση Ordinary Least Squares (OLS)**.
    *   **Περιορισμοί:** Σύνδεση με το trade-off μεροληψίας/διακύμανσης.

---

## **7. Μεροληψία, Διακύμανση & Περιορισμοί OLS (Bias-Variance Trade-off) (Slides 39-43)**

*   **Σφάλμα Μοντέλου:** Το αναμενόμενο σφάλμα πρόβλεψης ενός μοντέλου μπορεί να αναλυθεί σε τρεις συνιστώσες (Slide 41):
    `Expected Prediction Error = Bias² + Variance + Irreducible Error (σ²)`
    *   **Irreducible Error (σ²):** Ο θόρυβος `ε` που υπάρχει εγγενώς στα δεδομένα και δεν μπορεί να μειωθεί από κανένα μοντέλο.
    *   **Bias (Μεροληψία - Slide 40):** Σφάλμα που οφείλεται σε λανθασμένες υποθέσεις του μοντέλου για την πραγματική μορφή της `f`. Υψηλή μεροληψία οδηγεί σε **underfitting** (το μοντέλο είναι πολύ απλό και δεν συλλαμβάνει την υποκείμενη δομή). Π.χ., η υπόθεση γραμμικότητας της OLS.
    *   **Variance (Διακύμανση - Slide 40):** Πόσο αλλάζει η εκτίμηση `f̂` αν εκπαιδεύσουμε το μοντέλο σε διαφορετικά σύνολα δεδομένων εκπαίδευσης. Υψηλή διακύμανση οδηγεί σε **overfitting** (το μοντέλο προσαρμόζεται υπερβολικά στον θόρυβο των δεδομένων εκπαίδευσης και δεν γενικεύει καλά σε νέα δεδομένα). Π.χ., kNN με μικρό k, πολύπλοκα Decision Trees.
    *   **Trade-off:** Συνήθως, υπάρχει αντιστάθμιση: πιο πολύπλοκα μοντέλα μειώνουν τη μεροληψία αλλά αυξάνουν τη διακύμανση, και αντίστροφα (Slides 41, 42).
*   **Περιορισμοί OLS (Slide 43):**
    *   Η γραμμική παλινδρόμηση (OLS) εστιάζει στη μείωση της μεροληψίας (κάνοντας την υπόθεση γραμμικότητας) αλλά συχνά πάσχει από **υψηλή διακύμανση**.
    *   Γίνεται ασταθής όταν υπάρχουν πολλές μεταβλητές πρόβλεψης, ειδικά αν είναι **συσχετισμένες (correlated predictors)** (multicollinearity).
    *   Η αντιστροφή του πίνακα `XᵀX` μπορεί να γίνει προβληματική (αριθμητικά ασταθής) αν υπάρχει πολυγραμμικότητα.
*   **Λύση: Κανονικοποίηση (Regularization)**

---

## **8. Κανονικοποίηση (Regularization) (Slides 43-47)**

Τεχνικές που τροποποιούν τη συνάρτηση απώλειας για να ελέγξουν την πολυπλοκότητα του μοντέλου, μειώνοντας τη διακύμανση (συχνά με αντάλλαγμα μια μικρή αύξηση στη μεροληψία).

*   **Ιδέα:** Προσθέτουμε έναν **όρο ποινής (penalty term)** στη συνάρτηση απώλειας (MSE), ο οποίος τιμωρεί μεγάλες τιμές των συντελεστών `β`. Αυτό "συρρικνώνει" (shrinks) τους συντελεστές προς το μηδέν.
*   **Ridge Regression (L2 Regularization - Slide 44):**
    *   Προσθέτει ποινή ανάλογη του **αθροίσματος των τετραγώνων** των συντελεστών (L2 norm): `λ Σ(βj)² = λ ||β||₂²`.
    *   Η συνάρτηση απώλειας είναι: `L_Ridge = MSE + λ ||β||₂²`.
    *   Η παράμετρος `λ` ελέγχει την ένταση της ποινής (λ=0 -> OLS).
    *   Έχει αναλυτική λύση: `β̂_Ridge = (XᵀX + λI)⁻¹ XᵀY`. Το `λI` σταθεροποιεί την αντιστροφή του πίνακα.
    *   Συρρικνώνει τους συντελεστές *προς* το μηδέν, αλλά **δεν τους μηδενίζει** εντελώς (εκτός αν λ=∞).
*   **Lasso Regression (L1 Regularization - Slide 45):**
    *   Προσθέτει ποινή ανάλογη του **αθροίσματος των απόλυτων τιμών** των συντελεστών (L1 norm): `λ Σ|βj| = λ ||β||₁`.
    *   Η συνάρτηση απώλειας είναι: `L_Lasso = MSE + λ ||β||₁`.
    *   **Δεν έχει αναλυτική λύση.** Απαιτεί επαναληπτικούς αλγορίθμους (π.χ., τύπου Gradient Descent).
    *   Έχει την ιδιότητα να **μηδενίζει εντελώς** κάποιους συντελεστές `βj`, εκτελώντας έτσι αυτόματα **επιλογή χαρακτηριστικών (feature selection)**.
    *   Μπορεί να είναι ασταθής αν υπάρχουν έντονα συσχετισμένες μεταβλητές (επιλέγει τυχαία μία από την ομάδα).
*   **Elastic Net Regression (Slide 46):**
    *   Συνδυάζει και τις δύο ποινές (L1 και L2): `L_ElasticNet = MSE + λ₁ ||β||₂² + λ₂ ||β||₁`.
    *   Προσπαθεί να συνδυάσει τα πλεονεκτήματα των Ridge και Lasso.
    *   Χειρίζεται καλύτερα ομάδες συσχετισμένων μεταβλητών από το Lasso.
    *   Μειώνει την πολυπλοκότητα (μέσω L1) καλύτερα από το Ridge.
    *   Δεν έχει αναλυτική λύση.
*   **Επίδραση στους Συντελεστές (Slide 47):** Γράφημα που δείχνει πώς οι εκτιμώμενοι συντελεστές διαφέρουν μεταξύ OLS, Ridge, Lasso, Elastic Net σε σχέση με τους "πραγματικούς" (true) συντελεστές του ψευδο-μοντέλου. Το Lasso μηδενίζει κάποιους συντελεστές.

---

## **9. Αξιολόγηση Απόδοσης Μοντέλων Παλινδρόμησης (Slide 48)**

*   **Μετρική:** Μια βασική μετρική για την αξιολόγηση μοντέλων παλινδρόμησης είναι το **Ριζικό Μέσο Τετραγωνικό Σφάλμα (Root Mean Squared Error - RMSE)**:
    `RMSE = sqrt(MSE) = sqrt[(1/n) * Σ(yi - ŷi)²]`
*   **Ερμηνεία:** Το RMSE έχει τις ίδιες μονάδες μέτρησης με τη μεταβλητή απόκρισης `Y`. Αντιπροσωπεύει το "τυπικό" μέγεθος του σφάλματος πρόβλεψης.
*   **Στόχος:** Θέλουμε **μικρότερο RMSE**. Χαμηλότερη τιμή = καλύτερη απόδοση του μοντέλου.
*   **Γραφήματα (Slide 48):** Scatter plots των προβλεπόμενων τιμών (`Predicted Y`) έναντι των πραγματικών τιμών (`True Y`) για τα σύνολα εκπαίδευσης (Train) και ελέγχου (Test), για διάφορα μοντέλα (OLS, Ridge, Lasso, Elastic Net). Ιδανικά, τα σημεία πρέπει να βρίσκονται πάνω στην διαγώνιο `y=x`. Βλέπουμε και τις αντίστοιχες τιμές RMSE.

---

## **10. Αντιμετώπιση Μη-Γραμμικότητας (Non-linear Problems) (Slides 49-55)**

*   **Πρόβλημα:** Τι γίνεται αν η πραγματική σχέση `f` δεν είναι γραμμική; (Slide 49)
*   **Λύση: Πολυωνυμική Παλινδρόμηση (Polynomial Regression - Slide 50, 52):**
    *   Υποθέτουμε ένα πολυωνυμικό μοντέλο αντί για γραμμικό.
    *   **Τεχνική:** Δημιουργούμε *νέες* μεταβλητές πρόβλεψης (features) που είναι δυνάμεις ή γινόμενα των αρχικών μεταβλητών.
    *   *Παράδειγμα (Slide 50):* Από τις αρχικές `x₁`, `x₂`, δημιουργούμε `x₁²`, `x₂²`, `x₁x₂`.
    *   Το μοντέλο γίνεται: `Y = β₀ + β₁x₁² + β₂x₂² + β₃x₁x₂ + β₄x₁ + β₅x₂ + ε`.
    *   **Σημαντικό:** Αυτό το μοντέλο είναι ακόμα **γραμμικό ως προς τους συντελεστές `β`**. Επομένως, μπορούμε να χρησιμοποιήσουμε τις ίδιες μεθόδους (OLS, Ridge, Lasso, Elastic Net) για να βρούμε τους συντελεστές `β`, απλά εφαρμόζοντάς τες στο *επαυξημένο* σύνολο χαρακτηριστικών (`x₁`, `x₂`, `x₁²`, `x₂²`, `x₁x₂`).
*   **Οπτικοποίηση (Slide 51):** Τα δεδομένα και η προσαρμοσμένη (fitted) πολυωνυμική επιφάνεια.
*   **Σύγκριση Απόδοσης (Slides 53-54):** Γραφήματα Predicted vs True και τιμές RMSE για γραμμικά και πολυωνυμικά μοντέλα (2ου βαθμού εδώ) στα σύνολα Train και Test. Συχνά το πολυωνυμικό μοντέλο ταιριάζει καλύτερα στα δεδομένα εκπαίδευσης (χαμηλότερο RMSE στο Train) αλλά πρέπει να ελέγξουμε την απόδοση στο Test set για να αποφύγουμε overfitting.
*   **Επίδραση στους Συντελεστές (Slide 55):** Γράφημα που δείχνει τους εκτιμώμενους συντελεστές για τα πολυωνυμικά χαρακτηριστικά (`Intercept`, `x1`, `x2`, `x1^2`, `x1*x2`, `x2^2`) με τις διάφορες μεθόδους.

---

## **11. Συμπεράσματα (Conclusions - Slide 56)**

*   Η ανάλυση παλινδρόμησης είναι ισχυρό εργαλείο για την κατανόηση σχέσεων και την πρόβλεψη συνεχών τιμών.
*   Υπάρχουν διάφοροι τύποι (γραμμική, πολυωνυμική, κανονικοποιημένη) και η επιλογή εξαρτάται από το πρόβλημα.
*   Η ερμηνεία των συντελεστών, των μετρικών σφάλματος (π.χ., RMSE) και των διαγνωστικών ελέγχων είναι κρίσιμη.
*   Αν μια συγκεκριμένη μοντελοποίηση (π.χ., γραμμική) αποτύχει, μπορούμε πάντα να δοκιμάσουμε άλλους προσεγγιστές όπως kNN, SVR (Support Vector Regression), Δέντρα Αποφάσεων κ.λπ.
