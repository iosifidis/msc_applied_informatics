# Περίληψη, βάση των ερωτήσεων πολλαπλής επιλογής στην Μηχανική Μάθηση και Νευρωνικά Δίκτυα

Η Τεχνητή Νοημοσύνη, και ειδικότερα η Μηχανική Μάθηση και τα Νευρωνικά Δίκτυα, παρέχουν τα εργαλεία για να «μάθουν» τα συστήματα από δεδομένα και να παίρνουν αποφάσεις ή να κάνουν προβλέψεις χωρίς να είναι ρητά προγραμματισμένα για κάθε πιθανό σενάριο.

Ας δούμε κάποιες από τις βασικές μεθόδους:

**1. Βασικοί Αλγόριθμοι Μηχανικής Μάθησης:**

*   **k-Means:** Είναι ένας κλασικός αλγόριθμος *μη επιβλεπόμενης μάθησης (unsupervised learning)* για **συσταδοποίηση (clustering)**. Σκοπός του είναι να ομαδοποιήσει δεδομένα σε k διακριτές συστάδες, βρίσκοντας τα κέντρα των συστάδων και αντιστοιχίζοντας κάθε σημείο δεδομένων στην πλησιέστερη συστάδα. **Πότε χρησιμοποιείται:** Όταν θέλουμε να βρούμε κρυφές δομές ή ομάδες μέσα στα δεδομένα χωρίς να γνωρίζουμε εκ των προτέρων τις ομάδες (π.χ., τμηματοποίηση πελατών, ομαδοποίηση εγγράφων). Σημειώστε, όπως είδαμε στην Ε1 (Μηχανική Μάθηση), ότι το k-Means συνήθως συγκλίνει σε ένα *τοπικό* βέλτιστο, όχι απαραίτητα στο ολικό βέλτιστο, επειδή η αντικειμενική συνάρτηση είναι μη-κυρτή.
*   **Decision Trees (Δέντρα Απόφασης) / Regression Trees:** Είναι αλγόριθμοι για *επιβλεπόμενη μάθηση (supervised learning)*, που χρησιμοποιούνται τόσο για **ταξινόμηση (classification)** όσο και για **παλινδρόμηση (regression)**. Χτίζουν ένα μοντέλο σε μορφή δέντρου όπου κάθε εσωτερικός κόμβος αντιπροσωπεύει έναν έλεγχο σε ένα χαρακτηριστικό, κάθε κλαδί μια έκβαση του ελέγχου, και κάθε φύλλο αντιπροσωπεύει την τελική απόφαση (για ταξινόμηση) ή τιμή (για παλινδρόμηση, Ε10 ΜΜ). **Πότε χρησιμοποιούνται:** Είναι διαισθητικά, εύκολα στην ερμηνεία και μπορούν να χειριστούν τόσο αριθμητικά όσο και κατηγορικά δεδομένα. Είδαμε (Ε8 ΜΜ) ότι η αύξηση του "βάθους" (πολυπλοκότητας) ενός δέντρου, αν και βελτιώνει την επίδοση στο train set, μπορεί να οδηγήσει σε **υπερπροσαρμογή (overfitting)** (Ε13 ΜΜ), κάτι που θίγεται και σε άλλες τεχνικές (π.χ. ΝΝ, Ε10 ΝΝ).
*   **k-Nearest Neighbors (kNN):** Είναι ένας απλός αλγόριθμος *μη-παραμετρικής* *επιβλεπόμενης μάθησης* για **ταξινόμηση** και **παλινδρόμηση**. Για να ταξινομήσει ένα νέο σημείο δεδομένων, κοιτάζει τις k κοντινότερες "γειτονιές" του στο train set και του αποδίδει την κλάση που είναι συχνότερη μεταξύ των γειτονιών (για ταξινόμηση) ή τον μέσο όρο των τιμών τους (για παλινδρόμηση). Είναι **μη-παραμετρικός** (Ε3 ΜΜ, Ε18 ΜΜ), δηλαδή η πολυπλοκότητα του μοντέλου (ουσιαστικά το train set το ίδιο) αυξάνεται με τον αριθμό των δεδομένων εκπαίδευσης, σε αντίθεση με τα **παραμετρικά** μοντέλα (π.χ., Linear/Logistic Regression, Naïve Bayes - Ε3, Ε18 ΜΜ) που έχουν σταθερό αριθμό παραμέτρων ανεξάρτητα από το μέγεθος του dataset. **Πότε χρησιμοποιείται:** Όταν το dataset είναι μικρό προς μεσαίο και η γεωμετρική γειτνίαση είναι ένας καλός δείκτης ομοιότητας. Είναι ευαίσθητος στην κλίμακα των χαρακτηριστικών και τον θόρυβο. Η πολυπλοκότητά του στην εκτέλεση (classification) για μεγάλα dataset είναι υψηλή, καθώς συνήθως απαιτεί υπολογισμό αποστάσεων σε όλα ή μεγάλο μέρος των train data (Ε17 ΜΜ).
*   **Naïve Bayes:** Ένας αλγόριθμος *επιβλεπόμενης μάθησης* για **ταξινόμηση**, βασισμένος στο θεώρημα Bayes με μια "αφελή" (naïve) υπόθεση: την υπό συνθήκη ανεξαρτησία των χαρακτηριστικών (features) δεδομένης της κλάσης. **Πότε χρησιμοποιείται:** Είναι απλός, γρήγορος και συχνά αποτελεσματικός σε προβλήματα κειμενικής ταξινόμησης (π.χ., φιλτράρισμα spam). Ωστόσο, η "αφελή" υπόθεση σπάνια ισχύει στην πραγματικότητα. Όταν τα χαρακτηριστικά *δεν είναι* υπό συνθήκη ανεξάρτητα, μπορεί άλλα μοντέλα όπως η Logistic Regression να είναι προτιμότερα (Ε9 ΜΜ).
*   **Logistic Regression (Λογιστική Παλινδρόμηση):** Παρά την ονομασία "Regression", χρησιμοποιείται κυρίως για **ταξινόμηση** (δυαδική ή πολυωνυμική). Είναι ένα *παραμετρικό* μοντέλο που προβλέπει την πιθανότητα (μεταξύ 0 και 1) ότι ένα σημείο δεδομένων ανήκει σε μια συγκεκριμένη κλάση, χρησιμοποιώντας μια logistic (sigmoid) συνάρτηση για να "στριμώξει" την γραμμική έξοδο σε αυτό το εύρος πιθανότητας (Ε12 ΜΜ). **Πότε χρησιμοποιείται:** Είναι απλό, γρήγορο στην εκπαίδευση, ερμηνεύσιμο και λειτουργεί καλά σε γραμμικώς διαχωρίσιμα δεδομένα.
*   **Linear Regression (Γραμμική Παλινδρόμηση):** Ένα *παραμετρικό* μοντέλο για **παλινδρόμηση** (πρόβλεψη μιας συνεχούς τιμής). Προσπαθεί να βρει μια γραμμική σχέση μεταξύ των χαρακτηριστικών εισόδου και της εξόδου. **Πότε χρησιμοποιείται:** Για προβλέψεις συνεχών τιμών όταν υποθέτουμε μια γραμμική σχέση ή ως baseline μοντέλο. Η Lasso (Ε15 ΜΜ) είναι μια μορφή γραμμικής παλινδρόμησης με L1 κανονικοποίηση (regularization) στα βάρη, που βοηθάει στην επιλογή χαρακτηριστικών μειώνοντας τα βάρη μη σημαντικών χαρακτηριστικών σε μηδέν.
*   **Support Vector Machines (SVM):** Μοντέλα *επιβλεπόμενης μάθησης* που χρησιμοποιούνται κυρίως για **ταξινόμηση** (αλλά και παλινδρόμηση). Ο βασικός τους στόχος είναι να βρουν το βέλτιστο υπερεπίπεδο (γραμμή σε 2D, επίπεδο σε 3D, κλπ.) που διαχωρίζει τις κλάσεις με το μέγιστο δυνατό περιθώριο (margin) μεταξύ των κοντινότερων σημείων δεδομένων (support vectors). Το **Hard-margin SVM** (Ε16 ΜΜ) λειτουργεί *μόνο* εάν τα δεδομένα είναι τέλεια γραμμικώς διαχωρίσιμα, ενώ το **Soft-margin SVM** επιτρέπει σε ορισμένα σημεία να βρίσκονται μέσα στο περιθώριο ή ακόμη και στη λάθος πλευρά του υπερεπιπέδου, χειριζόμενο έτσι τα μη-γραμμικώς διαχωρίσιμα ή θορυβώδη δεδομένα. Με τη χρήση kernels (π.χ., RBF, polynomial - ο γραμμικός kernel αναφέρεται στο Ε16 ΜΜ), μπορούν να χειριστούν και μη-γραμμικά διαχωρίσιμα προβλήματα στο αρχικό χώρο χαρακτηριστικών, προβάλλοντάς τα σε χώρους υψηλότερης διάστασης.

**2. Προεπεξεργασία Δεδομένων (Preprocessing) και Χειρισμός Προβλημάτων:**

*   **Preprocessing Steps (Ε19 ΜΜ):**
    *   **Normalization:** Κλιμακώνει τα χαρακτηριστικά (features) σε ένα κοινό εύρος τιμών (π.χ., [0, 1] ή μέση τιμή 0 και διασπορά 1). **Πότε χρησιμοποιείται:** Σχεδόν πάντα, ειδικά σε αλγορίθμους που βασίζονται στην απόσταση (π.χ., kNN, SVMs, Νευρωνικά Δίκτυα, Logistic Regression) ή σε μεθόδους βελτιστοποίησης με βάση κλίση, καθώς βοηθάει στην ταχύτερη σύγκλιση και στην αποφυγή της κυριαρχίας χαρακτηριστικών με μεγάλες τιμές.
    *   **Outliers' Removal (Απομάκρυνση Αποκλινουσών Τιμών):** Οι αποκλίνουσες τιμές (outliers - Ε14 ΜΜ) είναι σημεία δεδομένων που διαφέρουν σημαντικά από τα υπόλοιπα. **Πότε χρησιμοποιείται:** Εξαρτάται από το πρόβλημα και τον αλγόριθμο. Ορισμένοι αλγόριθμοι (π.χ., Linear Regression, k-Means, Hard-margin SVM) είναι πολύ ευαίσθητοι στους outliers και η απομάκρυνσή τους ή ο χειρισμός τους μπορεί να βελτιώσει σημαντικά την επίδοση. Άλλοι (π.χ., Decision Trees, μοντέλα με L1 regularization όπως Lasso, Soft-margin SVM) είναι πιο ανθεκτικοί. Συνήθως δεν τους απομακρύνουμε "στα τυφλά" αλλά αφού κατανοήσουμε αν είναι λάθη μέτρησης ή γνήσιες αλλά σπάνιες παρατηρήσεις. Η επιλογή εξαρτάται *πάντα* από το εκάστοτε πρόβλημα (Ε14 δ. ΜΜ).
    *   **Class-imbalance Handling (Χειρισμός Ανισορροπίας Κλάσεων):** Όταν οι κλάσεις σε ένα πρόβλημα ταξινόμησης δεν έχουν περίπου τον ίδιο αριθμό δειγμάτων. Ένα μοντέλο μπορεί να τείνει να προβλέπει την πλειοψηφούσα κλάση. **Πότε χρησιμοποιείται:** Σε προβλήματα όπου μας ενδιαφέρει η σωστή πρόβλεψη και της μειοψηφούσας κλάσης (π.χ., ανίχνευση απάτης, διάγνωση σπάνιας ασθένειας). Τεχνικές όπως oversampling, undersampling ή χρήση εξειδικευμένων μετρικών (π.χ., F1-score, Precision, Recall, AUC-ROC - Ε21 ΜΜ, Ε21 ΝΝ), αντί μόνο της accuracy (Ε5 ΜΜ), είναι απαραίτητες.
    *   **Dimensionality Reduction (Μείωση Διάστασης - Ε7, Ε19 ΜΜ):** Μειώνει τον αριθμό των χαρακτηριστικών. **Πότε χρησιμοποιείται:** Για να αντιμετωπιστεί το "curse of dimensionality" (πρόβλημα υψηλών διαστάσεων), να μειωθεί ο υπολογιστικός φόρτος, να αφαιρεθεί θόρυβος ή να γίνει ευκολότερη οπτικοποίηση. Μπορεί να γίνει πριν την εφαρμογή πολλών αλγορίθμων Μηχανικής Μάθησης, όπως kNN, decision trees και Naïve Bayes (Ε7 ΜΜ), καθώς αυτοί μπορεί να επηρεάζονται αρνητικά από την υψηλή διάσταση.
*   **Διαχωρισμός Δεδομένων (Splitting Data):**
    *   **Train/Test Split:** Διαχωρίζουμε τα δεδομένα σε ένα σύνολο για εκπαίδευση (train set) και ένα για αξιολόγηση της γενίκευσης (test set - Ε5 ΜΜ). **Πότε χρησιμοποιείται:** Για να ελέγξουμε αν το μοντέλο έχει υπερπροσαρμογή (υψηλή επίδοση στο train, χαμηλή στο test) ή υποπροσαρμογή (χαμηλή επίδοση και στα δύο - Ε4, Ε5 ΜΜ).
    *   **Stratification (Στρωματοποίηση - Ε11 ΜΜ):** Μια τεχνική διαχωρισμού των δεδομένων (για train/test/validation) που **διατηρεί** την αναλογία των κλάσεων (στο πρόβλημα ταξινόμησης) ή την κατανομή των τιμών (στο πρόβλημα παλινδρόμησης) σε κάθε υποσύνολο (train/test) όπως ήταν στο αρχικό dataset. **Πότε χρησιμοποιείται:** Απαραίτητο ειδικά σε προβλήματα με ανισορροπία κλάσεων, για να διασφαλιστεί ότι τόσο το train όσο και το test set περιέχουν αντιπροσωπευτικά δείγματα από όλες τις κλάσεις.
    *   **Cross Validation (Διασταυρούμενη Επικύρωση - Ε11 ΜΜ):** Μια ισχυρότερη τεχνική αξιολόγησης του μοντέλου (και επιλογής υπερπαραμέτρων) από ένα απλό train/test split. Χωρίζει το dataset σε k "δίπλες" (folds), εκπαιδεύει το μοντέλο k φορές (κάθε φορά σε k-1 δίπλες) και αξιολογεί στην εναπομένουσα δίπλα (ως test set). **Πότε χρησιμοποιείται:** Όταν το dataset είναι μικρό ή μεσαίο, για να αξιολογηθεί καλύτερα η γενίκευση του μοντέλου και να μειωθεί η επίδραση μιας συγκεκριμένης διαίρεσης train/test.

**3. Βασικές Έννοιες Νευρωνικών Δικτύων:**

*   **Νευρώνες και Βάρη:** Οι βασικές υπολογιστικές μονάδες. Κάθε νευρώνας δέχεται εισόδους (x), τις πολλαπλασιάζει με αντίστοιχα βάρη (w), προσθέτει μια σταθερά (bias), και περνάει το αποτέλεσμα μέσω μιας **συνάρτησης ενεργοποίησης (activation function)** για να παραγάγει την έξοδό του (Ε9 ΝΝ). Η μάθηση συνίσταται στην προσαρμογή αυτών των βαρών και bias.
*   **Συναρτήσεις Ενεργοποίησης (Ε4, Ε5, Ε14 ΝΝ):** Είναι μη-γραμμικές συναρτήσεις (sigmoid, tanh, ReLU, Leaky ReLU, softmax, κλπ.) που εφαρμόζονται στην έξοδο ενός νευρώνα πριν την προωθητική διάδοση (forward pass). **Πρωταρχικός σκοπός (Ε4 ΝΝ):** Η εισαγωγή *μη-γραμμικότητας*. Χωρίς μη-γραμμικές συναρτήσεις ενεργοποίησης, ακόμη και ένα δίκτυο με πολλά κρυμμένα επίπεδα θα συμπεριφερόταν ως ένα απλό γραμμικό μοντέλο. Με αυτές, τα νευρωνικά δίκτυα μπορούν να μοντελοποιήσουν εξαιρετικά σύνθετες, μη-γραμμικές σχέσεις και να σχηματίσουν μη-γραμμικά όρια απόφασης (decision boundaries - Ε1 ΝΝ).
    *   **Sigmoid / Tanh:** Χρησιμοποιούνταν παλιότερα ευρέως, αλλά πάσχουν από το πρόβλημα του **vanishing gradient** (Ε5 ΝΝ), ειδικά σε βαθιά δίκτυα, όπου οι κλίσεις (gradients) γίνονται πολύ μικρές κατά την οπισθοδιάδοση (backpropagation - Ε13 ΝΝ), καθιστώντας την εκμάθηση στα αρχικά επίπεδα πολύ αργή.
    *   **ReLU (Rectified Linear Unit) / Leaky ReLU:** Πιο σύγχρονες και δημοφιλείς. Δεν παρουσιάζουν vanishing gradient για θετικές εισόδους, επιταχύνοντας την εκπαίδευση.
    *   **Softmax:** Συνήθως χρησιμοποιείται στο *τελευταίο* επίπεδο για προβλήματα *πολυωνυμικής ταξινόμησης*. Μετατρέπει τις "ωμές" εξόδους σε πιθανότητες για κάθε κλάση, όπου το άθροισμα των πιθανοτήτων είναι 1 (Ε14 γ. ΝΝ).
*   **Backpropagation (Οπισθοδιάδοση):** Ο βασικός αλγόριθμος για την εκπαίδευση των νευρωνικών δικτύων. Χρησιμοποιεί τον **κανόνα της αλυσίδας** (chain rule) από τον λογισμό και αρχές του Δυναμικού Προγραμματισμού για να υπολογίσει τις κλίσεις της συνάρτησης απώλειας (loss function) ως προς κάθε βάρος του δικτύου, ξεκινώντας από την έξοδο και πηγαίνοντας προς τα πίσω (Ε3 ΝΝ). Αυτές οι κλίσεις χρησιμοποιούνται στη συνέχεια από έναν **Optimizer** (όπως ο SGD, Adam, κ.ά.) για να ενημερωθούν τα βάρη (Ε6 β. ΝΝ).
*   **Overfitting (Υπερπροσαρμογή στα ΝΝ):** Όταν το δίκτυο "αποστηθίζει" το train set και αποδίδει άσχημα σε νέα, αόρατα δεδομένα (test set - Ε10 ΝΝ, Ε15 γ. ΝΝ).
*   **Μέθοδοι αντιμετώπισης Overfitting στα ΝΝ (Ε10, Ε16 ΝΝ):**
    *   **Data Augmentation:** Δημιουργία νέων δειγμάτων εκπαίδευσης μέσω μετασχηματισμών (π.χ., περιστροφές, μεγέθυνση, κροπάρισμα εικόνων) των υπαρχόντων. Αυξάνει την ποικιλία των δεδομένων εκπαίδευσης χωρίς να συλλέξουμε νέα.
    *   **Dropout:** Κατά την εκπαίδευση, ένα τυχαίο ποσοστό νευρώνων "απενεργοποιείται" προσωρινά σε κάθε βήμα, μαζί με τις συνδέσεις τους. Αυτό εμποδίζει το δίκτυο να γίνει υπερβολικά εξαρτημένο από συγκεκριμένα μονοπάτια ή νευρώνες και αναγκάζει το δίκτυο να μάθει πιο στιβαρές αναπαραστάσεις. Συχνά είναι πολύ αποτελεσματικό κατά του overfitting (Ε10 β. ΝΝ).
    *   **Batch Normalization:** Εφαρμόζει κανονικοποίηση στις "ενεργοποιήσεις" (outputs) των νευρώνων μέσα σε ένα επίπεδο για κάθε mini-batch δεδομένων. Βοηθάει στην ταχύτερη και σταθερότερη εκπαίδευση, επιτρέπει υψηλότερα learning rates, και λειτουργεί συχνά ως ρυθμιστής (regularizer) μειώνοντας το overfitting. (Ε12, Ε16 γ. ΝΝ).
    *   **Regularization (L1, L2):** Προσθήκη όρων στην συνάρτηση απώλειας που "τιμωρούν" τα μεγάλα βάρη, ενθαρρύνοντας απλούστερα μοντέλα.
*   **Αρχιτεκτονικές Νευρωνικών Δικτύων (Ε7 ΝΝ):**
    *   **CNNs (Convolutional Neural Networks):** Σχεδιασμένα ειδικά για δεδομένα με δομή πλέγματος, όπως εικόνες. Χρησιμοποιούν συνελικτικά επίπεδα (convolutional layers) για την εξαγωγή τοπικών χαρακτηριστικών και είναι πολύ επιτυχημένα στην όραση υπολογιστών.
    *   **RNNs (Recurrent Neural Networks):** Σχεδιασμένα για την επεξεργασία **ακολουθιακών δεδομένων** (sequential data) όπως κείμενο, χρονοσειρές, ή ομιλία. Διατηρούν μια "κατάσταση" (μνήμη) που τους επιτρέπει να επεξεργάζονται εισόδους βάσει της προηγούμενης ακολουθίας. Πιο εξελιγμένες μορφές περιλαμβάνουν τα LSTMs και GRUs.
    *   **GANs (Generative Adversarial Networks - Ε8, Ε11, Ε18 ΝΝ):** Αποτελούνται από δύο ανταγωνιστικά δίκτυα: τον **Generator** και τον **Discriminator**. Ο Generator προσπαθεί να παράγει νέα δεδομένα (π.χ., εικόνες) που μοιάζουν με τα αληθινά δεδομένα εκπαίδευσης (Ε11 α, δ. ΝΝ), ενώ ο Discriminator προσπαθεί να ξεχωρίσει αν ένα δείγμα είναι "αληθινό" (από το training set) ή "ψεύτικο" (παράγεται από τον Generator) (Ε11 β. ΝΝ). Εκπαιδεύονται ταυτόχρονα σε ένα είδος παιχνιδιού minimax. Χρησιμοποιούνται για την παραγωγή νέων δεδομένων, αναπαραστάσεις δεδομένων, κλπ. Ένα συνηθισμένο πρόβλημα στην εκπαίδευσή τους είναι το **mode collapse** (Ε18 ΝΝ), όπου ο Generator "κολλάει" παράγοντας περιορισμένη ποικιλία δεδομένων.
    *   **Autoencoders (Ε8 ΝΝ):** Δίκτυα που εκπαιδεύονται να αναπαράγουν την είσοδο στην έξοδό τους, αλλά με ένα ενδιάμεσο "κρυφό" επίπεδο που έχει μικρότερη διάσταση από την είσοδο (Bottleneck). Αυτό τα αναγκάζει να μάθουν μια συμπιεσμένη **αναπαράσταση** (representation) των δεδομένων. **Πότε χρησιμοποιούνται:** Για μείωση διάστασης (ως μορφή Dimensionality Reduction), εκμάθηση χαρακτηριστικών (feature learning), ή ανίχνευση ανωμαλιών (anomaly detection).

**4. Αξιολόγηση Μοντέλων:**

*   **Classification Metrics:** Accuracy (ποσοστό σωστών προβλέψεων), F1-score (αρμονικός μέσος Precision και Recall, χρήσιμο σε ανισορροπία κλάσεων - Ε21 ΜΜ), AUC-ROC (επιφάνεια κάτω από την καμπύλη ROC, δείχνει πόσο καλά διαχωρίζει το μοντέλο τις κλάσεις - Ε21 ΜΜ).
*   **Regression Metrics:** MAE (Mean Absolute Error), RMSE (Root Mean Squared Error - Ε21 ΜΜ). Μετρούν την μέση διαφορά (σφάλμα) μεταξύ των προβλεπόμενων και των πραγματικών τιμών.

**5. Παραμετρικά vs Μη-Παραμετρικά Μοντέλα (Ε3, Ε18 ΜΜ):**

*   **Parametric Models:** Υποθέτουν μια συγκεκριμένη, πεπερασμένη δομή (μοντέλο) για τα δεδομένα και μαθαίνουν ένα σταθερό σύνολο παραμέτρων αυτής της δομής (π.χ., Linear/Logistic Regression, Naïve Bayes). Ο αριθμός των παραμέτρων είναι ανεξάρτητος από το μέγεθος του dataset.
*   **Non-Parametric Models:** Δεν υποθέτουν μια συγκεκριμένη δομή μοντέλου και η πολυπλοκότητα ή/και ο "αριθμός παραμέτρων" (αν θεωρήσουμε τα ίδια τα δεδομένα ως μέρος του μοντέλου) μπορεί να αυξάνεται με το μέγεθος του dataset (π.χ., kNN, Decision Trees, Kernel SVMs). Αυτά τα μοντέλα μπορούν να προσαρμοστούν σε πιο πολύπλοκες δομές δεδομένων.

Αυτές είναι μερικές από τις κεντρικές έννοιες που θίγονται στα θέματα των εξετάσεων. Είναι σημαντικό να κατανοείτε όχι μόνο πώς λειτουργεί κάθε αλγόριθμος, αλλά και πότε και γιατί τον επιλέγουμε σε σχέση με άλλους, ποιες είναι οι υποθέσεις του, και πώς χειριζόμαστε πρακτικά προβλήματα όπως η υπερπροσαρμογή, οι αποκλίνουσες τιμές ή η ανισορροπία κλάσεων.

---

Πέρα από την κατηγοριοποίηση των αλγορίθμων με βάση το **είδος της μάθησης** (Επιβλεπόμενη, Μη-επιβλεπόμενη, Ενισχυτική, Ημι-επιβλεπόμενη), υπάρχει και μια σημαντική κατηγοριοποίηση με βάση τη **δομή** ή τη **μαθηματική μορφή** του μοντέλου.

Οι δύο βασικές κατηγορίες που είδατε να αναφέρονται και στα θέματα (είτε ρητά είτε έμμεσα) είναι:

1.  **Γραμμικά (Linear) vs. Μη-Γραμμικά (Non-linear) Μοντέλα:**
2.  **Παραμετρικά (Parametric) vs. Μη-Παραμετρικά (Non-parametric) Μοντέλα:**

Ας τις δούμε αναλυτικά:

**1. Γραμμικά vs. Μη-Γραμμικά Μοντέλα:**

Αυτή η κατηγοριοποίηση αφορά τη σχέση που μοντελοποιεί ο αλγόριθμος μεταξύ των χαρακτηριστικών εισόδου (features) και της εξόδου (target), ή/και το σχήμα του **ορίου απόφασης (decision boundary)** που μαθαίνει το μοντέλο σε προβλήματα ταξινόμησης.

*   **Γραμμικά Μοντέλα (Linear Models):** Αυτά τα μοντέλα εκφράζουν την έξοδο ως μια **γραμμική συνάρτηση** των χαρακτηριστικών εισόδου. Για παράδειγμα, στην παλινδρόμηση, η πρόβλεψη είναι μια στάθμιση (άθροισμα γινομένων χαρακτηριστικών επί βάρη) των χαρακτηριστικών. Στην ταξινόμηση, το όριο απόφασης είναι ένα **υπερεπίπεδο** (μια ευθεία γραμμή σε 2 διαστάσεις, ένα επίπεδο σε 3, κλπ.) που διαχωρίζει τις κλάσες.
    *   **Παραδείγματα από τα θέματα:**
        *   **Linear Regression (Ε12, Ε15 ΜΜ):** Κλασικό γραμμικό μοντέλο για παλινδρόμηση.
        *   **Logistic Regression (Ε9, Ε12 ΜΜ):** Αν και χρησιμοποιεί τη sigmoid function στην έξοδο, η *είσοδος* της sigmoid είναι μια γραμμική συνάρτηση των χαρακτηριστικών. Το όριο απόφασης στην ουσία είναι ένα υπερεπίπεδο.
        *   **Perceptron:** Ο αρχικός γραμμικός ταξινομητής, μαθαίνει ένα υπερεπίπεδο διαχωρισμού.
        *   **SVM με γραμμικό kernel (linear kernel - Ε6, Ε16 ΜΜ):** Μαθαίνει ένα γραμμικό όριο απόφασης (υπερεπίπεδο). Ο όρος "Hard-margin SVM" και "Soft-margin SVM" αναφέρονται στον τρόπο που χειρίζονται τα σημεία κοντά στο όριο ή στην λάθος πλευρά, όχι στην γραμμικότητα του ορίου αυτού, αν και ο "σκληρός" Hard-margin SVM δουλεύει μόνο σε γραμμικά διαχωρίσιμα δεδομένα (Ε16).
        *   **Νευρωνικά Δίκτυα *χωρίς* μη-γραμμικές συναρτήσεις ενεργοποίησης:** Ένα δίκτυο αποτελούμενο μόνο από γραμμικές μεταβολές παραμένει ουσιαστικά ένα γραμμικό μοντέλο (σαν να έχουμε μόνο μία γραμμική μεταβολή), ανεξάρτητα από το πόσα επίπεδα έχει.

*   **Μη-Γραμμικά Μοντέλα (Non-linear Models):** Αυτά τα μοντέλα μπορούν να μοντελοποιήσουν πιο σύνθετες, μη-γραμμικές σχέσεις μεταξύ χαρακτηριστικών και εξόδου. Τα όρια απόφασης που μαθαίνουν μπορεί να είναι καμπύλες ή πιο περίπλοκα σχήματα.
    *   **Παραδείγματα από τα θέματα:**
        *   **Decision Trees (Δέντρα Απόφασης - Ε2, Ε8 ΜΜ):** Χτίζουν ένα μοντέλο που χωρίζει το χώρο των χαρακτηριστικών με ορθογώνια (παράλληλα με τους άξονες) κομμάτια, τα οποία συνολικά σχηματίζουν μη-γραμμικά όρια. Τα **Regression Trees** (Ε10 ΜΜ) χρησιμοποιούνται για **μη-γραμμική παλινδρόμηση** (αν και τοπικά μπορούν να δώσουν σταθερή τιμή, η συνολική συνάρτηση που μοντελοποιείται δεν είναι απαραίτητα γραμμική).
        *   **kNN (Ε17, Ε18 ΜΜ):** Το όριο απόφασης του kNN καθορίζεται από τα σημεία δεδομένων και είναι συχνά μη-γραμμικό και πολύπλοκο.
        *   **SVM με μη-γραμμικούς kernels (π.χ., RBF, polynomial):** Μεταφέρουν τα δεδομένα σε έναν χώρο υψηλότερης διάστασης όπου μπορούν να γίνουν γραμμικά διαχωρίσιμα. Το όριο διαχωρισμού σε αυτόν τον υψηλότερο χώρο αντιστοιχεί σε ένα **μη-γραμμικό** όριο στον αρχικό χώρο χαρακτηριστικών.
        *   **Νευρωνικά Δίκτυα *με* μη-γραμμικές συναρτήσεις ενεργοποίησης (Ε1, Ε4 ΝΝ):** Η εισαγωγή μη-γραμμικότητας μέσω συναρτήσεων όπως Sigmoid, ReLU, Tanh επιτρέπει στα βαθιά νευρωνικά δίκτυα να μοντελοποιήσουν εξαιρετικά σύνθετες, μη-γραμμικές σχέσεις και να μάθουν οποιοδήποτε σχήμα ορίου απόφασης (με αρκετά δεδομένα και νευρώνες).

**Πότε χρησιμοποιείται:** Γραμμικά μοντέλα είναι απλούστερα, πιο γρήγορα, και συχνά λειτουργούν καλά όταν η σχέση είναι πραγματικά γραμμική ή όταν τα δεδομένα δεν είναι αρκετά για να εκπαιδεύσουν ένα πολύπλοκο μη-γραμμικό μοντέλο (αποφεύγουν την υπερπροσαρμογή). Μη-γραμμικά μοντέλα είναι απαραίτητα όταν οι σχέσεις είναι πολύπλοκες και μη-γραμμικές και έχουμε αρκετά δεδομένα για να τις μοντελοποιήσουμε χωρίς να "αποστηθίσουμε" απλώς τα δεδομένα εκπαίδευσης (overfitting).

**2. Παραμετρικά vs. Μη-Παραμετρικά Μοντέλα:**

Αυτή η κατηγοριοποίηση αφορά το αν το μοντέλο κάνει συγκεκριμένες υποθέσεις για την κατανομή των δεδομένων και αν ο αριθμός των παραμέτρων του είναι σταθερός ανεξάρτητα από το μέγεθος του dataset.

*   **Παραμετρικά Μοντέλα (Parametric Models - Ε3, Ε18 ΜΜ):** Κάνουν μια συγκεκριμένη υπόθεση για την υποκείμενη συνάρτηση ή την κατανομή των δεδομένων και μαθαίνουν ένα **σταθερό, πεπερασμένο αριθμό παραμέτρων** από τα δεδομένα. Μόλις μάθουν αυτές τις παραμέτρους, μπορούν να απορρίψουν τα δεδομένα εκπαίδευσης.
    *   **Παραδείγματα από τα θέματα:**
        *   **Linear Regression / Logistic Regression:** Μαθαίνουν τους συντελεστές (βάρη και bias). Ο αριθμός τους εξαρτάται από τον αριθμό των χαρακτηριστικών, όχι από τον αριθμό των δειγμάτων εκπαίδευσης.
        *   **Naïve Bayes (Ε9, Ε18 ΜΜ):** Υποθέτει την ανεξαρτησία των χαρακτηριστικών και μαθαίνει πιθανότητες και στατιστικές από τα δεδομένα (οι παράμετροι είναι ουσιαστικά αυτές οι πιθανότητες/στατιστικές).
        *   Άλλα παραδείγματα: Gaussian Mixture Models, Discriminant Analysis.
    *   **Πλεονεκτήματα:** Συχνά πιο γρήγορα στην εκπαίδευση, απαιτούν λιγότερα δεδομένα (όταν οι υποθέσεις τους είναι σωστές), πιο εύκολα στην ερμηνεία (ερμηνεύουμε τις παραμέτρους).
    *   **Μειονεκτήματα:** Μπορεί να έχουν χαμηλή επίδοση αν οι υποθέσεις τους για τα δεδομένα είναι λανθασμένες.

*   **Μη-Παραμετρικά Μοντέλα (Non-parametric Models - Ε3, Ε18 ΜΜ):** Δεν κάνουν ισχυρές υποθέσεις για την υποκείμενη συνάρτηση ή την κατανομή των δεδομένων. Η πολυπλοκότητα του μοντέλου και ο "αριθμός παραμέτρων" (ή τουλάχιστον τα δεδομένα εκπαίδευσης παίζουν κεντρικό ρόλο) **αυξάνεται με τον αριθμό των δεδομένων εκπαίδευσης**. Συχνά χρειάζεται να διατηρήσουν όλα ή ένα σημαντικό μέρος των δεδομένων εκπαίδευσης για να κάνουν προβλέψεις.
    *   **Παραδείγματα από τα θέματα:**
        *   **kNN (Ε17, Ε18 ΜΜ):** Ουσιαστικά, το μοντέλο είναι το σύνολο δεδομένων εκπαίδευσης.
        *   **Decision Trees (Ε2, Ε8, Ε22 ΜΜ):** Η δομή του δέντρου εξαρτάται από τα δεδομένα, και μπορεί να γίνει πολύπλοκη με την αύξηση των δεδομένων.
        *   **SVMs (γενικά, ειδικά με kernels - Ε16, Ε18 ΜΜ):** Αν και ορισμένες πτυχές (όπως οι support vectors) είναι παραμετρικές, η χρήση kernels τα κατατάσσει συχνά στην μη-παραμετρική κατηγορία ως προς την πολυπλοκότητα του μοντέλου που μπορούν να μάθουν.
        *   **Νευρωνικά Δίκτυα (μεγάλα/βαθιά - Ε22 ΜΜ, γενικά στα ΝΝ θέματα):** Αν και έχουν παραμέτρους (τα βάρη), ο αριθμός των παραμέτρων μπορεί να είναι *τεράστιος* και το δίκτυο μπορεί να μάθει αυθαίρετα πολύπλοκες συναρτήσεις, συμπεριφερόμενο ως μη-παραμετρικό σε πολλές περιπτώσεις.
        *   Άλλα παραδείγματα: Kernel Density Estimation, Random Forests.
    *   **Πλεονεκτήματα:** Μπορούν να μάθουν πιο σύνθετες σχέσεις, δεν δεσμεύονται από αυστηρές υποθέσεις.
    *   **Μειονεκτήματα:** Συχνά απαιτούν περισσότερα δεδομένα, πιο αργά στην εκπαίδευση/πρόβλεψη (ειδικά όσα διατηρούν τα train data), μπορεί να είναι πιο επιρρεπή στην υπερπροσαρμογή (με την απουσία κατάλληλης κανονικοποίησης/ ρύθμισης).

**Συνοπτικά:**

*   **Γραμμικά/Μη-Γραμμικά** αφορά το σχήμα της σχέσης/ορίου.
*   **Παραμετρικά/Μη-Παραμετρικά** αφορά την υπόθεση για την κατανομή και τον τρόπο που αυξάνεται η πολυπλοκότητα του μοντέλου με τα δεδομένα.

Ένα μοντέλο μπορεί να είναι **γραμμικό και παραμετρικό** (Linear Regression), **μη-γραμμικό και παραμετρικό** (Naïve Bayes, νευρωνικό δίκτυο με fixed αρχιτεκτονική, αν και η μη-παραμετρική πτυχή τους είναι ισχυρή στην πράξη), **γραμμικό και μη-παραμετρικό** (Perceptron ή SVM με linear kernel *μπορούν* να θεωρηθούν μη-παραμετρικά σε κάποια πλαίσια λόγω του τρόπου εκμάθησης, αν και η αυστηρή γραμμική μορφή τους τα περιορίζει), ή **μη-γραμμικό και μη-παραμετρικό** (kNN, Decision Trees, SVM με RBF kernel, βαθιά Νευρωνικά Δίκτυα).
