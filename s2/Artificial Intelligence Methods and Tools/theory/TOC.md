# **Συγκεντρωτικός Πίνακας Έννοιών Machine Learning & Neural Networks**

Θα κατηγοριοποιήσουμε τις έννοιες για ευκολία.

| Έννοια (Ελλ/Αγγλ)         | Κατηγορία                               | Σκοπός / Τι κάνει                                                                 | Βασικά Χαρακτηριστικά / Ιδιότητες                                                                                                                               | Πότε χρησιμοποιείται                                                                                                    | Αναφέρεται στις Ερωτήσεις (Διαγ.1/Διαγ.2) |
| :------------------------ | :-------------------------------------- | :------------------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------- | :---------------------------------------- |
| **Μοντέλα / Αλγόριθμοι**  |                                         |                                                                                 |                                                                                                                                                                 |                                                                                                                          |                                           |
| Νευρωνικό Δίκτυο (NN)     | Μοντέλο ML / NN                         | Μαθαίνει πολύπλοκα μοτίβα και σχέσεις στα δεδομένα.                               | Αποτελείται από διασυνδεδεμένους νευρώνες. Μπορεί να έχει κρυμμένα επίπεδα. Ικανό να μοντελοποιήσει μη-γραμμικές σχέσεις (με τις κατάλληλες συναρτήσεις).          | Για Ταξινόμηση, Παλινδρόμηση, Αναγνώριση προτύπων σε διάφορους τύπους δεδομένων (εικόνα, κείμενο, ήχος, κλπ).              | E1, E2, E6, E9, E10, E15 (Δ1)             |
| Perceptron                | Μοντέλο ML / NN (πιο απλό)              | Βασικός γραμμικός ταξινομητής.                                                    | Μόνο ένα επίπεδο εξόδου. Μπορεί να λύσει ΜΟΝΟ γραμμικά διαχωρίσιμα προβλήματα.                                                                                   | Για απλή δυαδική ταξινόμηση σε γραμμικά διαχωρίσιμα δεδομένα.                                                           | E6, E22 (Δ2)                              |
| Συνελικτικό ΝΔ (CNN)      | Αρχιτεκτονική ΝΔ                      | Επεξεργασία δεδομένων με δομή πλέγματος (π.χ. εικόνες).                            | Χρησιμοποιεί συνελικτικά φίλτρα (kernels) για την εξαγωγή χαρακτηριστικών. Αποδοτικό στην εύρεση τοπικών προτύπων.                                             | Κυρίως για Επεξεργασία Εικόνας (Ταξινόμηση, Ανίχνευση, Τμηματοποίηση), αλλά και βίντεο, ακόμη και κείμενο.                | E7, E8 (Δ1)                               |
| Αναδρομικό ΝΔ (RNN)       | Αρχιτεκτονική ΝΔ                      | Επεξεργασία ακολουθιακών δεδομένων (κείμενο, χρονοσειρές).                          | Διατηρεί εσωτερική κατάσταση (μνήμη) που επηρεάζεται από προηγούμενες εισόδους στην ακολουθία. Καλό στην μοντελοποίηση χρονικών/σειριακών εξαρτήσεων.              | Για Κείμενο (μετάφραση, παραγωγή, ανάλυση συναισθήματος), Χρονοσειρές (πρόβλεψη), Αναγνώριση ομιλίας.                       | E7 (Δ1)                                   |
| Γενεσιουργά Αντιπαλικά ΝΔ (GAN) | Αρχιτεκτονική ΝΔ / Γενεσιουργό Μοντέλο | Δημιουργία νέων δεδομένων που μοιάζουν με τα δεδομένα εκπαίδευσης.                  | Αποτελείται από 2 δίκτυα (Generator, Discriminator) που ανταγωνίζονται. Ο Generator μαθαίνει να παράγει ψεύτικα, ο Discriminator μαθαίνει να τα ξεχωρίζει από τα αληθινά. | Παραγωγή Ρεαλιστικών Εικόνων, Βίντεο, Ήχου, Συνθετικών Δεδομένων.                                                        | E7 (ως GANs), E8, E11, E18 (Δ1)           |
| Autoencoder               | Μοντέλο ΝΔ / Μη-επιβλεπόμενη Τεχνική | Μάθηση μιας συμπιεσμένης αναπαράστασης (κωδικοποίηση) των δεδομένων.                  | Αποτελείται από έναν κωδικοποιητή (encoder) και έναν αποκωδικοποιητή (decoder). Εκπαιδεύεται να ανακατασκευάζει την είσοδο στην έξοδο. Χρησιμοποιείται η κωδικοποίηση. | Μείωση Διάστασης, Ανίχνευση Ανορμολιών, Αφαίρεση Θορύβου.                                                                  | E8 (Δ1)                                   |
| Linear Regression         | Μοντέλο ML / Παραμετρικό              | Πρόβλεψη συνεχούς τιμής βασισμένη σε μια γραμμική σχέση με τις εισόδους.            | Γραμμικό μοντέλο. Εύκολο στην ερμηνεία. Ευαίσθητο σε outliers.                                                                                                  | Για προβλήματα Παλινδρόμησης (πρόβλεψη αριθμητικής τιμής).                                                                | E15 (Δ1), E10, E12, E18 (Δ2)              |
| Logistic Regression       | Μοντέλο ML / Παραμετρικό              | Πρόβλεψη πιθανότητας δυαδικής κατηγορίας.                                        | Χρησιμοποιεί τη σιγμοειδή συνάρτηση για να μετατρέψει την έξοδο σε πιθανότητα. Γραμμικός ταξινομητής.                                                              | Για προβλήματα Δυαδικής Ταξινόμησης (π.χ., ναι/όχι, 0/1).                                                                 | E9, E12, E16 (Δ2)                         |
| k-Means                   | Αλγόριθμος Clustering / Μη-επιβλεπόμενος | Ομαδοποίηση δεδομένων σε k συστάδες βάσει της εγγύτητας (απόστασης).                 | Αλγόριθμος βασισμένος στην απόσταση. Χρειάζεται να καθοριστεί το k εκ των προτέρων. Μπορεί να κολλήσει σε τοπικό βέλτιστο. Ευαίσθητο στην αρχικοποίηση και σε outliers. | Για προβλήματα Clustering (ομαδοποίηση δεδομένων).                                                                       | E1 (Δ2), E20 (Δ2 - Minibatch k-means)     |
| DBSCAN, OPTICS, Spectral Clustering | Αλγόριθμοι Clustering / Μη-επιβλεπόμενοι | Ομαδοποίηση δεδομένων.                                                          | **DBSCAN/OPTICS:** Βασίζονται στην πυκνότητα (density). Μπορούν να βρουν συστάδες αυθαίρετου σχήματος. Δεν χρειάζεται να καθοριστεί το πλήθος των συστάδων εκ των προτέρων. **Spectral:** Χρησιμοποιεί ιδιοτιμές/ιδιοδιανύσματα ενός πίνακα ομοιότητας/γειτνίασης. | Για προβλήματα Clustering, ειδικά όταν οι συστάδες δεν είναι σφαιρικές ή όταν υπάρχουν outliers.                         | E20 (Δ2)                                  |
| Decision Tree             | Μοντέλο ML / Μη-παραμετρικό (σε πλήρη ανάπτυξη) | Λήψη αποφάσεων (ταξινόμηση ή παλινδρόμηση) βάσει μιας δενδροειδούς δομής ερωτήσεων. | Εύκολο στην ερμηνεία (αν δεν είναι πολύ βαθύ). Μπορεί να χειριστεί γραμμικές και μη-γραμμικές σχέσεις. Επιρρεπές σε overfitting αν αφεθεί να αναπτυχθεί πλήρως.         | Για προβλήματα Ταξινόμησης και Παλινδρόμησης.                                                                            | E2, E8, E10, E22 (Δ2)                     |
| k-Nearest Neighbors (kNN) | Αλγόριθμος ML / Μη-παραμετρικό / Lazy Learning | Ταξινόμηση/Παλινδρόμηση νέας περίπτωσης βάσει της πλειοψηφίας/μέσου των k πιο κοντινών γειτόνων στην εκπαίδευση. | Δεν υπάρχει ρητή φάση εκπαίδευσης ("lazy"). Η απόφαση λαμβάνεται κατά τη στιγμή της πρόβλεψης. Ευαίσθητο στην επιλογή του k και στην κλίμακα των χαρακτηριστικών.   | Για προβλήματα Ταξινόμησης και Παλινδρόμησης. Απαιτείται αποδοτικός τρόπος εύρεσης γειτόνων για μεγάλα datasets (π.χ. KD-trees, hashing). | E7, E17, E18, E22 (Δ2)                    |
| Naïve Bayes               | Μοντέλο ML / Παραμετρικό              | Ταξινόμηση βάσει πιθανοτήτων και του θεωρήματος Bayes, υποθέτοντας ανεξαρτησία χαρακτηριστικών. | Απλό και γρήγορο. Λειτουργεί καλά ακόμα και αν η υπόθεση ανεξαρτησίας δεν ισχύει απόλυτα στην πράξη. Υπολογίζει υπό συνθήκη πιθανότητες.                           | Κυρίως για Ταξινόμηση, ιδιαίτερα σε προβλήματα κειμένου (π.χ. spam filtering).                                             | E7, E9, E18 (Δ2)                          |
| SVM (Support Vector Machine) | Μοντέλο ML                            | Βρίσκει τον βέλτιστο διαχωριστικό υπερεπίπεδο μεταξύ κλάσεων.                      | **Hard-margin:** Για γραμμικά διαχωρίσιμα δεδομένα, δεν επιτρέπει σφάλματα εντός του ορίου. **Soft-margin:** Επιτρέπει κάποια σφάλματα για μη-γραμμικά διαχωρίσιμα δεδομένα. **Kernel trick:** Μπορεί να χειριστεί μη-γραμμικά προβλήματα με χρήση κατάλληλων kernels (π.χ. radial basis function), αν και αναφέρεται γραμμικό kernel. | Για προβλήματα Ταξινόμησης (και Παλινδρόμησης, SVR). Ιδιαίτερα αποδοτικό σε χώρους υψηλής διάστασης.                      | E6, E16, E18, E22 (Δ2)                    |
| **Συναρτήσεις**           |                                         |                                                                                 |                                                                                                                                                                 |                                                                                                                          |                                           |
| Συνάρτηση Ενεργοποίησης | Στοιχείο ΝΔ                             | Εισάγει μη-γραμμικότητα στην έξοδο του νευρώνα.                                    | Καθορίζει την έξοδο ενός νευρώνα βάσει της βαρύτητας των εισόδων του. Απαραίτητη για να μοντελοποιήσουν τα ΝΔ μη-γραμμικές σχέσεις.                                    | Χρησιμοποιείται στην έξοδο κάθε νευρώνα (εκτός ίσως από τους νευρώνες εισόδου).                                         | E1, E4, E5, E10, E13, E14 (Δ1)            |
| Sigmoid                   | Συνάρτηση Ενεργοποίησης                 | Μετατρέπει οποιαδήποτε τιμή σε εύρος [0, 1].                                     | Συμπιέζει την έξοδο. Μπορεί να οδηγήσει σεvanishing gradient σε βαθιά δίκτυα (για τιμές μακριά από το 0).                                                        | Ιστορικά σε κρυμμένα επίπεδα. Συχνά σε επίπεδα εξόδου για δυαδική ταξινόμηση (ως πιθανότητα).                            | E1, E10, E13 (Δ1)                         |
| ReLU (Rectified Linear Unit) | Συνάρτηση Ενεργοποίησης                 | Μετατρέπει αρνητικές τιμές σε 0 και διατηρεί τις θετικές.                          | Απλή, αποδοτική υπολογιστικά. Βοηθά στη μείωση του vanishing gradient για θετικές τιμές. Μπορεί να οδηγήσει σε "νεκρούς" νευρώνες (dying ReLU).                    | Πολύ συχνά σε κρυμμένα επίπεδα Νευρωνικών Δικτύων.                                                                       | E5 (Δ1)                                   |
| Tanh (Hyperbolic Tangent) | Συνάρτηση Ενεργοποίησης                 | Μετατρέπει οποιαδήποτε τιμή σε εύρος [-1, 1].                                    | Συμπιέζει την έξοδο. Κέντρο στο 0 (σε αντίθεση με τη Sigmoid). Επίσης επιρρεπής σε vanishing gradient.                                                            | Σε κρυμμένα επίπεδα Νευρωνικών Δικτύων (συχνά καλύτερη από τη Sigmoid).                                                  | E5 (Δ1)                                   |
| Leaky ReLU                | Συνάρτηση Ενεργοποίησης                 | Παρόμοια με ReLU, αλλά επιτρέπει μια μικρή κλίση στις αρνητικές τιμές (π.χ. 0.01x). | Αντιμετωπίζει το πρόβλημα των "νεκρών" νευρώνων της ReLU. Βοηθά περαιτέρω στη μείωση του vanishing gradient.                                                     | Σε κρυμμένα επίπεδα Νευρωνικών Δικτύων, ως βελτίωση της ReLU.                                                              | E5 (Δ1)                                   |
| Softmax                   | Συνάρτηση Ενεργοποίησης                 | Μετατρέπει μια ακολουθία αριθμών σε μια κατανομή πιθανότητας (άθροισμα 1).        | Χρησιμοποιείται συνήθως στο τελικό επίπεδο για προβλήματα πολυκατηγορικής ταξινόμησης.                                                                       | Στο επίπεδο εξόδου Νευρωνικού Δικτύου για πολυκατηγορική ταξινόμηση.                                                     | E14 (Δ1)                                  |
| Συνάρτηση Απώλειας (Loss Function) | Στοιχείο Εκπαίδευσης                  | Μετράει πόσο "λάθος" είναι η πρόβλεψη του μοντέλου.                               | Η βελτιστοποίηση (μείωση) της συνάρτησης απώλειας οδηγεί στην εκπαίδευση του μοντέλου. Διαφορετικές συναρτήσεις για διαφορετικούς τύπους προβλημάτων (ταξινόμηση/παλινδρόμηση). | Για την εκπαίδευση επιβλεπόμενων μοντέλων (π.χ. Νευρωνικά Δίκτυα, Γραμμική/Λογιστική Παλινδρόμηση, SVMs).                | E6, E10 (Δ1)                              |
| MSE (Mean Squared Error)  | Συνάρτηση Απώλειας (για Παλινδρόμηση) | Μετράει τον μέσο όρο των τετραγώνων των σφαλμάτων (διαφοράς) πρόβλεψης-πραγματικής τιμής. | Ποινικοποιεί περισσότερο τις μεγάλες αποκλίσεις. Ευαίσθητο σε outliers.                                                                                         | Για προβλήματα Παλινδρόμησης.                                                                                            | E10 (Δ1)                                  |
| Cross-Entropy Loss        | Συνάρτηση Απώλειας (για Ταξινόμηση)   | Μετράει τη διαφορά μεταξύ της προβλεπόμενης κατανομής πιθανότητας και της πραγματικής. | Χρησιμοποιείται συχνά σε συνδυασμό με τη Softmax στην έξοδο για πολυκατηγορική ταξινόμηση. Μεγαλύτερη ποινή όταν η προβλεπόμενη πιθανότητα της σωστής κλάσης είναι χαμηλή. | Για προβλήματα Ταξινόμησης (δυαδικής ή πολυκατηγορικής).                                                                  | E10 (Δ1)                                  |
| RMSE (Root Mean Squared Error) | Μετρική Αξιολόγησης (για Παλινδρόμηση) | Η τετραγωνική ρίζα του MSE.                                                       | Στην ίδια μονάδα μέτρησης με την μεταβλητή εξόδου. Ποινικοποιεί περισσότερο τις μεγάλες αποκλίσεις.                                                               | Για αξιολόγηση μοντέλων Παλινδρόμησης.                                                                                    | E21 (Δ2)                                  |
| MAE (Mean Absolute Error) | Μετρική Αξιολόγησης (για Παλινδρόμηση) | Ο μέσος όρος των απόλυτων τιμών των σφαλμάτων πρόβλεψης-πραγματικής τιμής.         | Λιγότερο ευαίσθητο σε outliers από το MSE/RMSE.                                                                                                                 | Για αξιολόγηση μοντέλων Παλινδρόμησης, ειδικά όταν τα outliers αποτελούν πρόβλημα.                                          | E21 (Δ2)                                  |
| F1-Score                  | Μετρική Αξιολόγησης (για Ταξινόμηση)  | Αρμονικός μέσος της ακρίβειας (Precision) και της ανάκλησης (Recall).             | Ιδιαίτερα χρήσιμο για μη ισορροπημένα datasets (imbalanced datasets).                                                                                           | Για αξιολόγηση μοντέλων Ταξινόμησης, ειδικά όταν είναι σημαντικό να λαμβάνεται υπόψη η Precision και η Recall ταυτόχρονα. | E21 (Δ2)                                  |
| AUC-ROC                   | Μετρική Αξιολόγησης (για Ταξινόμηση)  | Μετράει την ικανότητα ενός ταξινομητή να διακρίνει μεταξύ κλάσεων σε διάφορα thresholds. | Η τιμή κυμαίνεται από 0 έως 1. Μια τιμή 1 σημαίνει τέλεια διάκριση. Λιγότερο ευαίσθητο σε μη ισορροπημένα datasets από την απλή Accuracy.                           | Για αξιολόγηση μοντέλων Ταξινόμησης, ειδικά σε δυαδική ταξινόμηση.                                                      | E21 (Δ2)                                  |
| **Τεχνικές / Διαδικασίες** |                                         |                                                                                 |                                                                                                                                                                 |                                                                                                                          |                                           |
| Backpropagation           | Αλγόριθμος Εκπαίδευσης ΝΔ               | Υπολογισμός των κλίσεων (gradients) της συνάρτησης απώλειας ως προς τα βάρη.        | Βασίζεται στον κανόνα της αλυσίδας (Chain Rule). Επιτρέπει την αποδοτική ενημέρωση των βαρών για τη μείωση της απώλειας.                                             | Για την εκπαίδευση Νευρωνικών Δικτύων (μαζί με Gradient Descent).                                                      | E3, E6, E13 (Δ1)                          |
| Stochastic Gradient       | Μέθοδος Βελτιστοποίησης                 | Εκτίμηση της κλίσης (gradient) και ενημέρωση των βαρών χρησιμοποιώντας ένα μικρό υποσύνολο (mini-batch) των δεδομένων κάθε φορά. | Πιο γρήγορο από το Batch Gradient Descent σε μεγάλα datasets. Εισάγει "θόρυβο" που μπορεί να βοηθήσει στην αποφυγή τοπικών βέλτιστων.                             | Για την εκπαίδευση Νευρωνικών Δικτύων και άλλων μοντέλων σε μεγάλα datasets.                                              | E2 (Δ1)                                   |
| Dynamic Programming       | Τεχνική Βελτιστοποίησης                 | Λύση σύνθετων προβλημάτων αναλύοντάς τα σε επικαλυπτόμενα υποπροβλήματα.             | Αποθηκεύει τα αποτελέσματα των υποπροβλημάτων για να μην τα υπολογίσει ξανά.                                                                                   | Σε προβλήματα όπου η βέλτιστη λύση μπορεί να χτιστεί από τις βέλτιστες λύσεις υποπροβλημάτων (π.χ. αλγόριθμοι κοστολόγησης). | E3 (Δ1) - *Σχετίζεται με την αποδοτικότητα υπολογισμού των κλίσεων στην Backpropagation, αν και ο κύριος μηχανισμός είναι η Chain Rule.* |
| Data Preprocessing        | Διαδικασία Προετοιμασίας Δεδομένων      | Μετασχηματισμός των ακατέργαστων δεδομένων σε κατάλληλη μορφή για τα μοντέλα.        | Περιλαμβάνει καθαρισμό, μετασχηματισμό, επιλογή χαρακτηριστικών κ.α. Απαραίτητο για την καλή απόδοση των μοντέλων.                                            | Πριν από την εκπαίδευση οποιουδήποτε μοντέλου ML/NN.                                                                   | E6 (ως σκοπός backprop - ΛΑΘΟΣ), E7, E19 (Δ2) |
| Dimensionality Reduction  | Τεχνική Preprocessing / Μη-επιβλεπόμενη | Μείωση του αριθμού των χαρακτηριστικών διατηρώντας σημαντικές πληροφορίες.          | Βοηθά στην οπτικοποίηση, μειώνει τον υπολογιστικό φόρτο, αντιμετωπίζει το "curse of dimensionality", μπορεί να μειώσει το overfitting.                               | Όταν τα δεδομένα έχουν πολύ υψηλή διάσταση, για βελτίωση της απόδοσης ή/και μείωση του θορύβου.                         | E7, E19 (Δ2)                              |
| Normalization             | Τεχνική Preprocessing                 | Προσαρμογή της κλίμακας των χαρακτηριστικών σε ένα κοινό εύρος.                     | Βοηθά πολλούς αλγόριθμους (ιδιαίτερα αυτούς που βασίζονται σε αποστάσεις ή gradient descent) να συγκλίνουν πιο γρήγορα και καλύτερα.                                 | Πριν την εκπαίδευση μοντέλων όπως ΝΔ, SVM, kNN, Gradient Descent βασισμένα μοντέλα.                                      | E19 (Δ2)                                  |
| Outliers' Removal         | Τεχνική Preprocessing                 | Εντοπισμός και χειρισμός (αφαίρεση ή μετασχηματισμός) σημείων δεδομένων που αποκλίνουν σημαντικά. | Οι outliers μπορούν να επηρεάσουν αρνητικά την εκπαίδευση ορισμένων μοντέλων (π.χ. Γραμμική Παλινδρόμηση, k-Means). Ο χειρισμός εξαρτάται από την αιτία τους.        | Όταν οι outliers είναι αποτέλεσμα σφάλματος μέτρησης ή καταγραφής και επηρεάζουν αρνητικά το μοντέλο.                   | E14, E19 (Δ2)                             |
| Class-Imbalance Handling  | Τεχνική Preprocessing / Εκπαίδευσης     | Αντιμετώπιση datasets όπου οι κλάσεις έχουν πολύ διαφορετικό πλήθος δειγμάτων.      | Μοντέλα εκπαιδευμένα σε imbalanced data τείνουν να κάνουν λάθος στις μειοψηφικές κλάσεις. Τεχνικές: oversampling, undersampling, χρήση κατάλληλων μετρικών (F1, AUC). | Όταν η κατανομή των κλάσεων στο training data είναι πολύ άνιση.                                                        | E19 (Δ2)                                  |
| Stratification            | Τεχνική Διαίρεσης Δεδομένων           | Διαίρεση dataset (π.χ. σε train/test) διατηρώντας την αναλογία των κλάσεων.          | Απαραίτητο σε προβλήματα ταξινόμησης, ειδικά με imbalanced data, για να διασφαλιστεί ότι κάθε subset (π.χ. test set) αντιπροσωπεύει σωστά την κατανομή των κλάσεων. | Κατά τη δημιουργία train/test/validation sets σε προβλήματα Ταξινόμησης.                                               | E11 (Δ2)                                  |
| Cross Validation          | Τεχνική Αξιολόγησης Μοντέλου            | Αξιολόγηση της απόδοσης του μοντέλου χρησιμοποιώντας πολλαπλές διαιρέσεις train/validation. | Δίνει μια πιο αξιόπιστη εκτίμηση της γενίκευσης του μοντέλου σε άγνωστα δεδομένα σε σχέση με ένα απλό train/test split. Βοηθά στον εντοπισμό overfitting.          | Για αξιολόγηση και επιλογή μοντέλου/υπερπαραμέτρων, ειδικά σε σχετικά μικρά datasets.                                     | E11 (Δ2)                                  |
| Bootstrapping             | Τεχνική Δειγματοληψίας                | Δημιουργία πολλαπλών datasets με δειγματοληψία με επανατοποθέτηση από το αρχικό dataset. | Χρησιμοποιείται ως βάση για αλγόριθμους ensemble (π.χ. Bagging, Random Forests) και για εκτίμηση διαστήματος εμπιστοσύνης (confidence intervals).             | Σε αλγόριθμους ensemble, για στατιστική ανάλυση.                                                                         | E11 (Δ2)                                  |
| Transfer Learning         | Τεχνική Εκπαίδευσης Μοντέλων          | Χρήση ενός προ-εκπαιδευμένου μοντέλου (σε ένα μεγάλο dataset) ως αφετηρία για μια νέα, παρόμοια εργασία. | Επιταχύνει την εκπαίδευση. Απαιτεί λιγότερα δεδομένα για τη νέα εργασία. Πολύ αποτελεσματικό σε τομείς όπως η επεξεργασία εικόνας/κειμένου.                   | Όταν έχουμε περιορισμένα δεδομένα για μια νέα εργασία, αλλά υπάρχει διαθέσιμο προ-εκπαιδευμένο μοντέλο από παρόμοια εργασία. | E8 (Δ1)                                   |
| Dropout                   | Τεχνική Τακτικοποίησης (Regularization) | Κατά την εκπαίδευση ΝΔ, "απενεργοποιούνται" τυχαία νευρώνες και οι συνδέσεις τους. | Μειώνει την συνεξάρτηση (co-adaptation) των νευρώνων. Λειτουργεί ως ensemble πολλών μικρότερων δικτύων. Μειώνει το overfitting.                                 | Κατά την εκπαίδευση Νευρωνικών Δικτύων για τη μείωση του overfitting.                                                    | E10 (Δ1), E16 (Δ2)                        |
| Batch Normalization       | Τεχνική Τακτικοποίησης / Βελτιστοποίησης | Κανονικοποίηση των εισόδων κάθε επιπέδου μέσα σε κάθε mini-batch κατά την εκπαίδευση. | Μειώνει το "internal covariate shift". Επιτρέπει τη χρήση μεγαλύτερου learning rate. Λειτουργεί και ως τακτικοποίηση (μειώνει overfitting). Βελτιώνει τη σύγκλιση. | Κατά την εκπαίδευση Νευρωνικών Δικτύων, συνήθως μεταξύ συνελικτικών/πλήρως συνδεδεμένων επιπέδων και της συνάρτησης ενεργοποίησης. | E12 (Δ1), E16 (Δ2)                        |
| Lasso (L1 Regularization) | Τεχνική Τακτικοποίησης                  | Προσθήκη ποινής (L1 norm) στην συνάρτηση απώλειας, ανάλογη με την απόλυτη τιμή των βαρών. | Τείνει να μηδενίζει τα βάρη για λιγότερο σημαντικά χαρακτηριστικά (feature selection). Μειώνει το overfitting.                                                    | Σε γραμμικά μοντέλα (π.χ. Linear/Logistic Regression) για τακτικοποίηση και επιλογή χαρακτηριστικών.                     | E15 (Δ2)                                  |
| L2 Regularization (Ridge) | Τεχνική Τακτικοποίησης                  | Προσθήκη ποινής (L2 norm) στην συνάρτηση απώλειας, ανάλογη με το τετράγωνο των βαρών. | Τείνει να συρρικνώνει τα βάρη προς το μηδέν, αλλά σπάνια τα μηδενίζει εντελώς. Μειώνει το overfitting.                                                            | Σε γραμμικά μοντέλα (π.χ. Linear/Logistic Regression, ΝΔ) για τακτικοποίηση.                                             | E15 (Δ2 - αναφέρεται η L2 norm)           |
| End-to-End Training       | Μέθοδος Εκπαίδευσης                     | Εκπαίδευση ενός συστήματος (π.χ. ΝΔ) ως ενιαίο σύνολο, από την είσοδο στην έξοδο, βελτιστοποιώντας όλες τις παραμέτρους μαζί. | Συνήθως απαιτεί λιγότερο feature engineering. Μπορεί να είναι πιο δύσκολο στην εκπαίδευση και στην ερμηνεία.                                                    | Σε σύνθετα συστήματα (π.χ. ΝΔ με πολλά επίπεδα ή διαφορετικούς τύπους επιπέδων) όπου θέλουμε να βελτιστοποιήσουμε απευθείας την τελική έξοδο. | E17 (Δ1)                                  |
| **Προβλήματα / Έννοιες Εκπαίδευσης** |                                         |                                                                                 |                                                                                                                                                                 |                                                                                                                          |                                           |
| Vanishing Gradient        | Πρόβλημα Εκπαίδευσης ΝΔ                 | Οι κλίσεις (gradients) γίνονται εξαιρετικά μικρές κατά την backpropagation, εμποδίζοντας την εκμάθηση σε πρώιμα επίπεδα. | Συμβαίνει συχνά σε βαθιά δίκτυα με συναρτήσεις ενεργοποίησης Sigmoid ή Tanh.                                                                                    | Επηρεάζει την εκπαίδευση βαθιών Νευρωνικών Δικτύων. Αντιμετωπίζεται με ReLU/Leaky ReLU, Batch Norm, αρχιτεκτονικές (π.χ. ResNets). | E5, E12, E13 (Δ1)                         |
| Overfitting               | Πρόβλημα Εκπαίδευσης Μοντέλου            | Το μοντέλο μαθαίνει πολύ καλά τα δεδομένα εκπαίδευσης, αλλά αποδίδει άσχημα σε νέα, άγνωστα δεδομένα. | Συμβαίνει όταν το μοντέλο είναι υπερβολικά πολύπλοκο σε σχέση με την ποσότητα/ποιότητα των δεδομένων. Μαθαίνει το "θόρυβο" των δεδομένων εκπαίδευσης.                | Όταν το μοντέλο δεν γενικεύει καλά. Αντιμετωπίζεται με τακτικοποίηση (dropout, L1/L2, Batch Norm), αύξηση δεδομένων (data augmentation), μείωση πολυπλοκότητας μοντέλου. | E10, E15 (Δ1), E8, E13, E16 (Δ2)          |
| Decision Boundaries       | Έννοια Ταξινόμησης                      | Οι "γραμμές" ή "επιφάνειες" που διαχωρίζουν τις διαφορετικές κλάσεις στον χώρο των χαρακτηριστικών. | Μπορούν να είναι γραμμικές (π.χ. Perceptron, Γραμμική Παλινδρόμηση, Logistic Regression, Linear SVM) ή μη-γραμμικές (π.χ. ΝΔ με κρυμμένα επίπεδα, SVM με non-linear kernel). | Για την κατανόηση του πώς ένα μοντέλο ταξινόμησης διαχωρίζει τις κλάσες.                                                 | E1 (Δ1)                                   |
| Τοπικό Βέλτιστο (Local Optimum) | Έννοια Βελτιστοποίησης                 | Ένα σημείο στον χώρο παραμέτρων όπου η συνάρτηση απώλειας είναι ελάχιστη σε σχέση με τα γειτονικά σημεία, αλλά όχι απαραίτητα σε όλο τον χώρο. | Αλγόριθμοι βελτιστοποίησης (όπως Gradient Descent) μπορούν να κολλήσουν σε τοπικά βέλτιστα.                                                                     | Επηρεάζει την εκπαίδευση μοντέλων όπως k-Means και Νευρωνικά Δίκτυα.                                                      | E1 (Δ2)                                   |
| Mode Collapse             | Πρόβλημα Εκπαίδευσης GAN                 | Ο Generator σε ένα GAN αρχίζει να παράγει μόνο ένα περιορισμένο υποσύνολο της κατανομής των δεδομένων (π.χ. μόνο ένα είδος Pokemon). | Ο Generator δεν μαθαίνει την πλήρη ποικιλία των δεδομένων. Δύσκολο πρόβλημα στην εκπαίδευση των GANs.                                                           | Κατά την εκπαίδευση GANs.                                                                                                | E18 (Δ1)                                  |
| Parametric Model          | Κατηγορία Μοντέλου                      | Μοντέλο με σταθερό αριθμό παραμέτρων, ανεξάρτητο από το πλήθος των δεδομένων εκπαίδευσης. | Οι παράμετροι συνοψίζουν τα δεδομένα.                                                                                                                           | Π.χ. Γραμμική/Λογιστική Παλινδρόμηση, Naive Bayes, ΝΔ με σταθερή αρχιτεκτονική.                                           | E3, E18 (Δ2)                              |
| Non-parametric Model      | Κατηγορία Μοντέλου                      | Μοντέλο όπου ο αριθμός των παραμέτρων μπορεί να αυξηθεί με το πλήθος των δεδομένων εκπαίδευσης. | Το μοντέλο ουσιαστικά "θυμάται" (μέρος των) δεδομένων εκπαίδευσης. Πιο ευέλικτο, αλλά απαιτεί περισσότερα δεδομένα για να αποφύγει το overfitting και είναι υπολογιστικά πιο ακριβό στην πρόβλεψη (π.χ. kNN). | Π.χ. kNN, Decision Trees (που αναπτύσσονται πλήρως).                                                                   | E3, E18 (Δ2)                              |

**Πρόσθετες Σημειώσεις για Μελέτη**

1.  **Διαφορές Parametric vs Non-Parametric:** Κατανοήστε γιατί τα Parametric models έχουν σταθερό αριθμό παραμέτρων (π.χ., η εξίσωση της γραμμικής παλινδρόμησης έχει σταθερό πλήθος συντελεστών), ενώ τα Non-Parametric (π.χ., kNN) ουσιαστικά χρησιμοποιούν τα ίδια τα δεδομένα εκπαίδευσης για την πρόβλεψη, άρα η "πολυπλοκότητα" τους εξαρτάται από το μέγεθος του dataset.
2.  **Είδη Εκμάθησης:**
    *   **Επιβλεπόμενη (Supervised):** Έχουμε δεδομένα εισόδου (X) και αντίστοιχες ετικέτες/τιμές εξόδου (y). Στόχος είναι να μάθουμε μια συνάρτηση που αντιστοιχίζει το X στο y. (π.χ., Ταξινόμηση, Παλινδρόμηση - Logistic Regression, SVM, Decision Trees, kNN, NN).
    *   **Μη-επιβλεπόμενη (Unsupervised):** Έχουμε μόνο δεδομένα εισόδου (X), χωρίς ετικέτες. Στόχος είναι να βρούμε δομή ή μοτίβα στα δεδομένα. (π.χ., Clustering - k-Means, DBSCAN, Spectral Clustering, Μείωση Διάστασης - Autoencoders).
    *   **Ημι-επιβλεπόμενη (Semi-supervised):** Συνδυάζει λίγα επισημασμένα δεδομένα με πολλά μη επισημασμένα. (Δεν αναφέρεται άμεσα, αλλά καλό για γενική κατανόηση).
    *   **Ενισχυτική (Reinforcement Learning):** Ένας agent μαθαίνει να λαμβάνει αποφάσεις σε ένα περιβάλλον μέσω δοκιμής και σφάλματος, λαμβάνοντας ανταμοιβές ή ποινές. (Δεν αναφέρεται).
3.  **Ρυθμός Μάθησης (Learning Rate):** Είναι η υπερπαράμετρος που ελέγχει πόσο μεγάλα βήματα κάνει ο αλγόριθμος βελτιστοποίησης (π.χ., Gradient Descent) κατά την ενημέρωση των βαρών. Μικρός ρυθμός -> αργή εκπαίδευση, κίνδυνος να κολλήσει σε τοπικό βέλτιστο. Μεγάλος ρυθμός -> γρήγορη εκπαίδευση, κίνδυνος να προσπεράσει το βέλτιστο (diverge).
4.  **Bias-Variance Tradeoff:** Βασική έννοια.
    *   **Bias (Μεροληψία):** Το σφάλμα που προκύπτει από λανθασμένες υποθέσεις στον αλγόριθμο. Ένα μοντέλο με υψηλή μεροληψία "χάνει" τις πραγματικές σχέσεις στα δεδομένα (underfitting).
    *   **Variance (Διασπορά):** Το σφάλμα που προκύπτει από την ευαισθησία σε μικρές διακυμάνσεις στα δεδομένα εκπαίδευσης. Ένα μοντέλο με υψηλή διασπορά επηρεάζεται πολύ από το συγκεκριμένο dataset εκπαίδευσης και αποδίδει άσχημα σε νέα δεδομένα (overfitting).
    *   Ο στόχος είναι να βρούμε μια ισορροπία. Πιο απλά μοντέλα έχουν συνήθως υψηλό bias, χαμηλή variance. Πιο πολύπλοκα μοντέλα έχουν χαμηλό bias, υψηλή variance.
5.  **Χειρισμός Overfitting:** Όπως αναφέρθηκε στον πίνακα: Data Augmentation, Dropout, Batch Normalization, L1/L2 Regularization, Early Stopping (διακοπή εκπαίδευσης όταν η απόδοση στο validation set αρχίζει να χειροτερεύει), μείωση πολυπλοκότητας μοντέλου, αύξηση training data.
6.  **Αρχικοποίηση Βαρών (Weight Initialization):** Η επιλογή αρχικών τιμών για τα βάρη σε ένα ΝΔ. Η αρχικοποίηση με μηδέν (Ε2, Δ1) είναι ΚΑΚΗ ιδέα για ΝΔ με κρυμμένα επίπεδα, γιατί όλοι οι νευρώνες σε ένα επίπεδο θα μαθαίνουν ταυτόχρονα το ίδιο πράγμα (όλοι οι gradient θα είναι ίδιοι). Χρειάζεται τυχαία αρχικοποίηση (μικρές τυχαίες τιμές, π.χ., Glorot/Xavier, He initialization).
7.  **Chain Rule & Backpropagation:** Η Chain Rule (κανόνας της αλυσίδας) από τον διαφορικό λογισμό είναι το θεμελιώδες μαθηματικό εργαλείο που επιτρέπει στην backpropagation να υπολογίσει τις κλίσεις (gradients) της συνάρτησης απώλειας ως προς κάθε βάρος, δουλεύοντας ανάποδα από την έξοδο προς την είσοδο του δικτύου.
8.  **Σκοπός Backpropagation (Ε6, Δ1):** Ο *πρωταρχικός* σκοπός δεν είναι να υπολογίσει τη συνάρτηση απώλειας (αυτό είναι απλή forward pass), ούτε να καθορίσει τον αριθμό των επιπέδων, ούτε να προ-επεξεργαστεί τα δεδομένα. Είναι να υπολογίσει τις κλίσεις (gradients) που χρειάζονται για την ενημέρωση των βαρών. Η ενημέρωση των βαρών γίνεται από τον αλγόριθμο βελτιστοποίησης (π.χ., Gradient Descent) *μετά* τον υπολογισμό των κλίσεων από την backpropagation.

Αυτός ο πίνακας, σε συνδυασμό με τις πρόσθετες σημειώσεις, θα πρέπει να καλύπτει τις περισσότερες έννοιες και τις σχέσεις μεταξύ τους όπως παρουσιάζονται στα διαγωνίσματα, βοηθώντας στην κατανόηση και την απομνημόνευση.
