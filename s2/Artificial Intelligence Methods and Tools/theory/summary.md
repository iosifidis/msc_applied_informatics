# ΣΥΝΟΛΙΚΗ ΠΕΡΙΛΗΨΗ

Η προηγούμενη ανάλυση κάλυψε τις περισσότερες έννοιες, αλλά ίσως όχι με τον τρόπο που τις συνδέει μια συνολική επισκόπηση ή καλύπτοντας *όλες* τις λεπτομέρειες των επιλογών σε κάθε ερώτηση.

Βάσει των ερωτήσεων που μου δώσατε, μπορώ να βγάλω μια πιο συγκεντρωτική και αναλυτική περίληψη των βασικών εννοιών και μεθόδων, ομαδοποιώντας τες λογικά, ώστε να είναι πιο χρήσιμη για να απαντήσετε σε παρόμοιες ερωτήσεις.

Ας οργανώσουμε την ύλη στις ακόλουθες ενότητες, όπως αυτές προκύπτουν από τις ερωτήσεις σας:

**Συγκεντρωτική Αναλυτική Περίληψη Μεθόδων & Εννοιών από τις Ερωτήσεις**

**Α. Θεμελιώδεις Έννοιες Μηχανικής Μάθησης**

1.  **Τύποι Μάθησης (Learning Types):**
    *   **Επιβλεπόμενη Μάθηση (Supervised Learning):** Εκπαίδευση σε ζεύγη εισόδου-εξόδου (δεδομένα *με ετικέτες*). Στόχος είναι η εκμάθηση μιας συνάρτησης που αντιστοιχεί την είσοδο στην έξοδο.
        *   **Ταξινόμηση (Classification):** Η έξοδος είναι μια διακριτή κατηγορία (π.χ., email είναι spam ή όχι, εικόνα είναι γάτα ή σκύλος).
        *   **Παλινδρόμηση (Regression):** Η έξοδος είναι μια συνεχής τιμή (π.χ., πρόβλεψη τιμής σπιτιού, πρόβλεψη θερμοκρασίας).
    *   **Μη-Επιβλεπόμενη Μάθηση (Unsupervised Learning):** Εκπαίδευση σε δεδομένα *χωρίς ετικέτες*. Στόχος είναι η εύρεση δομών, μοτίβων ή συνόψεων στα δεδομένα.
        *   **Συσταδοποίηση (Clustering):** Ομαδοποίηση παρόμοιων σημείων δεδομένων (π.χ., ομαδοποίηση πελατών - αναφέρεται στον k-Means, Ε1 ΜΜ, και στις πυκνο-βασισμένες τεχνικές DBSCAN/OPTICS, Ε20 ΜΜ).
        *   **Μείωση Διάστασης (Dimensionality Reduction):** Μείωση του αριθμού των χαρακτηριστικών (π.χ., PCA, Autoencoders - Ε7, Ε19 ΜΜ, Ε8 ΝΝ). Στόχος: αποφυγή "πρόβλημα υψηλών διαστάσεων" (curse of dimensionality), μείωση θορύβου, ταχύτερη εκπαίδευση, οπτικοποίηση.
        *   Εύρεση κανόνων συσχέτισης, κ.ά.

2.  **Κατηγοριοποιήσεις Μοντέλων:**
    *   **Παραμετρικά (Parametric) vs. Μη-Παραμετρικά (Non-parametric):** (Ε3, Ε18 ΜΜ)
        *   **Παραμετρικά:** Κάνουν υποθέσεις για τη μορφή της συνάρτησης (π.χ., γραμμική) και μαθαίνουν ένα σταθερό, πεπερασμένο σύνολο παραμέτρων. Ο αριθμός των παραμέτρων δεν εξαρτάται από τον αριθμό των δειγμάτων εκπαίδευσης. Π.χ.: Linear/Logistic Regression, Naïve Bayes. (Ε3, Ε18 ΜΜ)
        *   **Μη-Παραμετρικά:** Δεν κάνουν ισχυρές υποθέσεις για τη μορφή της συνάρτησης. Η πολυπλοκότητα του μοντέλου (ή/και η ανάγκη διατήρησης δεδομένων εκπαίδευσης) αυξάνεται με το μέγεθος του συνόλου εκπαίδευσης. Π.χ.: kNN, Decision Trees, SVMs (με kernels), Νευρωνικά Δίκτυα. (Ε3, Ε18 ΜΜ)
    *   **Γραμμικά (Linear) vs. Μη-Γραμμικά (Non-linear):** Αφορά το σχήμα του ορίου απόφασης (ταξινόμηση) ή τη μορφή της σχέσης (παλινδρόμηση). (Ε6, Ε10, Ε16, Ε22 ΜΜ, Ε1 ΝΝ)
        *   **Γραμμικά:** Μαθαίνουν γραμμικά όρια (υπερεπίπεδα) ή γραμμικές σχέσεις. Π.χ.: Linear Regression, Logistic Regression, SVM με γραμμικό kernel, Perceptron. (Ε6, Ε16, Ε18 ΜΜ). Ένα δίκτυο μόνο με γραμμικές "συναρτήσεις ενεργοποίησης" παραμένει γραμμικό.
        *   **Μη-Γραμμικά:** Μαθαίνουν μη-γραμμικά όρια ή σχέσεις. Απαιτούνται για πολύπλοκα δεδομένα. Π.χ.: Decision Trees, kNN, SVM με μη-γραμμικούς kernels, Νευρωνικά Δίκτυα με μη-γραμμικές συναρτήσεις ενεργοποίησης (Ε1, Ε4, Ε14 ΝΝ), Regression Trees για μη-γραμμική παλινδρόμηση (Ε10 ΜΜ).

**Β. Αλγόριθμοι και Μέθοδοι Μηχανικής Μάθησης (από Ερωτήσεις)**

1.  **Αλγόριθμοι Συσταδοποίησης (Clustering):**
    *   **k-Means:** (Ε1 ΜΜ) Κεντρο-βασισμένη (centroid-based) τεχνική. Συγκλίνει συνήθως σε *τοπικό βέλτιστο*. Ευαίσθητος σε αρχικοποίηση και outliers.
    *   **Density-based Clustering (πυκνο-βασισμένη):** Ομαδοποιεί σημεία που είναι κοντά μεταξύ τους σε "πυκνές" περιοχές, διαχωρίζοντάς τα από περιοχές χαμηλής πυκνότητας. Μπορεί να βρει συστάδες αυθαίρετου σχήματος και να χειριστεί outliers. Π.χ.: **DBSCAN, OPTICS**. (Ε20 ΜΜ)
    *   Άλλες (αναφέρονται ως επιλογές): **Minibatch k-means** (πιο γρήγορη εκδοχή k-Means), **Spectral Clustering** (βασισμένο στην ανάλυση του φάσματος μιας μήτρας ομοιότητας). (Ε20 ΜΜ)

2.  **Αλγόριθμοι Ταξινόμησης και Παλινδρόμησης (Επιβλεπόμενη Μάθηση):**
    *   **Decision Trees (Δέντρα Απόφασης):** (Ε2, Ε8, Ε22 ΜΜ) Δομή δέντρου, εύκολη ερμηνεία, χειρίζεται αριθμητικά/κατηγορικά features. Μπορεί να επιτύχει **0 σφάλμα εκπαίδευσης (training error)** σε datasets εκπαίδευσης αν έχει αρκετό βάθος (οδηγεί σε **υπερπροσαρμογή**). Η αύξηση βάθους *μπορεί* να βελτιώσει train error (Ε8 ΜΜ), αλλά όχι απαραίτητα test error (Ε2 ΜΜ).
    *   **Regression Trees:** (Ε10 ΜΜ) Έκδοση για παλινδρόμηση. Συνήθως χρησιμοποιούνται για μοντελοποίηση **μη γραμμικών δεδομένων**.
    *   **k-Nearest Neighbors (kNN):** (Ε7, Ε17, Ε18 ΜΜ, Ε22 ΜΜ) Απλός, μη-παραμετρικός, non-linear. Βασίζεται στην απόσταση. Ευαίσθητος σε κλίμακα features και outliers. Μπορεί να επιτύχει **0 σφάλμα εκπαίδευσης** με k=1 αν δεν υπάρχουν πανομοιότυπα σημεία με διαφορετικές ετικέτες στο train set. Η πολυπλοκότητα της πρόβλεψης είναι υψηλή για μεγάλα datasets (συνήθως Ο(n) ή Ο(nd) αν δεν χρησιμοποιούνται δομές ευρετηρίασης όπως KD-Trees Ο(log n)). Η επιλογή Ο(η²) στην Ε17 ΜΜ πιθανώς αναφέρεται σε έναν πολύ αργό τρόπο αναζήτησης γειτόνων ή σε προεπεξεργασία, αλλά Ο(η) ή Ο(λογη) είναι πιο τυπικές για την *πρόβλεψη*.
    *   **Naïve Bayes:** (Ε9, Ε18 ΜΜ) Παραμετρικός, ταξινόμηση. Βασίζεται στο θεώρημα Bayes. Κάνει την αφελή (naïve) υπόθεση της **υπό συνθήκη ανεξαρτησίας των χαρακτηριστικών** δεδομένης της κλάσης. Αν αυτή η υπόθεση δεν ισχύει, μοντέλα όπως η Logistic Regression μπορεί να είναι καλύτερα (Ε9 ΜΜ).
    *   **Logistic Regression:** (Ε9, Ε12, Ε18 ΜΜ) Παραμετρικός, κυρίως για ταξινόμηση. Μαθαίνει ένα **γραμμικό όριο** απόφασης. Παρέχει **υπό συνθήκη πιθανότητα** (conditioned probability) για την κάθε κλάση (Ε12 ΜΜ).
    *   **Linear Regression:** (Ε12, Ε15, Ε18 ΜΜ) Παραμετρικός, για παλινδρόμηση. Μαθαίνει μια **γραμμική σχέση** μεταξύ features και target. Η **Lasso** είναι μορφή Linear Regression με L1 κανονικοποίηση στα βάρη (ρυθμίζει τα βάρη με την **νόρμα L1** - Ε15 ΜΜ), βοηθώντας στην επιλογή χαρακτηριστικών. Η **Rige Regression** χρησιμοποιεί νόρμα L2 (Ε15 β ΜΜ είναι ψευδής).
    *   **Support Vector Machines (SVM):** (Ε6, Ε16, Ε18, Ε22 ΜΜ) Επιβλεπόμενη μάθηση (ταξινόμηση/παλινδρόμηση). Στόχος η εύρεση του βέλτιστου διαχωριστικού υπερεπιπέδου με μέγιστο περιθώριο.
        *   **Hard-margin SVM:** (Ε16, Ε22 ΜΜ) Λειτουργεί **μόνο** σε δεδομένα που είναι **τέλεια γραμμικώς διαχωρίσιμα** (linear separable). Σε αυτή την περίπτωση, μπορεί να επιτύχει **0 σφάλμα εκπαίδευσης**.
        *   **Soft-margin SVM:** Επιτρέπει μικρό σφάλμα στο train set, χειριζόμενο μη γραμμικά διαχωρίσιμα ή θορυβώδη δεδομένα. (Ε6, Ε16 ΜΜ)
        *   **Με Kernels (π.χ., RBF):** Επιτρέπει την εκμάθηση μη-γραμμικών ορίων, προβάλλοντας τα δεδομένα σε υψηλότερη διάσταση.

**Γ. Χειρισμός Δεδομένων & Αξιολόγηση (από Ερωτήσεις)**

1.  **Προεπεξεργασία (Preprocessing):** (Ε7, Ε19 ΜΜ)
    *   **Normalization (Κανονικοποίηση):** (Ε19 α. ΜΜ) Κλιμακώνει τα features σε κοινό εύρος ή κατανομή. Βοηθάει αλγορίθμους που βασίζονται σε αποστάσεις ή gradient descent.
    *   **Outliers' Removal (Απομάκρυνση Αποκλινουσών Τιμών):** (Ε14, Ε19 β. ΜΜ) Ο χειρισμός τους **εξαρτάται από το πρόβλημα και τον αλγόριθμο**. Δεν πρέπει να αφαιρούνται τυφλά (Ε14 δ. ΜΜ αληθές).
    *   **Dimensionality Reduction (Μείωση Διάστασης):** (Ε7, Ε19 δ. ΜΜ) Μειώνει τον αριθμό features. Ωφέλιμο πριν από πολλούς αλγορίθμους (kNN, Trees, Naïve Bayes, SVMs, NN).

2.  **Διαχωρισμός και Αξιολόγηση:**
    *   **Train/Test Split:** Διαχωρισμός σε σύνολο εκπαίδευσης και σύνολο ελέγχου για την αξιολόγηση της ικανότητας γενίκευσης του μοντέλου σε νέα δεδομένα (Ε4, Ε5 ΜΜ).
    *   **Overfitting (Υπερπροσαρμογή):** Όταν το μοντέλο "αποστηθίζει" το train set και έχει κακή επίδοση στο test set (Ε2, Ε4, Ε5, Ε8, Ε10, Ε13 ΜΜ, Ε10 ΝΝ). Το train accuracy είναι υψηλό, το test accuracy είναι σημαντικά χαμηλότερο (Ε5 ΜΜ, Ε10 ΝΝ).
    *   **Υποπροσαρμογή (Underfitting):** Το μοντέλο είναι πολύ απλό και δεν μπορεί να μάθει καλά ούτε καν τα δεδομένα εκπαίδευσης. Χαμηλή επίδοση σε train ΚΑΙ test set.
    *   **Generalization (Γενίκευση):** Η ικανότητα του μοντέλου να αποδίδει καλά σε αόρατα δεδομένα (test set). Αυτός είναι ο τελικός στόχος (Ε4, Ε5 ΜΜ, Ε10 ΝΝ, Ε15 ΝΝ). Η επίδοση στο train set **δεν αρκεί** για να ισχυριστούμε ότι έχουμε ένα καλό μοντέλο (Ε4 ΜΜ ψευδές).
    *   **Stratification (Στρωματοποίηση):** (Ε11 β. ΜΜ) Τεχνική διαχωρισμού που **διατηρεί** την αναλογία των κλάσεων (για ταξινόμηση) ή την κατανομή των τιμών (για παλινδρόμηση) στα επιμέρους σύνολα (train/test/validation). Απαραίτητο για unbalanced datasets.
    *   **Cross Validation (Διασταυρούμενη Επικύρωση):** (Ε11 α. ΜΜ) Πιο στιβαρή μέθοδος αξιολόγησης από ένα απλό train/test split. Βοηθά στην επιλογή υπερπαραμέτρων και καλύτερη εκτίμηση της γενίκευσης. (Δεν είναι το ίδιο με Stratification, αν και μπορούν να συνδυαστούν).

3.  **Μετρικές Αξιολόγησης:**
    *   **Για Ταξινόμηση (Classification):** (Ε5, Ε21 α, δ. ΜΜ)
        *   Accuracy (Ε5, Ε8 ΜΜ): Ποσοστό σωστών προβλέψεων. Μπορεί να παραπλανήσει σε unbalanced datasets.
        *   F1-score: Αρμονικός μέσος Precision και Recall. Χρήσιμο σε unbalanced datasets. (Ε21 α. ΜΜ)
        *   AUC-ROC (Area Under the Receiver Operating Characteristic Curve): Δείχνει πόσο καλά το μοντέλο διαχωρίζει θετικά από αρνητικά δείγματα. (Ε21 δ. ΜΜ)
    *   **Για Παλινδρόμηση (Regression):** (Ε21 γ, β. ΜΜ)
        *   MAE (Mean Absolute Error): Μέση απόλυτη διαφορά μεταξύ πρόβλεψης και πραγματικής τιμής. (Ε21 γ. ΜΜ)
        *   RMSE (Root Mean Squared Error): Τετραγωνική ρίζα του μέσου τετραγώνου των σφαλμάτων. Δίνει μεγαλύτερη βαρύτητα σε μεγάλες αποκλίσεις. (Ε21 β. ΜΜ)

**Δ. Νευρωνικά Δίκτυα (από Ερωτήσεις)**

1.  **Βασικά Δομικά Στοιχεία & Λειτουργία:**
    *   **Νευρώνας:** Υπολογίζει την έξοδο του εφαρμόζοντας μια **συνάρτηση ενεργοποίησης** στο σταθμισμένο άθροισμα των εισόδων συν ένα bias (Ε9 ΝΝ).
    *   **Βάρη (Weights) & Bias:** Οι παράμετροι που "μαθαίνει" το δίκτυο κατά την εκπαίδευση.
    *   **Συναρτήσεις Ενεργοποίησης (Activation Functions):** (Ε4, Ε5, Ε13, Ε14 ΝΝ) **Εισάγουν μη-γραμμικότητα** στο δίκτυο (Ε4 γ., Ε14 α., γ. ΝΝ), επιτρέποντας του να μάθει σύνθετες, μη-γραμμικές σχέσεις και όρια απόφασης (Ε1 ΝΝ). Χωρίς αυτές, το δίκτυο είναι γραμμικό.
        *   **Sigmoid & Tanh:** Συνηθισμένες, αλλά μπορεί να οδηγήσουν σε **vanishing gradient** (Ε5 β, Ε13 ΝΝ), ειδικά σε βαθιά δίκτυα, όπου οι κλίσεις γίνονται πολύ μικρές και η εκπαίδευση επιβραδύνεται ή σταματά (Ε13 δ. ΝΝ: μειώνεται το μέτρο της κλίσης, ενώ διατηρείται συνήθως το πρόσημο).
        *   **ReLU & Leaky ReLU:** (Ε5 α, γ. ΝΝ) Συνηθισμένες σήμερα, αντιμετωπίζουν το vanishing gradient για θετικές εισόδους.
        *   **Softmax:** (Ε14 γ. ΝΝ) Χρησιμοποιείται συνήθως στο *τελευταίο επίπεδο* για προβλήματα πολυωνυμικής ταξινόμησης, μετατρέποντας τις εξόδους σε **πιθανότητες** που αθροίζουν σε 1.
    *   **Μηδενική Αρχικοποίηση Βαρών (Zero Weight Initialization):** (Ε2 ΝΝ) **Δεν είναι αποδεκτή ιδέα** για τα περισσότερα multi-layer δίκτυα, καθώς όλοι οι νευρώνες στο ίδιο κρυμμένο επίπεδο θα υπολογίζουν τις ίδιες τιμές και οι κλίσεις (gradients) θα είναι πανομοιότυπες στην backpropagation, εμποδίζοντας το δίκτυο να μάθει διαφορετικά χαρακτηριστικά. Συνήθως χρησιμοποιείται τυχαία αρχικοποίηση.

2.  **Εκπαίδευση Νευρωνικών Δικτύων:**
    *   **Backpropagation (Οπισθοδιάδοση):** (Ε3, Ε6, Ε13 ΝΝ) Ο κύριος αλγόριθμος εκπαίδευσης. Υπολογίζει την κλίση της συνάρτησης απώλειας ως προς τα βάρη χρησιμοποιώντας τον **κανόνα της αλυσίδας** (chain rule) και αρχές του **Δυναμικού Προγραμματισμού**. Στόχος: να βρει πώς να ενημερώσει τα βάρη για να μειώσει την απώλεια (Ε6 β. ΝΝ).
    *   **Gradient Descent & Optimizers (SGD, Adam):** Χρησιμοποιούν τις κλίσεις που υπολογίζει η backpropagation για να προσαρμόσουν τα βάρη βήμα-βήμα προς την κατεύθυνση της ελαχιστοποίησης της συνάρτησης απώλειας (Ε6 β., Ε16 δ. ΝΝ).
    *   **Vanishing Gradient:** Αναφέρθηκε παραπάνω (Ε5, Ε13 ΝΝ). Αντίθετο φαινόμενο είναι το **Exploding Gradient**.
    *   **Επιπλέον Κρυμμένα Επίπεδα:** Αυξάνουν την ικανότητα του μοντέλου να μαθαίνει σύνθετα μοτίβα. Όμως, αυξάνουν τον **χρόνο εκπαίδευσης** (Ε15 α. ΝΝ), την πολυπλοκότητα, και τον κίνδυνο **υπερπροσαρμογής** (αν δεν ληφθούν μέτρα - Ε15 γ. ΝΝ). Μπορούν να μειώσουν το σφάλμα εκπαίδευσης (Ε15 β. ΝΝ) αλλά όχι απαραίτητα το σφάλμα γενίκευσης.

3.  **Αντιμετώπιση Υπερπροσαρμογής (Overfitting) σε ΝΝ:** (Ε10, Ε16 ΝΝ)
    *   **Data Augmentation (Αύξηση Δεδομένων):** (Ε16 α. ΝΝ) Δημιουργία νέων δειγμάτων εκπαίδευσης μέσω μετασχηματισμών υπαρχόντων (π.χ., περιστροφές εικόνων). Αυξάνει την ποικιλία του training set.
    *   **Dropout:** (Ε10 β., Ε16 β. ΝΝ) Τυχαία "απενεργοποίηση" νευρώνων και συνδέσεων κατά την εκπαίδευση. Λειτουργεί ως **regularization**, εμποδίζοντας το δίκτυο να βασίζεται σε συγκεκριμένα μονοπάτια. Πολύ αποτελεσματικό.
    *   **Batch Normalization (Κανονικοποίηση Batch):** (Ε12, Ε16 γ. ΝΝ) Κανονικοποίηση των activations ενός επιπέδου για κάθε batch δεδομένων. **Επιταχύνει την εκπαίδευση** (Ε12 β. ΝΝ), επιτρέπει υψηλότερα learning rates, και δρα και ως ρυθμιστής κατά του overfitting. Δεν προκαλεί vanishing gradient (Ε12 γ. ΝΝ ψευδές), αντίθετα το μετριάζει.
    *   **Regularization (L1, L2):** Προσθήκη όρων στη συνάρτηση απώλειας για "τιμωρία" των μεγάλων βαρών. (Αναφέρθηκε στην Lasso, ισχύει και για ΝΝ).

4.  **Αρχιτεκτονικές και Εξειδικευμένες Τεχνικές ΝΝ:**
    *   **CNNs (Convolutional Neural Networks - Συνελικτικά Νευρωνικά Δίκτυα):** (Ε7 α. ΝΝ) Σχεδιασμένα για δεδομένα με δομή πλέγματος (π.χ., **εικόνες**). Χρησιμοποιούν συνελικτικά επίπεδα για εξαγωγή τοπικών χαρακτηριστικών.
    *   **RNNs (Recurrent Neural Networks - Αναδρομικά Νευρωνικά Δίκτυα):** (Ε7 β. ΝΝ) Σχεδιασμένα για επεξεργασία **ακολουθιακών δεδομένων** (sequential data) όπως κείμενο, χρονοσειρές (Ε7 ΝΝ). Έχουν εσωτερική κατάσταση (μνήμη). Πιο προηγμένες μορφές: LSTMs, GRUs.
    *   **Autoencoders:** (Ε8 β. ΝΝ) Δίκτυα που μαθαίνουν μια **συμπιεσμένη αναπαράσταση** (representation) των δεδομένων σε ένα ενδιάμεσο (bottleneck) επίπεδο, εκπαιδευόμενα να αναπαράγουν την είσοδο στην έξοδο. Χρησιμοποιούνται για **representation learning από unlabeled data** (Ε8 ΝΝ) και μείωση διάστασης.
    *   **GANs (Generative Adversarial Networks - Παραγωγικά Ανταγωνιστικά Δίκτυα):** (Ε8 γ., Ε11, Ε18 ΝΝ) Αποτελούνται από **Generator** (παράγει νέα δεδομένα που μοιάζουν με train data - Ε11 α, δ. ΝΝ) και **Discriminator** (προσπαθεί να ξεχωρίσει αληθινά από ψεύτικα δεδομένα - Ε11 β. ΝΝ). Εκπαιδεύονται ανταγωνιστικά (adversarial training). Χρησιμοποιούνται για παραγωγή δεδομένων, representation learning. Ένα πρόβλημα είναι το **mode collapse** (Ε18 α. ΝΝ) όπου ο Generator παράγει περιορισμένη ποικιλία δειγμάτων. Ορισμένες μετρικές GAN (όχι απαραίτητα η απώλεια του Discriminator) μπορούν να **ταλαντώνονται** κατά την επιτυχημένη εκπαίδευση (Ε11 γ. ΝΝ).
    *   **End-to-End Training (Εκπαίδευση από άκρο σε άκρο):** (Ε17 ΝΝ) Εκπαίδευση ενός ενιαίου μοντέλου που χειρίζεται ολόκληρη τη διαδικασία (π.χ., από raw input σε final output) χωρίς ενδιάμεσα, χειροποίητα βήματα. Ένα όφελος είναι ότι **δεν απαιτείται ειδικός για τον χειρισμό/εξαγωγή χαρακτηριστικών (feature engineering)** (Ε17 β. ΝΝ), καθώς το δίκτυο μαθαίνει αυτόματα τα κατάλληλα χαρακτηριστικά.

**Ε. Επιπρόσθετα Σημεία:**

*   **Zero Training Error (Μηδενικό Σφάλμα Εκπαίδευσης):** Ορισμένα μοντέλα έχουν αρκετή **ικανότητα (capacity)** ή ευελιξία για να "αποστηθίσουν" τέλεια τα δεδομένα εκπαίδευσης, επιτυγχάνοντας 0 σφάλμα στο training set, ειδικά αν τα δεδομένα είναι γραμμικώς διαχωρίσιμα (για γραμμικά μοντέλα) ή αν είναι απλώς αρκετά πολύπλοκα (για non-linear μοντέλα). Decision Trees (αρκετό βάθος), kNN (k=1), SVM (Hard-margin αν γραμμικώς διαχωρίσιμο), Νευρωνικά Δίκτυα (με αρκετούς νευρώνες/επίπεδα) μπορούν θεωρητικά να επιτύχουν 0 training error σε κατάλληλα δεδομένα (Ε22 ΜΜ).
*   **Class Imbalance (Ανισορροπία Κλάσεων):** Όταν οι κλάσεις δεν έχουν τον ίδιο αριθμό δειγμάτων. Χρειάζονται ειδικές τεχνικές (oversampling, undersampling) ή μετρικές (F1, AUC-ROC) αντί της accuracy. Θεωρείται βήμα προεπεξεργασίας/χειρισμού δεδομένων (Ε19 γ. ΜΜ).

**Συμπέρασμα:**

Αυτή η ανακεφαλαίωση καλύπτει τις κύριες έννοιες και μεθόδους που θίγονται στις ερωτήσεις, οργανωμένες ανά θέμα. Περιλαμβάνει ορισμούς, χρήση, πλεονεκτήματα/μειονεκτήματα, και συνδέσεις μεταξύ τους (π.χ. πώς οι συναρτήσεις ενεργοποίησης επηρεάζουν την γραμμικότητα/μη-γραμμικότητα, πώς η πολυπλοκότητα μοντέλου επηρεάζει το overfitting, πώς διάφορες τεχνικές το αντιμετωπίζουν).

Μελετώντας αυτή την περίληψη, η οποία είναι άμεσα συνδεδεμένη με το περιεχόμενο των εξεταστικών ερωτήσεων που μου δώσατε, θα έχετε μια πολύ πιο ολοκληρωμένη εικόνα για να απαντήσετε σε παρόμοιες, νέες ερωτήσεις που βασίζονται σε αυτό το υλικό. Έχουμε καλύψει τα *τι*, τα *πώς*, τα *γιατί* και τα *πότε* των σημαντικότερων εργαλείων που εξετάστηκαν.
