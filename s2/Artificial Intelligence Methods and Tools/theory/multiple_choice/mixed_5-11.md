# **100 Ερωτήσεις Πολλαπλής Επιλογής για Εξετάσεις Μηχανικής Μάθησης / Τεχνητής Νοημοσύνης**

1.  Στο πρόβλημα ανίχνευσης καρκίνου του μαστού (Classification Tutorial, slide 2), αν η "θετική κλάση" (positive class) είναι οι κακοήθεις όγκοι, τι αντιπροσωπεύει ένα False Negative (FN);
    α. Ένας καλοήθης όγκος που ταξινομήθηκε ως κακοήθης.
    β. Ένας κακοήθης όγκος που ταξινομήθηκε ως καλοήθης.
    γ. Ένας καλοήθης όγκος που ταξινομήθηκε ως καλοήθης.
    δ. Ένας κακοήθης όγκος που ταξινομήθηκε ως κακοήθης.

2.  Σε ένα dataset με ανισορροπία κλάσεων (class imbalance), ποια μετρική αξιολόγησης ΔΕΝ είναι αξιόπιστη μόνη της (Classification Tutorial, slide 11, Performance Evaluation, slide 91);
    α. Precision
    β. Recall
    γ. F1-score
    δ. Accuracy

3.  Ποια τεχνική resampling περιλαμβάνει την αύξηση των δειγμάτων της κλάσης μειοψηφίας (minority class); (Classification Tutorial, slide 11)
    α. Undersampling
    β. Oversampling
    γ. Stratification
    δ. Cross-validation

4.  Ποια τεχνική διαχωρισμού συνόλου δεδομένων διασφαλίζει ότι η αναλογία των κλάσεων διατηρείται στα υποσύνολα; (Performance Evaluation, slide 105, Classification Tutorial, Ε11)
    α. Random split
    β. Stratified split
    γ. Leave-one-out cross-validation
    δ. k-fold cross-validation

5.  Στον αλγόριθμο k-Nearest Neighbors (kNN) για ταξινόμηση, πώς γίνεται η πρόβλεψη για ένα νέο σημείο; (Classification Tutorial, slide 55)
    α. Με βάση τον μέσο όρο των χαρακτηριστικών των k-πλησιέστερων γειτόνων.
    β. Με βάση την πλειοψηφία της κλάσης μεταξύ των k-πλησιέστερων γειτόνων.
    γ. Με βάση την απόσταση από το πλησιέστερο κεντροειδές.
    δ. Με βάση τη συνάρτηση απόστασης μόνο.

6.  Ποιος είναι ένας περιορισμός του βασικού αλγορίθμου kNN (όχι με βελτιστοποιήσεις) σε μεγάλα datasets; (Classification Tutorial, slide 55, Regression Tutorial, Ε17)
    α. Είναι πολύ γρήγορος.
    β. Κάθε νέο σημείο πρέπει να συγκριθεί με όλα τα υπάρχοντα, καθιστώντας τον αργό.
    γ. Δεν μπορεί να χειριστεί γραμμικά διαχωρίσιμα δεδομένα.
    δ. Πάντα υπερπροσαρμόζεται.

7.  Σε ένα Support Vector Machine (SVM) με "hard-margin" (Classification Tutorial, slide 60-61), ποιος είναι ο στόχος;
    α. Η ελαχιστοποίηση του αριθμού των υποστηρικτικών διανυσμάτων (support vectors).
    β. Η εύρεση ενός διαχωριστικού υπερεπιπέδου με το μέγιστο περιθώριο (margin) μεταξύ των κλάσεων, χωρίς σφάλματα ταξινόμησης στο training set.
    γ. Η εύρεση ενός οποιουδήποτε διαχωριστικού υπερεπιπέδου.
    δ. Η ελαχιστοποίηση της συνολικής απώλειας σε όλα τα σημεία.

8.  Τι επιτρέπει το "kernel trick" στα SVMs; (Classification Tutorial, slide 67)
    α. Να μειώσει τον υπολογιστικό χρόνο για μικρά datasets.
    β. Να επιτρέπει τον γραμμικό διαχωρισμό σε έναν υψηλότερης διάστασης χώρο χαρακτηριστικών χωρίς τον ρητό μετασχηματισμό.
    γ. Να χειριστεί ελλείπουσες τιμές.
    δ. Να επιλέξει αυτόματα τα πιο σημαντικά χαρακτηριστικά.

9.  Ποια είναι η βασική υπόθεση του ταξινομητή Naïve Bayes; (Classification Tutorial, slide 84)
    α. Τα δεδομένα ακολουθούν ομοιόμορφη κατανομή.
    β. Τα χαρακτηριστικά είναι υπό συνθήκη ανεξάρτητα δεδομένης της κλάσης.
    γ. Όλα τα χαρακτηριστικά έχουν την ίδια διασπορά.
    δ. Το dataset είναι γραμμικά διαχωρίσιμο.

10. Σε ένα πρόβλημα παλινδρόμησης, τι μετράει η μετρική Mean Absolute Error (MAE); (Performance Evaluation, slide 10)
    α. Τον μέσο όρο του τετραγώνου των διαφορών μεταξύ πραγματικών και προβλεπόμενων τιμών.
    β. Τη μέση τιμή των απόλυτων διαφορών μεταξύ πραγματικών και προβλεπόμενων τιμών.
    γ. Την τετραγωνική ρίζα του μέσου τετραγωνικού σφάλματος.
    δ. Το ποσοστό της διακύμανσης που εξηγεί το μοντέλο.

11. Ποια μετρική παλινδρόμησης εκφράζεται στις ίδιες μονάδες με την μεταβλητή απόκρισης (response variable); (Performance Evaluation, slide 12)
    α. MSE
    β. R-Squared
    γ. MAE
    δ. MPE

12. Ποιο είναι ένα πλεονέκτημα της Mean Absolute Error (MAE) σε σύγκριση με το Mean Squared Error (MSE); (Performance Evaluation, slide 14, 79)
    α. Είναι παραγωγίσιμη παντού.
    β. Είναι πιο ευαίσθητη στις ακραίες τιμές (outliers).
    γ. Είναι πιο ανθεκτική στις ακραίες τιμές (outliers).
    δ. Πάντα δίνει χαμηλότερες τιμές.

13. Σε ένα πρόβλημα παλινδρόμησης, τι υποδηλώνει μια τιμή R-Squared κοντά στο 0; (Performance Evaluation, slide 13)
    α. Το μοντέλο εξηγεί ένα μεγάλο ποσοστό της διακύμανσης.
    β. Το μοντέλο εξηγεί ένα μικρό ποσοστό της διακύμανσης, πιθανώς όχι καλύτερα από έναν απλό μέσο όρο.
    γ. Το μοντέλο υπερπροσαρμόζεται.
    δ. Η σχέση μεταξύ μεταβλητών είναι πολύπλοκη.

14. Στη γραμμική παλινδρόμηση με Ordinary Least Squares (OLS), ποιος είναι ο στόχος; (Regression Tutorial, slide 39)
    α. Η μεγιστοποίηση της συνολικής απώλειας.
    β. Η ελαχιστοποίηση του αθροίσματος των τετραγώνων των υπολοίπων (sum of squared residuals).
    γ. Η εύρεση των πιο συσχετισμένων χαρακτηριστικών.
    δ. Η εφαρμογή κανονικοποίησης L1.

15. Ποια από τις παρακάτω τεχνικές κανονικοποίησης (regularization) στην παλινδρόμηση μπορεί να μηδενίσει συντελεστές, κάνοντας έτσι επιλογή χαρακτηριστικών; (Regression Tutorial, slide 45)
    α. Ridge Regression
    β. Lasso Regression
    γ. Elastic Net Regression (μόνο ο L1 όρος της Lasso μηδενίζει)
    δ. Μόνο η OLS

16. Ποιο είναι ένα μειονέκτημα της Ridge Regression; (Regression Tutorial, slide 44)
    α. Δεν μπορεί να χειριστεί καθόλου την πολυγραμμική συσχέτιση.
    β. Δεν μηδενίζει συντελεστές, ακόμα και αν είναι ασήμαντοι.
    γ. Δεν έχει κλειστό τύπο λύσης.
    δ. Αυξάνει την υπερπροσαρμογή.

17. Ποιος αλγόριθμος χρησιμοποιείται συνήθως για την εύρεση των ελαχίστων μιας συνάρτησης απώλειας στην εκπαίδευση μοντέλων μηχανικής μάθησης; (Regression Tutorial, slide 31-34)
    α. K-means
    β. Principal Component Analysis
    γ. Gradient Descent
    δ. Support Vector Machines

18. Στον αλγόριθμο Gradient Descent, τι ελέγχει το μέγεθος του βήματος ενημέρωσης των βαρών; (Regression Tutorial, slide 33)
    α. Η συνάρτηση απώλειας.
    β. Η παράγωγος της συνάρτησης απώλειας.
    γ. Ο ρυθμός μάθησης (learning rate).
    δ. Ο αριθμός των εποχών.

19. Ποια είναι η κύρια διάκριση μεταξύ Partitional Clustering και Hierarchical Clustering; (Clustering Tutorial, slide 10)
    α. Η Partitional δημιουργεί επικαλυπτόμενες συστάδες, η Hierarchical όχι.
    β. Η Hierarchical οργανώνει τις συστάδες σε ιεραρχικό δέντρο, η Partitional δημιουργεί μη επικαλυπτόμενες υποομάδες.
    γ. Η Partitional απαιτεί προκαθορισμένο αριθμό συστάδων, η Hierarchical όχι.
    δ. Η Hierarchical είναι πάντα πιο γρήγορη.

20. Στην Agglomerative Hierarchical Clustering, ποια μέθοδος απόστασης μεταξύ συστάδων (inter-cluster distance) βασίζεται στην ελάχιστη απόσταση μεταξύ οποιουδήποτε ζεύγους σημείων στις δύο συστάδες; (Clustering Tutorial, slide 48, 53)
    α. MIN (Single Link)
    β. MAX (Complete Linkage)
    γ. Group Average
    δ. Ward's Method

21. Ποια μέθοδος ιεραρχικής συσταδοποίησης είναι ευαίσθητη στον θόρυβο αλλά μπορεί να χειριστεί μη ελλειπτικά σχήματα; (Clustering Tutorial, slide 55-56)
    α. MAX (Complete Linkage)
    β. Group Average
    γ. MIN (Single Link)
    δ. Ward's Method

22. Ποιος αλγόριθμος συσταδοποίησης βασίζεται στην πυκνότητα και μπορεί να αναγνωρίσει ακραίες τιμές ("noise points"); (Clustering Tutorial, slide 68)
    α. K-means
    β. Affinity Propagation
    γ. DBSCAN
    δ. Gaussian Mixture Models

23. Στον αλγόριθμο DBSCAN, τι χαρακτηρίζει ένα "border point"; (Clustering Tutorial, slide 68)
    α. Είναι ένα σημείο που έχει τουλάχιστον MinPts γείτονες εντός Eps.
    β. Είναι ένα σημείο που δεν ανήκει σε καμία συστάδα.
    γ. Είναι ένα σημείο που δεν είναι core point αλλά βρίσκεται στη γειτονιά ενός core point.
    δ. Είναι το κεντροειδές μιας συστάδας.

24. Ποιος αλγόριθμος συσταδοποίησης βασίζεται στην ιδέα της εύρεσης πυκνών περιοχών μετακινώντας επαναληπτικά ένα παράθυρο; (Clustering Tutorial, slide 23-24)
    α. Spectral Clustering
    β. Mean Shift
    γ. BIRCH
    δ. OPTICS

25. Ποια τεχνική συσταδοποίησης χρησιμοποιεί ιδιοδιανύσματα από έναν πίνακα Laplace για να μειώσει τη διαστατικότητα πριν εφαρμόσει k-means; (Clustering Tutorial, slide 33)
    α. Mean Shift
    β. DBSCAN
    γ. Spectral Clustering
    δ. Agglomerative Clustering

26. Τι αντιπροσωπεύουν οι "κοιλάδες" (valleys) στο reachability plot του αλγορίθμου OPTICS; (Clustering Tutorial, slide 78)
    α. Ακραίες τιμές (noise points).
    β. Σημεία που είναι πολύ μακριά από πυκνές περιοχές.
    γ. Πυρηνικά σημεία (core points) σε πυκνές συστάδες.
    δ. Σημεία στην περιφέρεια των συστάδων.

27. Ποια είναι μια βασική υπόθεση του αλγορίθμου Gaussian Mixture Models (GMM); (Clustering Tutorial, slide 84)
    α. Όλες οι συστάδες είναι σφαιρικές.
    β. Τα δεδομένα παράγονται από ένα μείγμα Γκαουσιανών κατανομών.
    γ. Δεν απαιτείται ο προκαθορισμός του αριθμού των συστάδων.
    δ. Οι συστάδες είναι γραμμικά διαχωρίσιμες.

28. Σύμφωνα με την παρουσίαση Performance Evaluation, γιατί η μέθοδος Test Set είναι δυνητικά προβληματική αν τα δεδομένα είναι λίγα; (Performance Evaluation, slide 23)
    α. Είναι υπολογιστικά πολύ αργή.
    β. Η εκτίμηση της απόδοσης μπορεί να είναι αναξιόπιστη ή απλά τυχαία.
    γ. Δεν μπορεί να χειριστεί προβλήματα παλινδρόμησης.
    δ. Οδηγεί πάντα σε υπερπροσαρμογή.

29. Ποια τεχνική Cross Validation είναι ισοδύναμη με την R-fold Cross Validation, όπου R είναι ο αριθμός των εγγραφών; (Performance Evaluation, slide 40)
    α. 10-fold Cross Validation
    β. Stratified Cross Validation
    γ. Leave-One-Out Cross Validation (LOOCV)
    δ. Randomized Cross Validation

30. Ποιο είναι ένα πλεονέκτημα της k-fold Cross Validation σε σύγκριση με τη μέθοδο Test Set; (Performance Evaluation, slide 40, 105)
    α. Σπαταλά λιγότερα δεδομένα για εκπαίδευση.
    β. Παρέχει μια πιο αξιόπιστη εκτίμηση της απόδοσης χρησιμοποιώντας όλα τα δεδομένα για εκπαίδευση και δοκιμή σε διαφορετικές επαναλήψεις.
    γ. Είναι πάντα πολύ πιο γρήγορη.
    δ. Δεν χρειάζεται καθορισμός παραμέτρων (π.χ., k).

31. Στις καμπύλες εκμάθησης (learning curves) που απεικονίζουν την απώλεια (loss) (Performance Evaluation, slide 84-85), τι υποδηλώνει η υποπροσαρμογή (underfitting);
    α. Υψηλή απώλεια εκπαίδευσης και επικύρωσης που δεν βελτιώνεται σημαντικά.
    β. Χαμηλή απώλεια εκπαίδευσης και υψηλή, αυξανόμενη απώλεια επικύρωσης.
    γ. Σύγκλιση της απώλειας εκπαίδευσης και επικύρωσης σε χαμηλή τιμή.
    δ. Μεγάλο κενό (gap) μεταξύ απώλειας εκπαίδευσης και επικύρωσης.

32. Στις καμπύρεις εκμάθησης που απεικονίζουν την απώλεια (loss) (Performance Evaluation, slide 88-89), τι υποδηλώνει η υπερπροσαρμογή (overfitting);
    α. Υψηλή απώλεια εκπαίδευσης και επικύρωσης.
    β. Η απώλεια εκπαίδευσης συνεχίζει να μειώνεται, ενώ η απώλεια επικύρωσης αυξάνεται.
    γ. Σύγκλιση σε χαμηλή τιμή.
    δ. Η απώλεια επικύρωσης είναι πάντα μηδέν.

33. Ποια είναι η διαφορά μεταξύ παραμέτρων μοντέλου (model parameters) και υπερπαραμέτρων (hyperparameters); (Performance Evaluation, slide 110, Introduction to Neural Networks, slide 110)
    α. Οι παράμετροι μαθαίνονται από τα δεδομένα, οι υπερπαράμετροι ορίζονται πριν την εκπαίδευση.
    β. Οι υπερπαράμετροι μαθαίνονται από τα δεδομένα, οι παράμετροι ορίζονται πριν την εκπαίδευση.
    γ. Και οι δύο μαθαίνονται από τα δεδομένα.
    δ. Καμία από τις παραπάνω.

34. Ποια μέθοδος βελτιστοποίησης υπερπαραμέτρων δοκιμάζει όλους τους συνδυασμούς από ένα προκαθορισμένο εύρος, αλλά μπορεί να είναι υπολογιστικά ακριβή; (Performance Evaluation, slide 115)
    α. Manual Search
    β. Randomized Search
    γ. Grid Search
    δ. Bayesian Optimization

35. Ποια μέθοδος βελτιστοποίησης υπερπαραμέτρων χτίζει ένα μοντέλο της αντικειμενικής συνάρτησης και το χρησιμοποιεί για την έξυπνη επιλογή των επόμενων τιμών προς αξιολόγηση; (Performance Evaluation, slide 121-123)
    α. Grid Search
    β. Randomized Search
    γ. Bayesian Optimization
    δ. Manual Search

36. Σύμφωνα με την παρουσίαση Introduction to Neural Networks, τι είναι το Perceptron; (Introduction to Neural Networks, slide 4)
    α. Ένας αλγόριθμος clustering.
    β. Ένας αλγόριθμος παλινδρόμησης.
    γ. Ένα βασικό δομικό στοιχείο (building block) νευρωνικών δικτύων.
    δ. Μια συνάρτηση απώλειας.

37. Ποια είναι η λειτουργία της συνάρτησης ενεργοποίησης (activation function) σε έναν νευρώνα; (Introduction to Neural Networks, slide 26, 27, 63)
    α. Να υπολογίζει το σταθμισμένο άθροισμα των εισόδων.
    β. Να εισάγει μη-γραμμικότητα στην έξοδο του νευρώνα.
    γ. Να αρχικοποιεί τα βάρη.
    δ. Να υπολογίζει την απώλεια.

38. Ποια συνάρτηση ενεργοποίησης έχει εύρος εξόδου (-1, 1) και είναι παραγωγίσιμη; (Introduction to Neural Networks, slide 66)
    α. Sigmoid
    β. ReLU
    γ. Tanh
    δ. Linear

39. Ποια συνάρτηση ενεργοποίησης είναι επιρρεπής στο πρόβλημα των "εξαφανιζόμενων κλίσεων" (vanishing gradients), ειδικά σε βαθιά δίκτυα; (Introduction to Neural Networks, slide 65-66)
    α. ReLU
    β. Leaky ReLU
    γ. Sigmoid και Tanh
    δ. Linear

40. Ποιος είναι ο σκοπός της συνάρτησης απώλειας (Loss Function) κατά την εκπαίδευση ενός νευρωνικού δικτύου; (Introduction to Neural Networks, slide 75)
    α. Να επιλέγει τις υπερπαραμέτρους.
    β. Να μετράει την απόκλιση μεταξύ της πρόβλεψης του μοντέλου και της πραγματικής τιμής.
    γ. Να αυξάνει τον αριθμό των κρυφών επιπέδων.
    δ. Να κανονικοποιεί τα δεδομένα εισόδου.

41. Ποια συνάρτηση απώλειας χρησιμοποιείται συνήθως για προβλήματα δυαδικής ταξινόμησης; (Introduction to Neural Networks, slide 80)
    α. Mean Squared Error
    β. Categorical Cross-Entropy
    γ. Binary Cross-Entropy
    δ. Hubber Loss

42. Σύμφωνα με το Universal Approximation Theorem (Introduction to Neural Networks, slide 59), ποιο είναι ένα πιθανό μειονέκτημα (Caveat);
    α. Λειτουργεί μόνο για γραμμικά μοντέλα.
    β. Μπορεί να χρειαστούν εκθετικά πολλές κρυφές μονάδες για να προσεγγίσει μια αυθαίρετη συνάρτηση.
    γ. Ισχύει μόνο για δίκτυα χωρίς κρυφά επίπεδα.
    δ. Εξασφαλίζει πάντα ότι το μοντέλο δεν θα υπερπροσαρμοστεί.

43. Στην παρουσίαση Dimensionality Reduction Techniques, γιατί η μείωση διαστατικότητας είναι συχνά χρήσιμη; (Dimensionality Reduction, slide 56)
    α. Αύξηση υπολογιστικού κόστους.
    β. Μετριασμός της υπερπροσαρμογής και απλοποίηση οπτικοποίησης/ανάλυσης.
    γ. Εισαγωγή περισσότερου θορύβου στα δεδομένα.
    δ. Μείωση της ερμηνευσιμότητας του μοντέλου.

44. Ποια γραμμική τεχνική μείωσης διαστατικότητας εστιάζει στην εύρεση άξονων που μεγιστοποιούν τον διαχωρισμό μεταξύ των κλάσεων; (Dimensionality Reduction, slide 30)
    α. PCA (Principal Component Analysis)
    β. LDA (Linear Discriminant Analysis)
    γ. Factor Analysis
    δ. t-SNE

45. Ποια τεχνική μείωσης διαστατικότητας είναι μη γραμμική και εστιάζει στη διατήρηση των τοπικών σχέσεων (γειτονιές) στα δεδομένα; (Dimensionality Reduction, slide 36)
    α. PCA
    β. LDA
    γ. Factor Analysis
    δ. t-SNE

46. Ποια μέθοδος μείωσης διαστατικότητας βασίζεται στην ιδέα ότι κάθε σημείο μπορεί να ανακατασκευαστεί γραμμικά από τους γείτονές του; (Dimensionality Reduction, slide 45)
    α. Isomap
    β. Locally Linear Embedding (LLE)
    γ. Multidimensional Scaling (MDS)
    δ. UMAP

47. Ποια μέθοδος μείωσης διαστατικότητας στοχεύει στη διατήρηση των αποστάσεων μεταξύ όλων των ζευγών σημείων στην αρχική και τη μειωμένη διάσταση; (Dimensionality Reduction, slide 49)
    α. PCA
    β. LDA
    γ. MDS (Multidimensional Scaling)
    δ. RFE

48. Ποια τεχνική μείωσης διαστατικότητας είναι μέθοδος επιλογής χαρακτηριστικών (Feature Selection) και αφαιρεί αναδρομικά τα λιγότερο σημαντικά χαρακτηριστικά; (Dimensionality Reduction, slide 52-53)
    α. PCA
    β. LDA
    γ. RFE (Recursive Feature Elimination)
    δ. Factor Analysis

49. Σύμφωνα με την παρουσίαση Convolutional Neural Networks, τι είναι ένα φίλτρο (filter) ή kernel σε ένα συνελικτικό επίπεδο; (Convolutional Neural Networks, slide 24)
    α. Ένας μεμονωμένος νευρώνας.
    β. Ένα σύνολο σταθμισμένων τιμών (weights) που εφαρμόζεται σε μικρές περιοχές της εικόνας εισόδου.
    γ. Μια συνάρτηση ενεργοποίησης.
    δ. Το αποτέλεσμα της pooling operation.

50. Τι επιτυγχάνει ο όρος "stride" σε μια συνελικτική λειτουργία; (Convolutional Neural Networks, slide 27)
    α. Καθορίζει το μέγεθος του φίλτρου.
    β. Καθορίζει το βήμα με το οποίο το φίλτρο κινείται πάνω στην είσοδο.
    γ. Καθορίζει τον αριθμό των φίλτρων.
    δ. Προσθέτει μηδενικές τιμές γύρω από την είσοδο.

51. Ποιος είναι ο σκοπός του "padding" σε ένα συνελικτικό επίπεδο; (Convolutional Neural Networks, slide 38)
    α. Να μειώσει τη διάσταση της εξόδου.
    β. Να διασφαλίσει ότι το φίλτρο μπορεί να καλύψει τις άκρες της εισόδου και να ελέγξει το μέγεθος της εξόδου.
    γ. Να αυξήσει τον αριθμό των χαρακτηριστικών.
    δ. Να αφαιρέσει τον θόρυβο.

52. Τι επιτυγχάνει η λειτουργία "Pooling" (π.χ. Max Pooling) σε ένα CNN; (Convolutional Neural Networks, slide 22)
    α. Εισάγει μη-γραμμικότητα.
    β. Αυξάνει τη χωρική διάσταση.
    γ. Μειώνει τη χωρική διάσταση, μειώνοντας υπολογισμούς και παρέχοντας ανθεκτικότητα σε μικρές μετατοπίσεις.
    δ. Μαθαίνει νέα χαρακτηριστικά.

53. Ποια συνάρτηση ενεργοποίησης είναι η πιο συχνά χρησιμοποιούμενη στα κρυφά επίπεδα των CNNs λόγω της αποτελεσματικότητάς της και της αντιμετώπισης του vanishing gradient; (Convolutional Neural Networks, slide 50)
    α. Sigmoid
    β. Tanh
    γ. ReLU
    δ. Linear

54. Τι μαθαίνουν συνήθως τα πρώτα συνελικτικά επίπεδα ενός CNN; (Convolutional Neural Networks, slide 59)
    α. Ολόκληρα αντικείμενα ή πολύπλοκα σχήματα.
    β. Απλές, βασικές ακμές και γωνίες.
    γ. Τις πιθανότητες των κλάσεων.
    δ. Τους όρους πόλωσης (biases).

55. Ποια αρχιτεκτονική CNN εισήγαγε την ιδέα των "Residual Blocks" για την εκπαίδευση πολύ βαθιών δικτύων; (Convolutional Neural Networks, slide 70)
    α. AlexNet
    β. VGG
    γ. GoogLeNet (Inception)
    δ. ResNet

56. Ποια αρχιτεκτονική CNN εισήγαγε την ιδέα των "Inception Modules" για την παράλληλη επεξεργασία με φίλτρα διαφορετικών μεγεθών; (Convolutional Neural Networks, slide 69)
    α. ResNet
    β. DenseNet
    γ. GoogLeNet
    δ. MobileNet

57. Ποια τεχνική κανονικοποίησης (regularization) "απενεργοποιεί" τυχαία νευρώνες κατά την εκπαίδευση για να μειώσει την υπερπροσαρμογή; (Convolutional Neural Networks, slide 110)
    α. L1 Regularization
    β. L2 Regularization (Weight Decay)
    γ. Dropout
    δ. Batch Normalization (αν και έχει κανονικοποιητική επίδραση, δεν είναι ο κύριος σκοπός της)

58. Στην εκπαίδευση ενός CNN, ποιος είναι ο ρόλος του validation set; (Convolutional Neural Networks, slide 83)
    α. Εκπαίδευση του τελικού μοντέλου.
    β. Εκτίμηση της απόδοσης του μοντέλου σε νέα δεδομένα μετά την επιλογή υπερπαραμέτρων.
    γ. Επιλογή υπερπαραμέτρων και αρχιτεκτονικής.
    δ. Εξαγωγή χαρακτηριστικών.

59. Τι είναι η "Data Augmentation" σε ένα CNN; (Convolutional Neural Networks, slide 88)
    α. Μείωση του μεγέθους του dataset.
    β. Τεχνητή αύξηση του dataset με μετασχηματισμούς (π.χ. περιστροφές, περικοπές) για βελτίωση της γενίκευσης.
    γ. Αφαίρεση ακραίων τιμών.
    δ. Προσθήκη νέων καναλιών στις εικόνες.

60. Σε ένα Perceptron, τι συμβαίνει αν αρχικοποιήσουμε όλα τα βάρη σε μηδέν; (Introduction to Neural Networks, Ε2)
    α. Το δίκτυο θα μάθει πιο γρήγορα.
    β. Το δίκτυο θα έχει πρόβλημα συμμετρίας και δεν θα μάθει αποτελεσματικά.
    γ. Το δίκτυο θα συγκλίνει σε ολικό βέλτιστο.
    δ. Η συνάρτηση ενεργοποίησης θα παραμείνει γραμμική.

61. Ποιος είναι ένας περιορισμός του αλγορίθμου k-means όσον αφορά το σχήμα των συστάδων; (Clustering Tutorial, slide 16-19)
    α. Αντιμετωπίζει προβλήματα με σφαιρικές συστάδες.
    β. Λειτουργεί καλύτερα με μη σφαιρικές συστάδες.
    γ. Αντιμετωπίζει προβλήματα με μη σφαιρικά σχήματα συστάδων.
    δ. Δεν επηρεάζεται από το σχήμα των συστάδων.

62. Σε ένα πρόβλημα ταξινόμησης, αν έχετε πολύ υψηλή accuracy στο train set και πολύ χαμηλή στο test set, αυτό πιθανότατα υποδηλώνει: (Performance Evaluation, slide 88-89)
    α. Underfitting
    β. Overfitting
    γ. Good fit
    δ. Class imbalance

63. Ποια από τις παρακάτω μετρικές χρησιμοποιείται για την αξιολόγηση προβλημάτων συσταδοποίησης και υψηλότερες τιμές είναι επιθυμητές; (Clustering Tutorial, slide 61)
    α. Davies-Bouldin Index
    β. Silhouette Coefficient (κοντά στο 1)
    γ. Inertia (Within-Cluster Sum of Squares)
    δ. Mean Absolute Error

64. Σύμφωνα με την παρουσίαση Dimensionality Reduction Techniques, ποιος παράγοντας είναι σημαντικός κατά την επιλογή μεθόδου μείωσης διαστατικότητας, εκτός από την απόδοση (π.χ., clustering/classification metrics) και τον υπολογιστικό χρόνο; (Dimensionality Reduction, slide 57)
    α. Η αρχικοποίηση των βαρών.
    β. Η ανάγκη για ερμηνευσιμότητα.
    γ. Ο ρυθμός μάθησης.
    δ. Το padding.

65. Τι συμβαίνει στην εκτιμώμενη τιμή του βάρους (β̂) στην OLS καθώς αυξάνεται η διακύμανση (variance) των υπολοίπων; (Regression Tutorial, slide 41)
    α. Παραμένει σταθερή.
    β. Η διακύμανση του β̂ αυξάνεται.
    γ. Η μεροληψία του β̂ αυξάνεται.
    δ. Ελαχιστοποιείται αυτόματα.

66. Σύμφωνα με το slide 102 της παρουσίασης Performance Evaluation, τι δείχνει ένα "μεγάλο κενό" (big gap) μεταξύ της accuracy στο training και στο validation set;
    α. Underfitting
    β. Overfitting
    γ. Good fit
    δ. Unrepresentative validation dataset

67. Στην παρουσίαση Introduction to Neural Networks, ποια συνάρτηση ενεργοποίησης έχει εύρος εξόδου [0, +∞) και είναι επιρρεπής στο dying ReLU πρόβλημα για αρνητικές εισόδους; (Introduction to Neural Networks, slide 67-68)
    α. Sigmoid
    β. Tanh
    γ. ReLU
    δ. Leaky ReLU

68. Ποιος αλγόριθμος συσταδοποίησης βασίζεται στην πυκνότητα και παράγει ένα ιεραρχικό αποτέλεσμα χωρίς να αναθέτει άμεσα σημεία σε συστάδες, αλλά αποθηκεύει τη σειρά επεξεργασίας; (Clustering Tutorial, slide 74)
    α. DBSCAN
    β. OPTICS
    γ. Mean Shift
    δ. BIRCH

69. Ποιος είναι ο ρόλος των "support vectors" σε ένα SVM; (Classification Tutorial, slide 61)
    α. Όλα τα σημεία του dataset είναι support vectors.
    β. Είναι τα σημεία που βρίσκονται πιο μακριά από το όριο απόφασης.
    γ. Είναι τα σημεία που βρίσκονται στην άκρη του περιθωρίου (margin) και καθορίζουν το όριο απόφασης.
    δ. Είναι τα σημεία που ταξινομήθηκαν λανθασμένα.

70. Σύμφωνα με την παρουσίαση Regression Problems, τι είναι η μεταβλητή "Sales" στο διαφημιστικό dataset; (Regression Tutorial, slide 2-3)
    α. Ένας predictor.
    β. Ένα feature.
    γ. Η response variable.
    δ. Μια ακραία τιμή.

71. Ποια τεχνική κανονικοποίησης στην παλινδρόμηση συνδυάζει ποινές από την L1 και την L2 νόρμα; (Regression Tutorial, slide 46)
    α. Ridge Regression
    β. Lasso Regression
    γ. Elastic Net Regression
    δ. Ordinary Least Squares

72. Σε ένα δυαδικό πρόβλημα ταξινόμησης, τι αντιπροσωπεύει η μετρική Specificity (Ειδικότητα); (Performance Evaluation, slide 55)
    α. TP / (TP + FP)
    β. TN / (TN + FP) (Ποσοστό πραγματικών αρνητικών που βρέθηκαν σωστά)
    γ. TP / (TP + FN)
    δ. TN / (TN + FN)

73. Ποια από τις παρακάτω τεχνικές μείωσης διαστατικότητας είναι μη επιβλεπόμενη (unsupervised); (Dimensionality Reduction, slide 18)
    α. LDA
    β. RFE
    γ. PCA
    δ. Όλες οι παραπάνω

74. Στην παρουσίαση Introduction to Neural Networks, ποια είναι η κύρια ιδέα πίσω από το mini-batch Gradient Descent σε σύγκριση με το απλό Gradient Descent (επεξεργασία όλων των δεδομένων); (Introduction to Neural Networks, slide 79)
    α. Είναι πάντα πιο ακριβές.
    β. Είναι υπολογιστικά πιο αποδοτικό για μεγάλα datasets, ενημερώνοντας τα βάρη με βάση ένα μικρό υποσύνολο δεδομένων.
    γ. Δεν απαιτεί καθόλου υπολογισμό κλίσεων.
    δ. Πάντα συγκλίνει πιο γρήγορα σε ολικό βέλτιστο.

75. Σύμφωνα με το slide 110 της παρουσίασης Performance Evaluation, τι υποδηλώνει μια πολύ μικρή τιμή learning rate;
    α. Η εκπαίδευση θα είναι πολύ γρήγορη.
    β. Η εκπαίδευση μπορεί να είναι πολύ αργή ή να "κολλήσει" σε ένα υποβέλτιστο σημείο.
    γ. Το μοντέλο θα υπερπροσαρμοστεί.
    δ. Το μοντέλο θα υποπροσαρμοστεί.

76. Στην παρουσίαση Convolutional Neural Networks, ποια αρχιτεκτονική είναι γνωστή για την απλότητα και το βάθος της, χρησιμοποιώντας αποκλειστικά φίλτρα 3x3 και MaxPool 2x2; (Convolutional Neural Networks, slide 68)
    α. AlexNet
    β. GoogLeNet
    γ. ResNet
    δ. VGG

77. Στην Agglomerative Hierarchical Clustering, ποια μέθοδος απόστασης μεταξύ συστάδων βασίζεται στη μέγιστη απόσταση μεταξύ οποιουδήποτε ζεύγους σημείων στις δύο συστάδες; (Clustering Tutorial, slide 48, 57)
    α. MIN (Single Link)
    β. MAX (Complete Linkage)
    γ. Group Average
    δ. Ward's Method

78. Σύμφωνα με το slide 95 της παρουσίασης Performance Evaluation, τι δείχνουν οι καμπύλες απώλειας (loss curves) αν τόσο η απώλεια εκπαίδευσης όσο και η απώλεια επικύρωσης συγκλίνουν σε μια χαμηλή τιμή με μικρό κενό;
    α. Overfitting
    β. Underfitting
    γ. Good fit
    δ. Unrepresentative data

79. Ποια τεχνική μείωσης διαστατικότητας είναι μη γραμμική και εστιάζει στη διατήρηση τόσο της τοπικής όσο και ορισμένης παγκόσμιας δομής; (Dimensionality Reduction, slide 39)
    α. t-SNE
    β. Isomap
    γ. UMAP
    δ. LLE

80. Στον αλγόριθμο Perceptron, τι συμβαίνει αν η πρόβλεψη είναι σωστή (match); (Introduction to Neural Networks, slide 19)
    α. Τα βάρη ενημερώνονται.
    β. Τα βάρη δεν ενημερώνονται.
    γ. Η συνάρτηση ενεργοποίησης αλλάζει.
    δ. Ο αλγόριθμος σταματά.

81. Σύμφωνα με την παρουσίαση Classification Tutorial, ποιο είναι ένα πιθανό πρόβλημα του kNN σε imbalanced datasets με μη καλά διαχωρίσιμα δεδομένα; (Classification Tutorial, slide 55)
    α. Ευνοεί την κλάση μειοψηφίας.
    β. Ευνοεί την κλάση πλειοψηφίας.
    γ. Πάντα βρίσκει το ολικό βέλτιστο.
    δ. Δεν μπορεί να χειριστεί αριθμητικά χαρακτηριστικά.

82. Ποια μέθοδος βελτιστοποίησης υπερπαραμέτρων είναι η πιο απλή αλλά μπορεί να είναι χρονοβόρα και να μην βρίσκει το βέλτιστο; (Performance Evaluation, slide 114)
    α. Manual Search
    β. Grid Search
    γ. Randomized Search
    δ. Bayesian Optimization

83. Στην παρουσίαση Introduction to Neural Networks, ποιος είναι ο ρόλος των bias terms; (Introduction to Neural Networks, slide 26)
    α. Είναι πάντα ίσα με μηδέν.
    β. Προσθέτουν μια σταθερή μετατόπιση στην έξοδο του νευρώνα, επιτρέποντας στο όριο απόφασης να μην περνά απαραίτητα από την αρχή.
    γ. Πολλαπλασιάζουν τις εισόδους.
    δ. Καθορίζουν τη συνάρτηση ενεργοποίησης.

84. Σύμφωνα με το slide 106 της παρουσίασης Performance Evaluation, τι δείχνει η καμπύλη εκμάθησης της accuracy καθώς αυξάνεται το μέγεθος του training set;
    α. Η accuracy πάντα μειώνεται.
    β. Η accuracy αρχικά αυξάνεται και μετά "ισοπεδώνεται", υποδεικνύοντας ότι περισσότερα δεδομένα μπορεί να μην βελτιώνουν σημαντικά την απόδοση.
    γ. Η accuracy παραμένει σταθερή.
    δ. Η accuracy πάντα φτάνει το 100% με αρκετά δεδομένα.

85. Ποια από τις παρακάτω μετρικές χρησιμοποιείται για την αξιολόγηση προβλημάτων συσταδοποίησης και χαμηλότερες τιμές είναι επιθυμητές; (Clustering Tutorial, slide 61)
    α. Calinski-Harabasz Index
    β. Silhouette Coefficient
    γ. Inertia (Within-Cluster Sum of Squares)
    δ. Davies-Bouldin Index

86. Σύμφωνα με το slide 16 της παρουσίασης Performance Evaluation, τι μπορεί να μας δείξει η μετρική MPE;
    α. Αν το μοντέλο υπερπροσαρμόζεται.
    β. Αν το μοντέλο υποπροσαρμόζεται.
    γ. Αν το μοντέλο συστηματικά υποεκτιμά ή υπερεκτιμά τις προβλέψεις.
    δ. Την ανθεκτικότητα του μοντέλου στους outliers.

87. Ποιος είναι ο κύριος σκοπός της μείωσης διαστατικότητας όσον αφορά τα προβλήματα υπερπροσαρμογής (overfitting); (Dimensionality Reduction, slide 56)
    α. Να αυξήσει την πολυπλοκότητα του μοντέλου.
    β. Να βοηθήσει στον μετριασμό της υπερπροσαρμογής αφαιρώντας πλεονάζοντα ή άσχετα χαρακτηριστικά.
    γ. Να εισάγει θόρυβο που λειτουργεί ως κανονικοποίηση.
    δ. Δεν έχει καμία επίδραση στην υπερπροσαρμογή.

88. Ποια αρχιτεκτονική CNN είναι γνωστή για την αποδοτικότητά της και τη χρήση "depthwise-separable convolutions"; (Convolutional Neural Networks, slide 74)
    α. ResNet
    β. DenseNet
    γ. MobileNet
    δ. VGG

89. Σύμφωνα με το slide 97 της παρουσίασης Performance Evaluation, τι μπορεί να σημαίνει αν η απώλεια μόλις και μετά βίας αλλάζει (loss barely changes) κατά την εκπαίδευση με έναν συγκεκριμένο ρυθμό μάθησης;
    α. Ο ρυθμός μάθησης είναι πολύ υψηλός.
    β. Ο ρυθμός μάθησης είναι πολύ χαμηλός ή η κανονικοποίηση είναι πολύ υψηλή.
    γ. Το μοντέλο έχει ήδη συγκλίνει στο ολικό βέλτιστο.
    δ. Το dataset είναι πολύ μικρό.

90. Στον αλγόριθμο k-means, ποιο είναι ένα πιθανό πρόβλημα όσον αφορά την αρχικοποίηση; (Classification Tutorial, Ε1)
    α. Πάντα συγκλίνει σε ολικό βέλτιστο ανεξάρτητα από την αρχικοποίηση.
    β. Η αρχικοποίηση των κεντροειδών μπορεί να επηρεάσει το τελικό αποτέλεσμα (τοπικό βέλτιστο).
    γ. Δεν απαιτείται αρχικοποίηση.
    δ. Η αρχικοποίηση γίνεται αυτόματα από τα δεδομένα.

91. Σύμφωνα με το slide 8 της παρουσίασης Dimensionality Reduction, τι είναι το "Curse of Dimensionality";
    α. Η δυσκολία στην οπτικοποίηση δεδομένων υψηλών διαστάσεων.
    β. Η αδυναμία εύρεσης κατάλληλων αλγορίθμων για υψηλές διαστάσεις.
    γ. Φαινόμενα όπου η απόδοση των μοντέλων χειροτερεύει καθώς αυξάνεται η διάσταση, συχνά λόγω της αραίωσης των δεδομένων και της κυριαρχίας του θορύβου.
    δ. Το πρόβλημα της υπερπροσαρμογής σε χαμηλές διαστάσεις.

92. Ποια τεχνική μείωσης διαστατικότητας είναι γραμμική και μοντελοβασισμένη (model-based), εστιάζοντας στην εξήγηση της δομής συνδιακύμανσης; (Dimensionality Reduction, slide 32)
    α. PCA
    β. LDA
    γ. Factor Analysis
    δ. MDS

93. Στην παρουσίαση CNNs, πώς υπολογίζεται ο αριθμός των παραμέτρων σε ένα συνελικτικό επίπεδο (δίνοντας μέγεθος kernel, αριθμό φίλτρων, αριθμό καναλιών εισόδου και bias); (Convolutional Neural Networks, slide 61)
    α. (Kernel\_Size * Kernel\_Size * Input\_Channels) + Num\_Filters
    β. (Kernel\_Size * Kernel\_Size * Input\_Channels + 1) * Num\_Filters
    γ. (Kernel\_Size + Kernel\_Size + Input\_Channels) * Num\_Filters + 1
    δ. (Kernel\_Size * Kernel\_Size * Input\_Channels) * Num\_Filters + Num\_Filters

94. Ποια τεχνική κανονικοποίησης μειώνει την υπερπροσαρμογή ενθαρρύνοντας τα βάρη να είναι μικρά; (Convolutional Neural Networks, slide 108)
    α. Dropout
    β. L1 και L2 Regularization
    γ. Data Augmentation
    δ. Batch Normalization

95. Ποια από τις παρακάτω μετρικές χρησιμοποιείται για την αξιολόγηση προβλημάτων δυαδικής ταξινόμησης και είναι χρήσιμη όταν το κόστος των False Positives και False Negatives είναι διαφορετικό;
    α. Accuracy
    β. F1-score (ισορροπεί Precision και Recall)
    γ. MAE
    δ. Inertia

96. Ποια τεχνική μείωσης διαστατικότητας είναι μη γραμμική και εστιάζει στη διατήρηση γεωδαισιακών αποστάσεων, με την υπόθεση συνδεδεμένου γραφήματος (connected graph); (Dimensionality Reduction, slide 42)
    α. t-SNE
    β. UMAP
    γ. Isomap
    δ. LLE

97. Στην εκπαίδευση ενός νευρωνικού δικτύου, τι μπορεί να δείξει η οπτικοποίηση των βαρών (Visualize the weights); (Convolutional Neural Networks, slide 103-104)
    α. Την ακρίβεια του μοντέλου.
    β. Αν η κανονικοποίηση είναι επαρκής ή αν τα βάρη είναι πολύ "θορυβώδη" ή "καθαρά".
    γ. Την απώλεια εκπαίδευσης.
    δ. Το μέγεθος του training set.

98. Σύμφωνα με την παρουσίαση Introduction to Neural Networks, ποια συνάρτηση ενεργοποίησης έχει εύρος εξόδου (0, 1) και χρησιμοποιείται συχνά στο επίπεδο εξόδου για προβλήματα δυαδικής ταξινόμησης; (Introduction to Neural Networks, slide 65)
    α. Tanh
    β. ReLU
    γ. Softmax (για πολυκατηγορική)
    δ. Sigmoid

99. Ποια αρχιτεκτονική CNN είναι γνωστή για την "κλιμάκωσή της κατά πλάτος" (scaling width) ως κύριο παράγοντα βελτίωσης, πέρα από το βάθος;
    α. AlexNet
    β. ResNet
    γ. GoogLeNet
    δ. Wide-Resnet (αναφέρεται στο "Beyond" slide 75)

100. Σύμφωνα με το slide 84 της παρουσίασης Performance Evaluation, γιατί πρέπει να είμαστε προσεκτικοί με την "false discovery" όταν χρησιμοποιούμε το test set επανειλημμένα;
    α. Επειδή το test set αλλάζει σε κάθε επανάληψη.
    β. Επειδή κάθε φορά που "κοιτάζουμε" στο test set για να πάρουμε αποφάσεις (π.χ. επιλογή μοντέλου), χάνουμε την ιδιότητά του ως ανεξάρτητη εκτίμηση της γενίκευσης.
    γ. Επειδή το test set είναι πάντα πολύ μικρό.
    δ. Επειδή το test set περιέχει μόνο ακραίες τιμές.

---

**Απαντήσεις**

1.  β
2.  δ
3.  β
4.  β
5.  β
6.  β
7.  β
8.  β
9.  β
10. β
11. γ
12. γ
13. β
14. β
15. β
16. β
17. γ
18. γ
19. β
20. α
21. γ
22. β
23. γ
24. β
25. γ
26. γ
27. β
28. β
29. γ
30. β
31. α
32. β
33. α
34. γ
35. γ
36. γ
37. β
38. γ
39. γ
40. β
41. γ
42. β
43. β
44. β
45. δ
46. β
47. γ
48. γ
49. β
50. β
51. β
52. γ
53. γ
54. β
55. δ
56. γ
57. γ
58. γ
59. β
60. β
61. γ
62. β
63. β
64. β
65. β
66. β
67. γ
68. β
69. γ
70. γ
71. γ
72. β
73. γ
74. β
75. β
76. δ
77. β
78. γ
79. γ
80. β
81. β
82. α
83. β
84. β
85. δ
86. γ
87. β
88. γ
89. β
90. β
91. γ
92. γ
93. δ
94. β
95. β
96. γ
97. β
98. δ
99. δ
100. β

Ελπίζω αυτές οι 100 ερωτήσεις να είναι μια ολοκληρωμένη βοήθεια για τη μελέτη σου! Καλή επιτυχία στις εξετάσεις!
