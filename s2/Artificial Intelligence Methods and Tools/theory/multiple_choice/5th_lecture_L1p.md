# **Ερωτήσεις Πολλαπλής Επιλογής**

1.  **Σύμφωνα με το σενάριο ανίχνευσης καρκίνου του μαστού (slide 2), ποιος είναι ο ελάχιστος στόχος ταξινόμησης για τους κακοήθεις όγκους;**
    *   α. 80% των κακοήθων όγκων να ταξινομηθούν σωστά.
    *   β. 90% των κακοήθων όγκων να ταξινομηθούν σωστά.
    *   γ. 80% των καλοήθων όγκων να ταξινομηθούν σωστά.
    *   δ. 90% όλων των όγκων να ταξινομηθούν σωστά.

2.  **Ποιο από τα παρακάτω βήματα ΔΕΝ ανήκει στο "Part 1: have a look at the situation" της διαχείρισης τέτοιων καταστάσεων (slide 4);**
    *   α. Data preprocessing
    *   β. Visualization
    *   γ. Experimental setup
    *   δ. Model selection

3.  **Όταν οι τιμές των χαρακτηριστικών (features) διαφέρουν σημαντικά ως προς το εύρος τους ("Range values vary significantly among features", slide 8), ποιο βήμα προεπεξεργασίας είναι συνήθως απαραίτητο;**
    *   α. Data enrichment
    *   β. Data reduction
    *   γ. Scaling / Normalization
    *   δ. Handling missing values

4.  **Σύμφωνα με το slide 10, το dataset για τον καρκίνο του μαστού παρουσιάζει:**
    *   α. Σοβαρή ανισορροπία κλάσεων (severe class imbalance).
    *   β. Κάποια ισορροπία μεταξύ των κλάσεων (some balance between classes).
    *   γ. Περισσότερους κακοήθεις από καλοήθεις όγκους.
    *   δ. Ακριβώς ίδιο αριθμό δειγμάτων για κάθε κλάση.

5.  **Ποια τεχνική resampling περιλαμβάνει την αύξηση του αριθμού των δειγμάτων στην κλάση μειοψηφίας (minority class) (slide 11);**
    *   α. Undersampling
    *   β. Class weights
    *   γ. Oversampling
    *   δ. Ensemble methods

6.  **Τι ΔΕΝ αναδεικνύει κυρίως ένα boxplot (slide 12);**
    *   α. Τη διάμεσο (median)
    *   β. Τα τεταρτημόρια (quartiles)
    *   γ. Την ακριβή μορφή της κατανομής πιθανότητας (probability density function)
    *   δ. Τις ακραίες τιμές (outliers)

7.  **Κατά την εξέταση boxplots ή ιστογραμμάτων για διαφορετικές κλάσεις (slides 13-27, 32-46), τι υποδεικνύει ένα δυνητικά καλό χαρακτηριστικό για ταξινόμηση;**
    *   α. Μεγάλη επικάλυψη (overlap) μεταξύ των κατανομών των κλάσεων.
    *   β. Μικρή επικάλυψη ή καλός διαχωρισμός μεταξύ των κατανομών των κλάσεων.
    *   γ. Πολλές ακραίες τιμές (outliers) μόνο στη μία κλάση.
    *   δ. Παρόμοιες διάμεσες τιμές (medians) για όλες τις κλάσεις.

8.  **Σύμφωνα με το slide 28, ποια χαρακτηριστικά (feature values) αναγνωρίστηκαν ως "Prominent feature values" μετά την αρχική ανάλυση;**
    *   α. radius\_mean, texture\_mean, perimeter\_mean
    *   β. concavity\_mean, concave points\_mean
    *   γ. Symmetry\_mean, Area\_worst, Symmetry\_worst
    *   δ. fractal\_dimension\_se, smoothness\_se

9.  **Ποιος είναι ένας από τους κύριους περιορισμούς του αλγορίθμου k-nearest neighbors (k-NN) όπως αναφέρεται στο slide 55;**
    *   α. Δεν μπορεί να χειριστεί μη γραμμικά δεδομένα.
    *   β. Είναι πολύ γρήγορος για μεγάλα σύνολα δεδομένων.
    *   γ. Απαιτεί τα χαρακτηριστικά να είναι κανονικοποιημένα.
    *   δ. Κάθε νέο δεδομένο συγκρίνεται με όλα τα υπάρχοντα (αργός αλγόριθμος).

10. **Ποια είναι η βασική αρχή πίσω από την επιλογή του βέλτιστου γραμμικού ορίου απόφασης (decision boundary) σε ένα Support Vector Machine (SVM) (slide 60-61);**
    *   α. Η ελαχιστοποίηση του αριθμού των support vectors.
    *   β. Η μεγιστοποίηση του περιθωρίου (margin) μεταξύ των κλάσεων.
    *   γ. Η διασφάλιση ότι όλα τα σημεία ταξινομούνται σωστά.
    *   δ. Η διέλευση από τους μέσους όρους των κλάσεων.

11. **Τι επιτρέπει το "kernel trick" στα SVMs, ειδικά για μη-γραμμικά διαχωρίσιμα δεδομένα (slide 67);**
    *   α. Να επιταχύνει τον υπολογισμό των αποστάσεων.
    *   β. Να επιτρέπει γραμμικό διαχωρισμό σε έναν υψηλότερης διάστασης χώρο χαρακτηριστικών χωρίς τον ρητό υπολογισμό του μετασχηματισμού.
    *   γ. Να χειρίζεται τις ελλείπουσες τιμές.
    *   δ. Να επιλέγει αυτόματα τα πιο σημαντικά χαρακτηριστικά.

12. **Στα δέντρα απόφασης (Decision Trees), πώς σχετίζεται το βάθος του δέντρου με την πολυπλοκότητά του (slide 77);**
    *   α. Τα βαθύτερα δέντρα είναι πάντα λιγότερο πολύπλοκα.
    *   β. Το βάθος του δέντρου δεν έχει σχέση με την πολυπλοκότητα.
    *   γ. Τα βαθύτερα δέντρα είναι γενικά πιο πολύπλοκα και μπορούν να προσαρμοστούν καλύτερα στα δεδομένα.
    *   δ. Τα βαθύτερα δέντρα χρησιμοποιούνται μόνο για παλινδρόμηση.

13. **Ποια δήλωση περιγράφει καλύτερα ένα μη-παραμετρικό μοντέλο (non-parametric model) (slide 78);**
    *   α. Υποθέτει ένα πεπερασμένο σύνολο παραμέτρων.
    *   β. Είναι πάντα πιο απλό από ένα παραμετρικό μοντέλο.
    *   γ. Η πολυπλοκότητά του μπορεί να αυξηθεί με την ποσότητα των δεδομένων.
    *   δ. Χρησιμοποιεί μόνο γραμμικά όρια απόφασης.

14. **Ποια είναι μια βασική υπόθεση του ταξινομητή Naïve Bayes (slide 84);**
    *   α. Τα χαρακτηριστικά είναι γραμμικά εξαρτώμενα.
    *   β. Τα δεδομένα ακολουθούν ομοιόμορφη κατανομή.
    *   γ. Τα χαρακτηριστικά είναι υπό συνθήκη ανεξάρτητα δεδομένης της κλάσης.
    *   δ. Όλες οι κλάσεις έχουν ίσο αριθμό δειγμάτων.

15. **Στον πίνακα σύγχυσης (confusion matrix) για την ανίχνευση καρκίνου, όπου "κακοήθης" (malignant) είναι η θετική κλάση, τι αντιπροσωπεύει ένα False Negative (FN) (slide 87);**
    *   α. Ένας καλοήθης όγκος που ταξινομήθηκε σωστά ως καλοήθης.
    *   β. Ένας κακοήθης όγκος που ταξινομήθηκε σωστά ως κακοήθης.
    *   γ. Ένας καλοήθης όγκος που ταξινομήθηκε λανθασμένα ως κακοήθης.
    *   δ. Ένας κακοήθης όγκος που ταξινομήθηκε λανθασμένα ως καλοήθης.

16. **Γιατί η μετρική "accuracy" δεν είναι πάντα η καλύτερη για την αξιολόγηση, ειδικά σε περιπτώσεις ανισορροπίας κλάσεων (class imbalance) (slide 91, 93);**
    *   α. Είναι πολύπλοκη στον υπολογισμό της.
    *   β. Δεν λαμβάνει υπόψη τα True Negatives.
    *   γ. Μπορεί να είναι υψηλή ακόμα και αν το μοντέλο αποδίδει άσχημα στην κλάση μειοψηφίας.
    *   δ. Πάντα ευνοεί την κλάση μειοψηφίας.

17. **Τι μετράει η μετρική "Recall" (ή Sensitivity) σε ένα πρόβλημα ταξινόμησης (slide 90);**
    *   α. Από όλα τα πραγματικά θετικά δείγματα, πόσα αναγνωρίστηκαν σωστά;
    *   β. Από όλα τα δείγματα που προβλέφθηκαν ως θετικά, πόσα ήταν πραγματικά θετικά;
    *   γ. Η συνολική ορθότητα του μοντέλου.
    *   δ. Το ποσοστό των πραγματικά αρνητικών δειγμάτων που αναγνωρίστηκαν σωστά.

18. **Σύμφωνα με το slide 89, ποια είναι η σημασία της αξιολόγησης στο test set;**
    *   α. Το test set είναι πάντα μεγαλύτερο.
    *   β. Για να δούμε πόσο καλά το μοντέλο γενικεύει σε αόρατα δεδομένα.
    *   γ. Το μοντέλο εκπαιδεύεται καλύτερα στο test set.
    *   δ. Το test set έχει λιγότερες ακραίες τιμές.

19. **Στο "imaginary case" του slide 93, η accuracy είναι 0.98 αλλά το recall για την "non-healthy" (θετική) κλάση είναι 0.71. Αυτό υποδηλώνει ότι:**
    *   α. Το μοντέλο είναι εξαιρετικό στην αναγνώριση των non-healthy δειγμάτων.
    *   β. Το dataset είναι απόλυτα ισορροπημένο.
    *   γ. Το μοντέλο χάνει ένα σημαντικό ποσοστό των non-healthy δειγμάτων, παρά την υψηλή συνολική accuracy.
    *   δ. Η precision πρέπει επίσης να είναι 0.98.

20. **Ποιος από τους παρακάτω αλγορίθμους ΔΕΝ αναφέρεται ρητά ως μια προσέγγιση μηχανικής μάθησης για το έργο στις διαφάνειες 49-85;**
    *   α. k-Nearest Neighbors
    *   β. Support Vector Classifier (SVC)
    *   γ. Logistic Regression
    *   δ. Decision Trees
    *   ε. Naïve Bayes

---

**Απαντήσεις**

1.  **β.** 90% των κακοήθων όγκων να ταξινομηθούν σωστά.
2.  **γ.** Experimental setup (Αυτό ανήκει στο Part 2)
3.  **γ.** Scaling / Normalization
4.  **β.** Κάποια ισορροπία μεταξύ των κλάσεων (some balance between classes). (Ratio 1.7/1)
5.  **γ.** Oversampling
6.  **γ.** Την ακριβή μορφή της κατανομής πιθανότητας (probability density function)
7.  **β.** Μικρή επικάλυψη ή καλός διαχωρισμός μεταξύ των κατανομών των κλάσεων.
8.  **γ.** Symmetry\_mean, Area\_worst, Symmetry\_worst
9.  **δ.** Κάθε νέο δεδομένο συγκρίνεται με όλα τα υπάρχοντα (αργός αλγόριθμος).
10. **β.** Η μεγιστοποίηση του περιθωρίου (margin) μεταξύ των κλάσεων.
11. **β.** Να επιτρέπει γραμμικό διαχωρισμό σε έναν υψηλότερης διάστασης χώρο χαρακτηριστικών χωρίς τον ρητό υπολογισμό του μετασχηματισμού.
12. **γ.** Τα βαθύτερα δέντρα είναι γενικά πιο πολύπλοκα και μπορούν να προσαρμοστούν καλύτερα στα δεδομένα.
13. **γ.** Η πολυπλοκότητά του μπορεί να αυξηθεί με την ποσότητα των δεδομένων.
14. **γ.** Τα χαρακτηριστικά είναι υπό συνθήκη ανεξάρτητα δεδομένης της κλάσης.
15. **δ.** Ένας κακοήθης όγκος που ταξινομήθηκε λανθασμένα ως καλοήθης.
16. **γ.** Μπορεί να είναι υψηλή ακόμα και αν το μοντέλο αποδίδει άσχημα στην κλάση μειοψηφίας.
17. **α.** Από όλα τα πραγματικά θετικά δείγματα, πόσα αναγνωρίστηκαν σωστά;
18. **β.** Για να δούμε πόσο καλά το μοντέλο γενικεύει σε αόρατα δεδομένα.
19. **γ.** Το μοντέλο χάνει ένα σημαντικό ποσοστό των non-healthy δειγμάτων, παρά την υψηλή συνολική accuracy.
20. **γ.** Logistic Regression (Αν και αναφέρεται στα συμπεράσματα (slide 96), δεν παρουσιάζεται ως προσέγγιση στις διαφάνειες 49-85 με τον ίδιο τρόπο που παρουσιάζονται οι άλλοι).
