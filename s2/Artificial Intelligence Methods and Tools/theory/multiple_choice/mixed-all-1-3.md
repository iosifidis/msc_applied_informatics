# Ερωτήσεις-απαντήσεις

Ορίστε 100 ερωτήσεις πολλαπλής επιλογής, βασισμένες αποκλειστικά στην ύλη των διαφανειών, Κατηγοριοποίηση, Παλινδρόμηση και Συσταδοποίηση, με στόχο την κάλυψη όσο το δυνατόν περισσότερων σημείων.

**ΕΝΟΤΗΤΑ 1: Μονή απάντηση ανά ερώτημα.**

1.  Στην Επιβλεπόμενη Μάθηση (Supervised Learning), ο στόχος είναι:   
    α. Η εύρεση υποκείμενων προτύπων σε δεδομένα χωρίς ετικέτες.   
    β. Η εκμάθηση μιας συνάρτησης απεικόνισης από εισόδους σε αντίστοιχες ετικέτες.   
    γ. Η εκμάθηση να ενεργεί με βάση ανατροφοδότηση/ανταμοιβή.   
    δ. Η μείωση της διάστασης των δεδομένων.   
    **Σωστή απάντηση:** β

2.  Ποια από τις παρακάτω τεχνικές ανήκει στην κατηγορία της Μη Επιβλεπόμενης Μάθησης (Unsupervised Learning);   
    α. Logistic Regression   
    β. Naïve Bayes   
    γ. k-means Clustering   
    δ. Decision Trees   
    **Σωστή απάντηση:** γ

3.  Ποιος είναι ο πρωταρχικός σκοπός της χρήσης μη-γραμμικών συναρτήσεων ενεργοποίησης σε ένα νευρωνικό δίκτυο;   
    α. Η επιτάχυνση της σύγκλισης.   
    β. Η εισαγωγή της δυνατότητας εκμάθησης μη-γραμμικών σχέσεων.   
    γ. Η κανονικοποίηση των εξόδων κάθε επιπέδου.   
    δ. Η επιλογή των πιο σημαντικών χαρακτηριστικών.   
    **Σωστή απάντηση:** β

4.  Σύμφωνα με τις διαφάνειες, ποια συνάρτηση ενεργοποίησης τείνει να "κορεσθεί" (saturate) στα άκρα, οδηγώντας ενδεχομένως σε προβλήματα vanishing gradient;   
    α. ReLU   
    β. Tanh   
    γ. Leaky ReLU   
    δ. Linear   
    **Σωστή απάντηση:** β

5.  Σε ένα πρόβλημα **πολλαπλής ταξινόμησης** (multi-class classification), ποια συνάρτηση ενεργοποίησης χρησιμοποιείται συνήθως στο **τελευταίο επίπεδο** (output layer) για την παροχή πιθανοτήτων για κάθε κλάση;   
    α. Sigmoid   
    β. Tanh   
    γ. ReLU   
    δ. Softmax   
    **Σωστή απάντηση:** δ

6.  Τι αντιπροσωπεύει η συνάρτηση απώλειας (Loss Function) στην εκπαίδευση νευρωνικών δικτύων;   
    α. Την ταχύτητα εκπαίδευσης του μοντέλου.   
    β. Την πολυπλοκότητα της αρχιτεκτονικής του δικτύου.   
    γ. Τη διαφορά μεταξύ της προβλεπόμενης και της πραγματικής εξόδου.   
    δ. Τον αριθμό των εσφαλμένα ταξινομημένων δειγμάτων.   
    **Σωστή απάντηση:** γ

7.  Ο αλγόριθμος **Backpropagation** χρησιμοποιείται κυρίως για:   
    α. Την αρχικοποίηση των βαρών του δικτύου.   
    β. Τον υπολογισμό των κλίσεων (gradients) της συνάρτησης απώλειας ως προς τα βάρη.   
    γ. Τη μείωση της διάστασης των δεδομένων εισόδου.   
    δ. Την επιλογή της κατάλληλης συνάρτησης ενεργοποίησης.   
    **Σωστή απάντηση:** β

8.  Σε μεγάλα σύνολα δεδομένων, ποια παραλλαγή του Gradient Descent είναι η πιο συχνά χρησιμοποιούμενη, προσφέροντας μια ισορροπία μεταξύ ταχύτητας και σταθερότητας;   
    α. Vanilla Gradient Descent   
    β. Stochastic Gradient Descent   
    γ. Mini-batch Gradient Descent   
    δ. Newton's Method   
    **Σωστή απάντηση:** γ

9.  Ποιο πρόβλημα χαρακτηρίζεται από πολύ μικρές κλίσεις (gradients) που διαδίδονται προς τα πίσω σε ένα βαθύ νευρωνικό δίκτυο, επιβραδύνοντας την εκμάθηση;   
    α. Overfitting   
    β. Mode Collapse   
    γ. Exploding Gradient   
    δ. Vanishing Gradient   
    **Σωστή απάντηση:** δ

10. Ποια τεχνική κανονικοποίησης (regularization) μειώνει την υπερπροσαρμογή "ρίχνοντας" τυχαία νευρώνες και τις συνδέσεις τους κατά την εκπαίδευση;   
    α. Batch Normalization   
    β. Weight Decay   
    γ. Dropout   
    δ. Early Stopping   
    **Σωστή απάντηση:** γ

11. Ποια τεχνική κανονικοποίησης σταματά την εκπαίδευση όταν η απόδοση σε ένα ξεχωριστό σύνολο (validation set) σταματήσει να βελτιώνεται;   
    α. Batch Normalization   
    β. Dropout   
    γ. Weight Decay   
    δ. Early Stopping   
    **Σωστή απάντηση:** δ

12. Ποια αρχιτεκτονική νευρωνικού δικτύου σχεδιάστηκε κυρίως για την αποτελεσματική επεξεργασία δεδομένων με χωρική δομή, όπως εικόνες;   
    α. Recurrent Neural Networks (RNNs)   
    β. Generative Adversarial Networks (GANs)   
    γ. Convolutional Neural Networks (CNNs)   
    δ. Autoencoders   
    **Σωστή απάντηση:** γ

13. Ποια αρχιτεκτονική νευρωνικού δικτύου είναι η πιο κατάλληλη για την επεξεργασία ακολουθιακών δεδομένων, όπως κείμενο ή χρονοσειρές, λόγω της ικανότητάς της να διατηρεί κατάσταση (state) από προηγούμενα βήματα;   
    α. Feedforward Neural Networks (FNNs)   
    β. Convolutional Neural Networks (CNNs)   
    γ. Recurrent Neural Networks (RNNs)   
    δ. Support Vector Machines (SVMs)   
    **Σωστή απάντηση:** γ

14. Σύμφωνα με το No-Free-Lunch Theorem στην Επιβλεπόμενη Μάθηση, τι ισχύει;   
    α. Όλοι οι αλγόριθμοι αποδίδουν εξίσου καλά σε όλα τα προβλήματα.   
    β. Τα νευρωνικά δίκτυα είναι πάντα η βέλτιστη επιλογή.   
    γ. Κανένας μεμονωμένος αλγόριθμος δεν είναι ο καλύτερος για *όλα* τα πιθανά προβλήματα.   
    δ. Μόνο οι παραμετρικοί αλγόριθμοι μπορούν να γενικεύσουν.   
    **Σωστή απάντηση:** γ

15. Στο παράδειγμα ανίχνευσης καρκίνου του μαστού, το σύνολο δεδομένων περιείχε περίπου πόσες παρατηρήσεις (ασθενείς);   
    α. 30   
    β. 60   
    γ. 600   
    δ. 6000   
    **Σωστή απάντηση:** γ

16. Στο ίδιο παράδειγμα, πόσα χαρακτηριστικά (medical index scores) είχε κάθε παρατήρηση;   
    α. 2   
    β. 10   
    γ. 30   
    δ. 60   
    **Σωστή απάντηση:** γ

17. Στο παράδειγμα ταξινόμησης, η κατανομή των κλάσεων (καλοήθης/κακοήθης) περιγράφηκε ως:   
    α. Ιδιαίτερα μη ισορροπημένη (unbalanced).   
    β. Σχετικά ισορροπημένη (good ratio).   
    γ. Με άγνωστη κατανομή.   
    δ. Με λιγότερα καλοήθη δείγματα από κακοήθη.   
    **Σωστή απάντηση:** β

18. Σε ένα dataset με σημαντικό Class Imbalance, γιατί η Accuracy από μόνη της δεν είναι αξιόπιστη μετρική αξιολόγησης;   
    α. Επηρεάζεται από outliers.   
    β. Μπορεί να είναι υψηλή ακόμα κι αν το μοντέλο αποδίδει άσχημα στη μειοψηφούσα κλάση.   
    γ. Απαιτεί χειροκίνητη ρύθμιση παραμέτρων.   
    δ. Υπολογίζει μόνο το σφάλμα, όχι την ακρίβεια.   
    **Σωστή απάντηση:** β

19. Ποια τεχνική αντιμετώπισης του Class Imbalance περιλαμβάνει τη μείωση του αριθμού των δειγμάτων της πλειοψηφούσας κλάσης;   
    α. Oversampling   
    β. Undersampling   
    γ. Data Augmentation   
    δ. Feature Selection   
    **Σωστή απάντηση:** β

20. Στο παράδειγμα ταξινόμησης, ποιο είδος οπτικοποίησης ήταν ιδιαίτερα χρήσιμο για την σύγκριση της κατανομής ενός ποσοτικού χαρακτηριστικού μεταξύ των δύο κλάσεων, αναδεικνύοντας εύρος, τεταρτημόρια και ακραίες τιμές;   
    α. Scatter Plot   
    β. Histogram   
    γ. Boxplot   
    δ. Pair Plot   
    **Σωστή απάντηση:** γ

21. Στο παράδειγμα ταξινόμησης, τι υποδηλώνει η μεγάλη **επικάλυψη** (overlap) μεταξύ των ιστογραμμάτων ενός χαρακτηριστικού για τις δύο κλάσεις;   
    α. Ότι το χαρακτηριστικό είναι πολύ σημαντικό για τη διάκριση των κλάσεων.   
    β. Ότι το χαρακτηριστικό δεν είναι ιδιαίτερα διακριτικό για τις κλάσεις.   
    γ. Την παρουσία missing values.   
    δ. Την ανάγκη για κανονικοποίηση.   
    **Σωστή απάντηση:** β

22. Ο αλγόριθμος **k-Nearest Neighbors (kNN)** για ταξινόμηση ταξινομεί ένα νέο δείγμα με βάση:   
    α. Τη μέση τιμή όλων των χαρακτηριστικών του.   
    β. Την κλάση του κοντινότερου γείτονα στο training set.   
    γ. Την πλειοψηφία των κλάσεων των k κοντινότερων γειτόνων στο training set.   
    δ. Τη γραμμική συνάρτηση που διαχωρίζει καλύτερα τις κλάσεις.   
    **Σωστή απάντηση:** γ

23. Σε ένα πρόβλημα ταξινόμησης, τι ονομάζουμε **Decision Boundary**;   
    α. Τη συνάρτηση κόστους που ελαχιστοποιείται.   
    β. Την επιφάνεια στο χώρο των χαρακτηριστικών που διαχωρίζει τις περιοχές των διαφορετικών κλάσεων.   
    γ. Τον αριθμό των κρυμμένων επιπέδων σε ένα νευρωνικό δίκτυο.   
    δ. Το σύνολο των support vectors σε ένα SVM.   
    **Σωστή απάντηση:** β

24. Ένα σύνολο δεδομένων ταξινόμησης λέγεται **γραμμικά διαχωρίσιμο** αν:   
    α. Μπορεί να χωριστεί τέλεια από μία ευθεία γραμμή (σε 2D), ένα επίπεδο (σε 3D), ή μια υπερεπιφάνεια (σε υψηλότερες διαστάσεις).   
    β. Όλα τα χαρακτηριστικά του είναι συνεχείς μεταβλητές.   
    γ. Δεν περιέχει outliers.   
    δ. Η κατανομή των κλάσεων είναι ισορροπημένη.   
    **Σωστή απάντηση:** α

25. Ποιος αλγόριθμος ταξινόμησης στοχεύει στην εύρεση της διαχωριστικής υπερεπιφάνειας με το μέγιστο περιθώριο (margin) μεταξύ των κλάσεων;   
    α. Naïve Bayes   
    β. k-Nearest Neighbors   
    γ. Decision Trees   
    δ. Support Vector Machines (SVMs)   
    **Σωστή απάντηση:** δ

26. Πώς μπορούν τα SVM να χειριστούν **μη-γραμμικά διαχωρίσιμα** δεδομένα;   
    α. Χρησιμοποιώντας μια γραμμική συνάρτηση ενεργοποίησης.   
    β. Μετασχηματίζοντας τα δεδομένα σε έναν υψηλότερο διαστατικό χώρο όπου γίνονται γραμμικά διαχωρίσιμα (Kernel Trick).   
    γ. Εφαρμόζοντας undersampling στη μεγαλύτερη κλάση.   
    δ. Χρησιμοποιώντας ένα δέντρο αποφάσεων.   
    **Σωστή απάντηση:** β

27. Ο αλγόριθμος ταξινόμησης **Naïve Bayes** βασίζεται στην (ενδεχομένως απλοϊκή) υπόθεση ότι:   
    α. Τα χαρακτηριστικά είναι υπό συνθήκη ανεξάρτητα δεδομένης της κλάσης.   
    β. Όλα τα χαρακτηριστικά έχουν κανονική κατανομή.   
    γ. Το dataset είναι γραμμικά διαχωρίσιμο.   
    δ. Η απόσταση μεταξύ των σημείων είναι ευκλείδεια.   
    **Σωστή απάντηση:** α

28. Στην **Παλινδρόμηση (Regression)**, ο στόχος είναι:   
    α. Η ταξινόμηση δεδομένων σε διακριτές κλάσες.   
    β. Η πρόβλεψη μιας συνεχούς μεταβλητής (response variable).   
    γ. Η ομαδοποίηση δεδομένων σε συστάδες.   
    δ. Η μείωση του αριθμού των χαρακτηριστικών.   
    **Σωστή απάντηση:** β

29. Στο παράδειγμα παλινδρόμησης με δεδομένα διαφήμισης, η μεταβλητή **Sales** θεωρείται ως:   
    α. Predictor Variable   
    β. Response Variable   
    γ. Feature   
    δ. Covariate   
    **Σωστή απάντηση:** β

30. Στην **Παλινδρόμηση**, ο όρος **Predictor Variable** είναι συνώνυμος με:   
    α. Response Variable   
    β. Output Variable   
    γ. Target Variable   
    δ. Feature / Covariate   
    **Σωστή απάντηση:** δ

31. Στο πλαίσιο της Παλινδρόμησης, τι είναι το **Statistical Model**;   
    α. Μια βάση δεδομένων με ιστορικά στοιχεία.   
    β. Ένας αλγόριθμος που εκτιμά την υποκείμενη συνάρτηση που σχετίζει τους predictors με το response.   
    γ. Μια συνάρτηση απώλειας που ελαχιστοποιείται.   
    δ. Μια μέθοδος οπτικοποίησης δεδομένων.   
    **Σωστή απάντηση:** β

32. Ποια είναι η **συνάρτηση απώλειας** (Loss Function) που χρησιμοποιείται συνήθως στην **Γραμμική Παλινδρόμηση** (Linear Regression) για την εκτίμηση των συντελεστών, όπως αναφέρεται στις διαφάνειες;   
    α. Cross-Entropy   
    β. Mean Squared Error (MSE)   
    γ. F1-score   
    δ. Precision   
    **Σωστή απάντηση:** β

33. Στην απλή γραμμική παλινδρόμηση (ένας predictor), η εκτίμηση των συντελεστών (β₀, β₁) μπορεί να βρεθεί αναλυτικά (explicit solution) ελαχιστοποιώντας το MSE. Αυτή η μέθοδος είναι γνωστή ως:   
    α. Ridge Regression   
    β. Lasso Regression   
    γ. Elastic Net Regression   
    δ. Ordinary Least Squares (OLS)   
    **Σωστή απάντηση:** δ

34. Ποια είναι μία από τις βασικές **περιορισμούς** (limitations) της μεθόδου OLS, ιδιαίτερα σε περιπτώσεις με μεγάλο αριθμό predictors και συσχέτιση μεταξύ τους;   
    α. Δεν μπορεί να χειριστεί γραμμικά διαχωρίσιμα δεδομένα.   
    β. Έχει υψηλή απόδοση σε noisy δεδομένα.   
    γ. Υποφέρει από variance inflation.   
    δ. Δεν είναι κατάλληλη για προβλήματα παλινδρόμησης.   
    **Σωστή απάντηση:** γ

35. Ποια τεχνική **κανονικοποίησης** (Regularization) στην Παλινδρόμηση προσθέτει ένα πέναλτι ανάλογο της **L₂ νόρμας** των συντελεστών στη συνάρτηση απώλειας;   
    α. Lasso   
    β. Ridge   
    γ. Elastic Net   
    δ. Dropout   
    **Σωστή απάντηση:** β

36. Ποια τεχνική **κανονικοποίησης** στην Παλινδρόμηση προσθέτει ένα πέναλτι ανάλογο της **L₁ νόρμας** των συντελεστών, με αποτέλεσμα κάποιοι συντελεστές να γίνονται ακριβώς μηδέν (feature selection);   
    α. Lasso   
    β. Ridge   
    γ. Elastic Net   
    δ. Early Stopping   
    **Σωστή απάντηση:** α

37. Ποια τεχνική **κανονικοποίησης** συνδυάζει τα πέναλτι της L₁ και L₂ νόρμας, προσφέροντας ένα συμβιβασμό μεταξύ Ridge και Lasso;   
    α. Lasso   
    β. Ridge   
    γ. Elastic Net   
    δ. Batch Normalization   
    **Σωστή απάντηση:** γ

38. Στην παλινδρόμηση, τι υποδηλώνει ένα **χαμηλότερο** σκορ **RMSE** (Root Mean Squared Error);   
    α. Χαμηλότερη ακρίβεια του μοντέλου.   
    β. Καλύτερη προσαρμογή του μοντέλου στα δεδομένα (χαμηλότερο σφάλμα).   
    γ. Υψηλότερη πολυπλοκότητα του μοντέλου.   
    δ. Παρουσία outliers στα δεδομένα.   
    **Σωστή απάντηση:** β

39. Ο αλγόριθμος **k-Nearest Neighbors (kNN)** μπορεί να χρησιμοποιηθεί και για **Παλινδρόμηση**. Πώς προκύπτει συνήθως η πρόβλεψη για ένα νέο σημείο σε αυτή την περίπτωση;   
    α. Με την κλάση του κοντινότερου γείτονα.   
    β. Με τη μέση τιμή των response variables των k κοντινότερων γειτόνων.   
    γ. Με την ελαχιστοποίηση του MSE.   
    δ. Με εφαρμογή ταξινομητή Naïve Bayes.   
    **Σωστή απάντηση:** β

40. Σύμφωνα με τις διαφάνειες, ποιος από τους παρακάτω αλγορίθμους **δεν** μπορεί να ταξινομήσει δεδομένα με **μη-γραμμικό** διαχωρισμό;   
    α. Hard-margin Linear SVM   
    β. Decision Tree   
    γ. Naïve Bayes   
    δ. k-Nearest Neighbors   
    **Σωστή απάντηση:** α

41. Στην **Συσταδοποίηση (Clustering)**, ο στόχος είναι:   
    α. Η πρόβλεξη μιας συνεχούς μεταβλητής.   
    β. Η ομαδοποίηση δεδομένων σε συστάδες (clusters) με βάση την ομοιότητά τους.   
    γ. Η ταξινόμηση δεδομένων σε προκαθορισμένες κλάσες.   
    δ. Η εκμάθηση μιας συνάρτησης από εισόδους σε εξόδους.   
    **Σωστή απάντηση:** β

42. Ποια ιδιότητα πρέπει να έχει ένα καλό μέτρο απόστασης (Distance Measure), όπως αναφέρεται στις διαφάνειες;   
    α. Να είναι πάντα μηδέν.   
    β. Να μην επηρεάζεται από την κλίμακα των χαρακτηριστικών.   
    γ. Να ικανοποιεί την τριγωνική ανισότητα (Triangular Inequality).   
    δ. Να είναι πάντα θετικό.   
    **Σωστή απάντηση:** γ

43. Σε ένα πρόβλημα συσταδοποίησης, τι ονομάζουμε **Partitional Clustering**;   
    α. Μια ιεραρχία από ένθετες συστάδες.   
    β. Μια διαίρεση των αντικειμένων δεδομένων σε μη επικαλυπτόμενα υποσύνολα (συστάδες).   
    γ. Μια μέθοδος που απαιτεί τον καθορισμό του αριθμού των συστάδων εκ των προτέρων.   
    δ. Μια τεχνική που βασίζεται στην πυκνότητα των δεδομένων.   
    **Σωστή απάντηση:** β

44. Σε ένα πρόβλημα συσταδοποίησης, τι ονομάζουμε **Hierarchical Clustering**;   
    α. Μια διαίρεση των αντικειμένων σε μη επικαλυπτόμενες συστάδες.   
    β. Μια μέθοδος που βασίζεται στην απόσταση από τα centroids.   
    γ. Ένα σύνολο από ένθετες συστάδες οργανωμένες ως ιεραρχικό δέντρο (dendrogram).   
    δ. Μια τεχνική που χρησιμοποιεί πίνακες ομοιότητας.   
    **Σωστή απάντηση:** γ

45. Ποιος αλγόριθμος συσταδοποίησης είναι Partitional και βασίζεται στον προσδιορισμό των **centroids** των συστάδων;   
    α. DBSCAN   
    β. Spectral Clustering   
    γ. Agglomerative Clustering   
    δ. k-Means   
    **Σωστή απάντηση:** δ

46. Ποια από τις παρακάτω είναι μια **περιορισμός** (limitation) του αλγορίθμου k-Means;   
    α. Χειρίζεται καλά συστάδες με μη-ελλειπτικά σχήματα.   
    β. Έχει προβλήματα όταν οι συστάδες έχουν διαφορετικά μεγέθη ή πυκνότητες.   
    γ. Δεν επηρεάζεται από outliers.   
    δ. Δεν απαιτεί τον καθορισμό του αριθμού των συστάδων εκ των προτέρων.   
    **Σωστή απάντηση:** β

47. Ο αλγόριθμος συσταδοποίησης **Mean Shift** είναι μια **μη-παραμετρική** τεχνική που βασίζεται στην:   
    α. Ελαχιστοποίηση της διακύμανσης εντός των συστάδων.   
    β. Εκτίμηση της πυκνότητας των δεδομένων και την εύρεση των modes.   
    γ. Ιεραρχική ομαδοποίηση σημείων.   
    δ. Ανάλυση φάσματος ενός πίνακα ομοιότητας.   
    **Σωστή απάντηση:** β

48. Ο αλγόριθμος συσταδοποίησης **DBSCAN** βασίζεται στην **πυκνότητα** των δεδομένων και ορίζει τους όρους:   
    α. Centroid, Radius, Distance.   
    β. Core Point, Border Point, Noise Point.   
    γ. Exemplar, Preference, Similarity.   
    δ. Eigenvector, Laplacian Matrix, Affinity Graph.   
    **Σωστή απάντηση:** β

49. Ποια από τις παρακάτω είναι ένα **πλεονέκτημα** (strength) του αλγορίθμου DBSCAN;   
    α. Χειρίζεται καλά δεδομένα με ποικίλες πυκνότητες.   
    β. Είναι ανθεκτικός στον θόρυβο (resistant to noise).   
    γ. Απαιτεί ελάχιστους υπερπαραμέτρους.   
    δ. Εγγυάται σφαιρικές συστάδες.   
    **Σωστή απάντηση:** β

50. Σε ένα **Hierarchical Clustering**, ποια μέθοδος ορισμού της απόστασης μεταξύ συστάδων βασίζεται στην **ελάχιστη απόσταση** μεταξύ οποιουδήποτε ζεύγους σημείων από τις δύο συστάδες;   
    α. MIN (Single Link)   
    β. MAX (Complete Link)   
    γ. Group Average   
    δ. Ward's Method   
    **Σωστή απάντηση:** α

51. Σε ένα **Hierarchical Clustering**, ποια μέθοδος ορισμού της απόστασης μεταξύ συστάδων βασίζεται στη **μέγιστη απόσταση** μεταξύ οποιουδήποτε ζεύγους σημείων από τις δύο συστάδες;   
    α. MIN (Single Link)   
    β. MAX (Complete Link)   
    γ. Group Average   
    δ. Ward's Method   
    **Σωστή απάντηση:** β

52. Σε ένα **Hierarchical Clustering**, ποια μέθοδος ορισμού της απόστασης μεταξύ συστάδων βασίζεται στον **μέσο όρο** των αποστάσεων μεταξύ όλων των ζευγών σημείων από τις δύο συστάδες;   
    α. MIN (Single Link)   
    β. MAX (Complete Link)   
    γ. Group Average   
    δ. Distance Between Centroids   
    **Σωστή απάντηση:** γ

53. Ο αλγόριθμος **Spectral Clustering** βασίζεται στη δημιουργία ενός **πίνακα ομοιότητας** (affinity matrix) και στην ανάλυση:   
    α. Των centroids.   
    β. Της πυκνότητας.   
    γ. Του Laplacian Matrix και των eigenvectors του.   
    δ. Των exemplars.   
    **Σωστή απάντηση:** γ

54. Ποιος αλγόριθμος συσταδοποίησης αναφέρεται στις διαφάνειες ως **ιεραρχικός αναλογικός** (analogue) του k-Means;   
    α. Mean Shift   
    β. DBSCAN   
    γ. BIRCH   
    δ. Ward's Method (Agglomerative Clustering)   
    **Σωστή απάντηση:** δ

55. Στο παράδειγμα ταξινόμησης, η **Confusion Matrix** δείχνει:   
    α. Την κατανομή των χαρακτηριστικών.   
    β. Την απόσταση μεταξύ των σημείων.   
    γ. Την κατανομή των πραγματικών και προβλεπόμενων κλάσεων.   
    δ. Τις συναρτήσεις ενεργοποίησης που χρησιμοποιήθηκαν.   
    **Σωστή απάντηση:** γ

56. Στην Confusion Matrix για binary classification, τι σημαίνει **TP**;   
    α. False Positive (Εσφαλμένο Θετικό)   
    β. True Negative (Αληθές Αρνητικό)   
    γ. True Positive (Αληθές Θετικό)   
    δ. False Negative (Εσφαλμένο Αρνητικό)   
    **Σωστή απάντηση:** γ

57. Στην Confusion Matrix για binary classification, τι σημαίνει **FN**;   
    α. False Positive (Εσφαλμένο Θετικό)   
    β. True Negative (Αληθές Αρνητικό)   
    γ. True Positive (Αληθές Θετικό)   
    δ. False Negative (Εσφαλμένο Αρνητικό)   
    **Σωστή απάντηση:** δ

58. Η μετρική **Precision** στην ταξινόμηση υπολογίζεται ως:   
    α. TP / (TP + TN)   
    β. TP / (TP + FP)   
    γ. TP / (TP + FN)   
    δ. (TP + TN) / (TP + TN + FP + FN)   
    **Σωστή απάντηση:** β

59. Η μετρική **Recall** στην ταξινόμηση υπολογίζεται ως:   
    α. TP / (TP + TN)   
    β. TP / (TP + FP)   
    γ. TP / (TP + FN)   
    δ. (TP + TN) / (TP + TN + FP + FN)
    **Σωστή απάντηση:** γ

60. Η μετρική **F1-score** είναι ο **αρμονικός μέσος** (harmonic mean) ποιας δύο μετρικών;   
    α. Accuracy και Recall   
    β. Precision και Recall      
    γ. Precision και Accuracy   
    δ. TP και TN   
    **Σωστή απάντηση:** β

61. Στο παράδειγμα παλινδρόμησης, η **Bias** στην ανάλυση σφάλματος οφείλεται κυρίως:   
    α. Στην παρουσία θορύβου στα δεδομένα.   
    β. Στις υποκείμενες υποθέσεις που έγιναν για να απλοποιηθεί η συνάρτηση-στόχος.   
    γ. Στον υπερβολικό αριθμό παραμέτρων του μοντέλου.   
    δ. Στην ευαισθησία του μοντέλου στις μικρές διακυμάνσεις των δεδομένων.   
    **Σωστή απάντηση:** β

62. Στο παράδειγμα παλινδρόμησης, η **Variance** στην ανάλυση σφάλματος οφείλεται κυρίως:   
    α. Στην παρουσία θορύβου στα δεδομένα (ή outliers).   
    β. Στις υποκείμενες υποθέσεις που έγιναν για να απλοποιηθεί η συνάρτηση-στόχος.   
    γ. Στον υπερβολικό αριθμό δεδομένων εκπαίδευσης.   
    δ. Στην απλότητα του μοντέλου.   
    **Σωστή απάντηση:** α

63. Ένα μοντέλο που υποφέρει από **Underfitting** συνήθως έχει:   
    α. Υψηλή Bias και Υψηλή Variance.   
    β. Χαμηλή Bias και Υψηλή Variance.   
    γ. Υψηλή Bias και Χαμηλή Variance.   
    δ. Χαμηλή Bias και Χαμηλή Variance.   
    **Σωστή απάντηση:** γ

64. Ένα μοντέλο που υποφέρει από **Overfitting** συνήθως έχει:   
    α. Υψηλή Bias και Υψηλή Variance.   
    β. Χαμηλή Bias και Υψηλή Variance.   
    γ. Υψηλή Bias και Χαμηλή Variance.   
    δ. Χαμηλή Bias και Χαμηλή Variance.   
    **Σωστή απάντηση:** β

65. Σύμφωνα με τις διαφάνειες, η **Batch Normalization** βοηθά στην επιτάχυνση της εκπαίδευσης νευρωνικών δικτύων και στη μείωση του internal covariate shift. Πού συνιστάται να εισάγεται;   
    α. Μόνο στο επίπεδο εισόδου.   
    β. Μόνο στο επίπεδο εξόδου.   
    γ. Μετά τα convolutional ή fully-connected επίπεδα και πριν τις συναρτήσεις ενεργοποίησης.   
    δ. Μόνο σε RNNs.   
    **Σωστή απάντηση:** γ

66. Ποιος αλγόριθμος βελτιστοποίησης, αναφερόμενος στις διαφάνειες, συνδυάζει ιδέες από το Momentum και το RMSprop, χρησιμοποιώντας weighted averages τόσο των gradients όσο και των τετραγώνων τους;   
    α. SGD   
    β. Adagrad   
    γ. Adam   
    δ. Nesterov Accelerated Momentum   
    **Σωστή απάντηση:** γ

67. Στην ιεραρχική συσταδοποίηση, ένα **dendrogram** οπτικοποιεί:   
    α. Την πυκνότητα των δεδομένων.   
    β. Την απόσταση μεταξύ των centroids.   
    γ. Την ιεραρχία των ένθετων συστάδων και τις ακολουθίες συγχωνεύσεων/διασπάσεων.   
    δ. Τα support vectors.   
    **Σωστή απάντηση:** γ

68. Στην ιεραρχική συσταδοποίηση, πώς μπορούμε να καθορίσουμε τον αριθμό των συστάδων αν δεν τον γνωρίζουμε εκ των προτέρων;   
    α. Παρατηρώντας τα peaks στο histogram.   
    β. Επιλέγοντας τον αριθμό των k γειτόνων.   
    γ. "Κόβοντας" το dendrogram στο κατάλληλο επίπεδο.   
    δ. Εφαρμόζοντας την Principal Component Analysis.   
    **Σωστή απάντηση:** γ

69. Στον αλγόριθμο **Affinity Propagation**, αντί να προσδιορίσουμε τον αριθμό των συστάδων (K) εκ των προτέρων, τι χρησιμοποιείται για να επηρεάσει τον αριθμό των συστάδων που θα βρεθούν;   
    α. Η απόσταση Manhattan.   
    β. Ο ρυθμός μάθησης (learning rate).   
    γ. Η προτίμηση (preference) των σημείων να είναι exemplars.   
    δ. Το μέγεθος του mini-batch.   
    **Σωστή απάντηση:** γ

70. Ένα **πλεονέκτημα** του αλγορίθμου **Mean Shift** που αναφέρεται στις διαφάνειες είναι ότι:   
    α. Εγγυάται σφαιρικές συστάδες.   
    β. Δεν απαιτεί τον καθορισμό του αριθμού των συστάδων εκ των προτέρων.   
    γ. Είναι υπολογιστικά πολύ αποδοτικός σε μεγάλα datasets.   
    δ. Χειρίζεται καλά δεδομένα με πολύ διαφορετικές πυκνότητες.   
    **Σωστή απάντηση:** β

71. Ποια μέθοδος συσταδοποίησης βασίζεται στην κατανομή των δεδομένων (density-based) και απαιτεί τους υπερπαραμέτρους Eps (ακτίνα) και MinPts (ελάχιστος αριθμός σημείων);   
    α. k-Means   
    β. Spectral Clustering   
    γ. DBSCAN   
    δ. Gaussian Mixture Model (GMM)   
    **Σωστή απάντηση:** γ

72. Στον αλγόριθμο DBSCAN, ένα σημείο χαρακτηρίζεται ως **Noise Point** αν:   
    α. Έχει τον MinPts γείτονες εντός ακτίνας Eps.   
    β. Βρίσκεται στο περίγραμμα μιας συστάδας.   
    γ. Δεν είναι ούτε Core Point ούτε Border Point.   
    δ. Είναι το πιο απομακρυσμένο σημείο σε μια συστάδα.   
    **Σωστή απάντηση:** γ

73. Ποια από τις παρακάτω είναι μια **αδυναμία** (weakness) του αλγορίθμου DBSCAN;   
    α. Δεν μπορεί να χειριστεί συστάδες με μη-ελλειπτικά σχήματα.   
    β. Είναι ευαίσθητος στην παρουσία θορύβου.   
    γ. Έχει προβλήματα με δεδομένα που έχουν πολύ διαφορετικές πυκνότητες (varying densities).   
    δ. Απαιτεί πολύπλοκο πίνακα ομοιότητας.   
    **Σωστή απάντηση:** γ

74. Ο αλγόριθμος **OPTICS** είναι μια βελτιωμένη έκδοση του DBSCAN που:   
    α. Δημιουργεί πάντα σφαιρικές συστάδες.   
    β. Παράγει ένα ιεραρχικό αποτέλεσμα συσταδοποίησης για μια μεταβλητή ακτίνα γειτονιάς.   
    γ. Δεν απαιτεί κανέναν υπερπαράμετρο.   
    δ. Βασίζεται σε centroids.   
    **Σωστή απάντηση:** β

75. Στον αλγόριθμο OPTICS, ο όρος **Reachability Distance** ορίζεται σε σχέση με ένα άλλο σημείο και είναι:   
    α. Πάντα ίση με την ευκλείδεια απόσταση.   
    β. Πάντα μικρότερη από την Core Distance του άλλου σημείου.   
    γ. Ο μικρότερος δυνατός αριθμός που είναι μεγαλύτερος ή ίσος της Core Distance του άλλου σημείου και της ευκλείδειας απόστασης μεταξύ τους.   
    δ. Μόνο για Core Points.   
    **Σωστή απάντηση:** γ

76. Ο αλγόριθμος **BIRCH** είναι μια τεχνική συσταδοποίησης που είναι **κλιμακωτή** (scalable) και **αποδοτική στη μνήμη** (memory-efficient) επειδή:   
    α. Επεξεργάζεται τα δεδομένα σε mini-batches.   
    β. Χρησιμοποιεί ένα ιεραρχικό δέντρο Clustering Feature (CF tree) για να συμπιέσει τα δεδομένα.   
    γ. Βασίζεται σε πυκνότητες, οι οποίες είναι εύκολο να υπολογιστούν.   
    δ. Χρησιμοποιεί αναλυτικές λύσεις.   
    **Σωστή απάντηση:** β

77. Ο αλγόριθμος **Gaussian Mixture Model (GMM)** είναι ένα **μοντέλο-βασισμένη** (model-based) προσέγγιση συσταδοποίησης που υποθέτει ότι τα δεδομένα προέρχονται από:   
    α. Ένα ενιαίο, ομοιόμορφα κατανεμημένο σύνολο.   
    β. Ένα μίγμα από διάφορα Gaussian components.   
    γ. Ένα δέντρο αποφάσεων.   
    δ. Ένα γράφημα ομοιότητας.   
    **Σωστή απάντηση:** β

78. Ποιος αλγόριθμος χρησιμοποιείται συνήθως για την εκτίμηση των παραμέτρων (means, covariances) ενός GMM;   
    α. k-Means   
    β. DBSCAN   
    γ. Expectation-Maximization (EM)   
    δ. Spectral Clustering   
    **Σωστή απάντηση:** γ

79. Ένα **πλεονέκτημα** του GMM που αναφέρεται στις διαφάνειες είναι ότι:   
    α. Δεν απαιτεί τον καθορισμό του αριθμού των συστάδων εκ των προτέρων.   
    β. Προσφέρει **soft clustering**, αναθέτοντας πιθανότητες στα σημεία να ανήκουν σε διάφορες συστάδες.   
    γ. Είναι ανθεκτικό σε outliers.   
    δ. Είναι υπολογιστικά πολύ αποδοτικό σε μεγάλα datasets.   
    **Σωστή απάντηση:** β

80. Ένας **περιορισμός** του GMM που αναφέρεται στις διαφάνειες είναι ότι:   
    α. Δεν μπορεί να χειριστεί επικαλυπτόμενες συστάδες.   
    β. Έχει προβλήματα με συστάδες σφαιρικού σχήματος.   
    γ. Απαιτεί τον καθορισμό του αριθμού των συστάδων εκ των προτέρων.   
    δ. Δεν προσφέρει καμία μαθηματική ερμηνεία.   
    **Σωστή απάντηση:** γ

81. Στο παράδειγμα ταξινόμησης με τον kNN, όταν ταξινομείται ένα νέο σημείο με k=3, και οι 3 πιο κοντινοί γείτονες ανήκουν στην Κλάση 1, ποια θα είναι η πρόβλεψη;   
    α. Κλάση 1   
    β. Κλάση 2   
    γ. Άγνωστη Κλάση   
    δ. Μια τυχαία κλάση   
    **Σωστή απάντηση:** α

82. Στο παράδειγμα ταξινόμησης με τον Decision Tree (σχήμα στις διαφάνειες), αν ένα σημείο έχει feature 1 = 3 και feature 2 = 3, ποια κλάση θα προβλεφθεί; (Συμβουλευτείτε το σχήμα με το δέντρο).   
    α. Κλάση 1   
    β. Κλάση 2   
    γ. Άγνωστη   
    δ. Εξαρτάται από την υλοποίηση   
    **Σωστή απάντηση:** α (feature1>2 True, feature2<4 True -> Class 1)

83. Στο παράδειγμα παλινδρόμησης, αν χρησιμοποιήσουμε ένα γραμμικό μοντέλο (OLS) και παρατηρήσουμε ότι οι συντελεστές (ιδίως σε Lasso/Elastic Net) είναι κοντά στο μηδέν, τι υποδηλώνει αυτό;   
    α. Ότι το μοντέλο υποφέρει από overfitting.   
    β. Ότι τα αντίστοιχα χαρακτηριστικά δεν έχουν μεγάλη επίδραση στο response variable.   
    γ. Ότι το dataset είναι γραμμικά διαχωρίσιμο.   
    δ. Ότι η συνάρτηση απώλειας δεν συγκλίνει.   
    **Σωστή απάντηση:** β

84. Στο παράδειγμα συσταδοποίησης, γιατί τα IDs των συστάδων (δηλαδή οι αριθμοί ή τα χρώματα) που επιστρέφονται από αλγορίθμους όπως k-Means ή GMM είναι τυχαία και δεν πρέπει να ερμηνεύονται ως κατηγορίες ταξινόμησης;   
    α. Επειδή η συσταδοποίηση είναι supervised learning.   
    β. Επειδή τα IDs εξαρτώνται από την αρχικοποίηση του αλγορίθμου.   
    γ. Επειδή η συσταδοποίηση δεν βρίσκει διαχωριστικές επιφάνειες.   
    δ. Επειδή τα δεδομένα είναι πάντα noisy.   
    **Σωστή απάντηση:** β

85. Ποιο από τα παρακάτω *δεν* είναι βήμα στην Data Science Process όπως παρουσιάζεται στις διαφάνειες;   
    α. Model Selection   
    β. Data Cleansing   
    γ. Communicate/Visualize Results   
    δ. Problem Definition   
    **Σωστή απάντηση:** β (Το Data Cleansing είναι βήμα στην Data Preprocessing, όχι top-level βήμα της Data Science Process στο συγκεκριμένο σχήμα).

86. Τι σημαίνει ο όρος "End-to-End Learning System" στο πλαίσιο του Deep Learning;   
    α. Το μοντέλο εκπαιδεύεται μόνο στο επίπεδο εξόδου.   
    β. Το μοντέλο μαθαίνει αυτόματα τα χαρακτηριστικά (features) από τα ακατέργαστα δεδομένα.   
    γ. Η εκπαίδευση ολοκληρώνεται σε ένα μόνο epoch.   
    δ. Το μοντέλο μπορεί να χειριστεί μόνο σειριακά δεδομένα.   
    **Σωστή απάντηση:** β

87. Τα **Residual Networks (ResNets)** εισήγαγαν μια καινοτομία για την εκπαίδευση πολύ βαθιών δικτύων, την οποία ονομάζουμε:   
    α. Batch Normalization   
    β. Dropout   
    γ. Skip Connections / Identity Mappings   
    δ. Recurrent Connections   
    **Σωστή απάντηση:** γ

88. Στα CNNs, ποια λειτουργία μειώνει τις χωρικές διαστάσεις των feature maps και μειώνει τον αριθμό των παραμέτρων, συμβάλλοντας στην αποφυγή overfitting;   
    α. Convolutional Layer   
    β. Activation Function   
    γ. Pooling Layer   
    δ. Fully Connected Layer   
    **Σωστή απάντηση:** γ

89. Ποια είναι μια **αδυναμία** του αλγορίθμου **BIRCH** που αναφέρεται στις διαφάνειες;   
    α. Δεν μπορεί να χειριστεί outliers.   
    β. Έχει περιορισμούς στο σχηματισμό σφαιρικών συστάδων (Limited to forming spherical clusters).   
    γ. Απαιτεί μεγάλο αριθμό υπερπαραμέτρων.   
    δ. Δεν είναι αποδοτικός στη μνήμη.   
    **Σωστή απάντηση:** β

90. Ο όρος **"Curse of Dimensionality"** αναφέρεται στις δυσκολίες που προκύπτουν όταν εργαζόμαστε με δεδομένα:   
    α. Πολύ λίγες παρατηρήσεις.   
    β. Πολλές εσφαλμένες ετικέτες.   
    γ. Πολύ υψηλές διαστάσεις (πολλά χαρακτηριστικά).   
    δ. Σημαντικό class imbalance.   
    **Σωστή απάντηση:** γ

91. Ποιος αλγόριθμος συσταδοποίησης είναι ευαίσθητος στην **αρχικοποίηση** των παραμέτρων του, η οποία μπορεί να επηρεάσει την ποιότητα των τελικών συστάδων;   
    α. k-Means   
    β. Mean Shift   
    γ. DBSCAN   
    δ. OPTICS   
    **Σωστή απάντηση:** α (Και το GMM είναι ευαίσθητο στην αρχικοποίηση, αλλά το k-Means είναι πιο χαρακτηριστικό παράδειγμα από τις διαφάνειες σε αυτό το πλαίσιο).

92. Σύμφωνα με τις διαφάνειες, ποιος αλγόριθμος Gradient Descent χρησιμοποιεί το "momentum" του gradient για την βελτιστοποίηση;   
    α. Vanilla Gradient Descent   
    β. Stochastic Gradient Descent   
    γ. Gradient Descent with Momentum   
    δ. Adam   
    **Σωστή απάντηση:** γ (Ο Adam *χρησιμοποιεί* momentum, αλλά η επιλογή γ. είναι πιο άμεση περιγραφή αλγορίθμου).

93. Ποια είναι η βασική ιδέα πίσω από το **Ensemble Learning**;   
    α. Η χρήση ενός μόνο, πολύ μεγάλου μοντέλου.   
    β. Η εκπαίδευση πολλαπλών μοντέλων ξεχωριστά και ο συνδυασμός των προβλέψεών τους.   
    γ. Η εκμάθηση μιας ιεραρχίας από συναρτήσεις.   
    δ. Η μείωση της διάστασης πριν την εκπαίδευση.   
    **Σωστή απάντηση:** β

94. Στο παράδειγμα ταξινόμησης του καρκίνου του μαστού, οι περιορισμοί του "job related" απαιτούσαν συγκεκριμένες ελάχιστες αποδόσεις για:
    α. Την συνολική ακρίβεια (Accuracy).   
    β. Την Precision και την Recall.   
    γ. Την ακρίβεια για κάθε μία από τις δύο κλάσεις ξεχωριστά.   
    δ. Τον χρόνο εκτέλεσης του μοντέλου.   
    **Σωστή απάντηση:** γ (Ελάχιστο 90% malignant, 80% non-malignant, που αφορά την ακρίβεια *ανά κλάση*).   

95. Στον αλγόριθμο Naïve Bayes, η υπόθεση ανεξαρτησίας των χαρακτηριστικών είναι **υπό συνθήκη** (conditional) δεδομένης:
    α. Της απόστασης μεταξύ των σημείων.   
    β. Της κλάσης.   
    γ. Του αριθμού των χαρακτηριστικών.   
    δ. Της συνάρτησης απώλειας.   
    **Σωστή απάντηση:** β

96. Τα **Recurrent Neural Networks (RNNs)** είναι πιο ευαίσθητα σε ποιο πρόβλημα κατά την εκπαίδευση σε σχέση με τα CNNs;
    α. Overfitting   
    β. Underfitting   
    γ. Vanishing Gradient   
    δ. Exploding Gradient   
    **Σωστή απάντηση:** γ (Αν και μπορούν να έχουν και Exploding Gradient, οι διαφάνειες εστιάζουν περισσότερο στην ευαισθησία τους στο Vanishing Gradient).

97. Ποιο από τα παρακάτω **δεν** είναι βήμα στην **Ιεραρχική Συσταδοποίηση (Agglomerative)**;   
    α. Έναρξη με τα σημεία ως μεμονωμένες συστάδες.   
    β. Συγχώνευση του κοντινότερου ζεύγους συστάδων.   
    γ. Υπολογισμός των centroids και ανάθεση σημείων σε αυτά.   
    δ. Επανάληψη μέχρι να μείνει μία συστάδα (ή Κ συστάδες).   
    **Σωστή απάντηση:** γ (Αυτό είναι βήμα του k-Means).   

98. Ποιος όρος περιγράφει την διαδικασία εύρεσης των βέλτιστων υπερπαραμέτρων για ένα μοντέλο;   
    α. Regularization   
    β. Optimization   
    γ. Hyper-parameter Tuning   
    δ. Ensemble Learning   
    **Σωστή απάντηση:** γ

99. Όταν χρησιμοποιούμε **k-fold Cross-Validation** για την αξιολόγηση ενός μοντέλου ή την εύρεση υπερπαραμέτρων, το dataset χωρίζεται σε k **ίσα** μέρη. Σε κάθε επανάληψη:   
    α. Το ένα μέρος χρησιμοποιείται για εκπαίδευση και τα υπόλοιπα k-1 για validation.   
    β. Τα k-1 μέρη χρησιμοποιούνται για εκπαίδευση και το ένα για validation.   
    γ. Όλα τα k μέρη χρησιμοποιούνται για εκπαίδευση.   
    δ. Όλα τα k μέρη χρησιμοποιούνται για validation.   
    **Σωστή απάντηση:** β

100. Στην ανάλυση σφάλματος (Bias/Variance), το σφάλμα ενός μοντέλου μπορεί να αναλυθεί σε Bias², Variance και:   
    α. Learning Rate   
    β. Reducible Error Term (unreducible error term στις διαφάνειες)   
    γ. Model Complexity   
    δ. Sample Size   
    **Σωστή απάντηση:** β (Οι διαφάνειες αναφέρουν το Unreducible error term, το οποίο είναι μέρος της ανάλυσης του συνολικού σφάλματος, πέρα από Bias και Variance, και συχνά αναφέρεται ως αναγώγιμο σφάλμα. Αν και η διατύπωση στην επιλογή β. μπορεί να προκαλέσει σύγχυση, είναι η πιο κοντινή στην έννοια από τις διαφάνειες). *Αναθεώρηση*: Η επιλογή β. θα έπρεπε να είναι "Unreducible Error Term". Με τη δεδομένη διατύπωση, είναι η *μόνη* που αναφέρεται σε ανάλυση σφάλματος εκτός Bias/Variance από τις επιλογές, οπότε παραμένει η πιο πιθανή σωστή.
