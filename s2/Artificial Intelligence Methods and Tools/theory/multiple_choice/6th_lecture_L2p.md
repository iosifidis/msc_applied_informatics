# **Ερωτήσεις Πολλαπλής Επιλογής**

1.  **Στο πρόβλημα παλινδρόμησης, ποιος είναι ο κύριος στόχος (slide 2);**
    *   α. Να ταξινομήσει τα δεδομένα σε κατηγορίες.
    *   β. Να προβλέψει την τιμή μιας συνεχούς μεταβλητής απόκρισης (response variable).
    *   γ. Να μειώσει τις διαστάσεις του συνόλου δεδομένων.
    *   δ. Να ομαδοποιήσει παρόμοιες παρατηρήσεις.

2.  **Στη σχέση Y = f(X) + ε (slide 5), τι αντιπροσωπεύει ο όρος ε;**
    *   α. Την υποκείμενη συνάρτηση που συνδέει το X με το Y.
    *   β. Την εκτιμώμενη τιμή της μεταβλητής απόκρισης.
    *   γ. Ένα τυχαίο σφάλμα ή θόρυβο που δεν σχετίζεται με το X.
    *   δ. Τις μεταβλητές πρόβλεψης (predictors).

3.  **Σύμφωνα με το slide 6, γιατί οι "Actual values" δεν είναι παρατηρήσιμες στην πράξη;**
    *   α. Γιατί δεν έχουμε την πληροφορία για την f(x).
    *   β. Γιατί ακόμα και ο καλύτερος εξοπλισμός θα έχει κάποιο θόρυβο.
    *   γ. Γιατί δεν ξέρουμε αν έχουμε γραμμικό μοντέλο.
    *   δ. Όλα τα παραπάνω.

4.  **Ποια ήταν η "απλή ιδέα" για εκτίμηση που παρουσιάστηκε στο slide 11 και κρίθηκε ως "όχι χρήσιμη";**
    *   α. Εκτίμηση με βάση τον πλησιέστερο γείτονα.
    *   β. Εκτίμηση με βάση τον μέσο όρο των k-πλησιέστερων γειτόνων.
    *   γ. Εκτίμηση χρησιμοποιώντας την μέση τιμή όλων των παρατηρημένων τιμών της μεταβλητής απόκρισης.
    *   δ. Εκτίμηση χρησιμοποιώντας γραμμική παλινδρόμηση.

5.  **Στο μοντέλο k-Nearest Neighbor (kNN) για παλινδρόμηση (slide 15), πώς προβλέπεται η τιμή για ένα νέο σημείο x\_q;**
    *   α. Λαμβάνοντας την τιμή του πιο μακρινού γείτονα.
    *   β. Λαμβάνοντας τον μέσο όρο των τιμών των k-πλησιέστερων γειτόνων.
    *   γ. Χρησιμοποιώντας μόνο τον πλησιέστερο γείτονα (k=1).
    *   δ. Λαμβάνοντας τη διάμεσο των τιμών των k-πλησιέστερων γειτόνων.

6.  **Τι συμβαίνει στις προβλέψεις του μοντέλου kNN για παλινδρόμηση καθώς η τιμή του k αυξάνεται σημαντικά (slide 17);**
    *   α. Οι προβλέψεις γίνονται πιο ακριβείς.
    *   β. Οι προβλέψεις τείνουν προς την μέση τιμή όλων των τιμών της μεταβλητής απόκρισης.
    *   γ. Οι προβλέψεις τείνουν προς μηδέν.
    *   δ. Το μοντέλο γίνεται πιο ευαίσθητο στον θόρυβο.

7.  **Εκτός από το kNN, ποιες άλλες τεχνικές αναφέρονται στο slide 18 ότι μπορούν να χρησιμοποιηθούν για παλινδρόμηση;**
    *   α. Μόνο γραμμική παλινδρόμηση.
    *   β. Decision trees, support vector regression, Naïve Bayes regression.
    *   γ. Clustering αλγόριθμοι.
    *   δ. Μόνο παραμετρικές μέθοδοι.

8.  **Στην απλή γραμμική παλινδρόμηση Y = β1\*X + β0 + ε, τι αντιπροσωπεύουν τα β1 και β0 (slide 21);**
    *   α. β1 είναι η τιμή της μεταβλητής απόκρισης και β0 είναι ο θόρυβος.
    *   β. β1 είναι η κλίση της γραμμής και β0 είναι η τομή με τον άξονα Υ (intercept).
    *   γ. β1 είναι η τομή με τον άξονα Υ και β0 είναι η κλίση.
    *   δ. Είναι οι εκτιμώμενες τιμές της μεταβλητής πρόβλεψης.

9.  **Ποια συνάρτηση απώλειας (loss function) χρησιμοποιείται συνήθως για την εκτίμηση των συντελεστών στην γραμμική παλινδρόμηση (slide 27);**
    *   α. Absolute Error (Απόλυτο Σφάλμα)
    *   β. Mean Squared Error (MSE) (Μέσο Τετραγωνικό Σφάλμα)
    *   γ. Accuracy (Ακρίβεια)
    *   δ. F1-score

10. **Ποια είναι η κύρια ιδέα πίσω από την μέθοδο "gradient descent" για την εκτίμηση των συντελεστών (slide 31-32);**
    *   α. Να υπολογίζει απευθείας τους συντελεστές με έναν κλειστό τύπο.
    *   β. Να δοκιμάζει τυχαία συνδυασμούς συντελεστών.
    *   γ. Να κινείται επαναληπτικά προς την κατεύθυνση που μειώνει την συνάρτηση απώλειας, χρησιμοποιώντας την παράγωγο.
    *   δ. Να επιλέγει πάντα το μεγαλύτερο βήμα (step size).

11. **Τι αντιπροσωπεύει ο όρος "λ" (λάμδα) στην ενημέρωση των βαρών κατά το gradient descent (w\_new = w\_old - λ \* dL/dw) (slide 33);**
    *   α. Την τιμή της συνάρτησης απώλειας.
    *   β. Την παράγωγο της συνάρτησης απώλειας.
    *   γ. Τον ρυθμό μάθησης (learning rate).
    *   δ. Τον αριθμό των επαναλήψεων.

12. **Γιατί η μέθοδος "brute force" για την εύρεση των βέλτιστων συντελεστών παλινδρόμησης δεν θεωρείται καλή πρακτική (slide 28);**
    *   α. Είναι πολύ γρήγορη και μπορεί να χάσει το βέλτιστο.
    *   β. Απαιτεί τον υπολογισμό της συνάρτησης απώλειας για κάθε πιθανό συνδυασμό παραμέτρων, κάτι που είναι υπολογιστικά ασύμφορο.
    *   γ. Δεν μπορεί να χειριστεί γραμμικά μοντέλα.
    *   δ. Πάντα οδηγεί σε τοπικά ελάχιστα.

13. **Σύμφωνα με το slide 36, από τι εξαρτώνται οι παράμετροι ενός μοντέλου παλινδρόμησης (π.χ., β1, β0);**
    *   α. Μόνο από τον τύπο του μοντέλου που επιλέχθηκε.
    *   β. Από τα δεδομένα εκπαίδευσης (train data).
    *   γ. Από τα δεδομένα δοκιμής (test data).
    *   δ. Από τον αριθμό των χαρακτηριστικών μόνο.

14. **Τι είναι η λύση "Ordinary Least Squares" (OLS) (slide 39);**
    *   α. Μια επαναληπτική μέθοδος για την εύρεση των συντελεστών.
    *   β. Μια μέθοδος που χρησιμοποιεί μόνο τα πλησιέστερα γειτονικά σημεία.
    *   γ. Μια λύση κλειστού τύπου για την ελαχιστοποίηση του αθροίσματος των τετραγώνων των υπολοίπων.
    *   δ. Μια μέθοδος που προσθέτει όρους ποινής για την κανονικοποίηση.

15. **Ποια είναι μια κύρια αδυναμία της λύσης OLS, ειδικά όταν υπάρχουν πολλοί προβλεπτικοί παράγοντες ή συσχετισμένοι προβλεπτικοί παράγοντες (slide 43);**
    *   α. Χαμηλή μεροληψία (bias).
    *   β. Χαμηλή διακύμανση (variance).
    *   γ. Υψηλή διακύμανση (high variance) και αστάθεια.
    *   δ. Αδυναμία χειρισμού συνεχών μεταβλητών.

16. **Ποιος είναι ο σκοπός της κανονικοποίησης (Regularization) στην παλινδρόμηση (slide 43);**
    *   α. Να αυξήσει την πολυπλοκότητα του μοντέλου.
    *   β. Να κάνει το μοντέλο πιο ευαίσθητο στον θόρυβο.
    *   γ. Να συρρικνώσει τους συντελεστές, να ελέγξει την πολυπλοκότητα και να σταθεροποιήσει το μοντέλο.
    *   δ. Να εξασφαλίσει ότι όλοι οι συντελεστές είναι θετικοί.

17. **Ποια είναι η κύρια διαφορά μεταξύ Ridge Regression και Lasso Regression όσον αφορά την επίδρασή τους στους συντελεστές (slides 44, 45);**
    *   α. Η Ridge μηδενίζει κάποιους συντελεστές, ενώ η Lasso όχι.
    *   β. Η Lasso μηδενίζει κάποιους συντελεστές (επιλογή χαρακτηριστικών), ενώ η Ridge τους συρρικνώνει προς το μηδέν αλλά δεν τους μηδενίζει απαραίτητα.
    *   γ. Και οι δύο πάντα μηδενίζουν τους ίδιους συντελεστές.
    *   δ. Η Ridge χρησιμοποιεί την L1 νόρμα, ενώ η Lasso την L2 νόρμα.

18. **Πώς η Elastic Net Regression συνδυάζει τα χαρακτηριστικά της Ridge και της Lasso (slide 46);**
    *   α. Χρησιμοποιεί μόνο την L1 νόρμα.
    *   β. Χρησιμοποιεί μόνο την L2 νόρμα.
    *   γ. Ελαχιστοποιεί μια συνάρτηση απώλειας με όρους ποινής και από την L1 και από την L2 νόρμα.
    *   δ. Δεν χρησιμοποιεί όρους ποινής.

19. **Στην ανάλυση παλινδρόμησης, τι σημαίνει υψηλή μεροληψία (high bias) (slide 40);**
    *   α. Το μοντέλο είναι πολύ ευαίσθητο στις μικρές διακυμάνσεις των δεδομένων εκπαίδευσης.
    *   β. Το μοντέλο κάνει ισχυρές υποθέσεις για την μορφή της συνάρτησης στόχου, οδηγώντας πιθανώς σε υποπροσαρμογή (underfitting).
    *   γ. Το μοντέλο προσαρμόζεται τέλεια στα δεδομένα εκπαίδευσης.
    *   δ. Το μοντέλο μαθαίνει τον θόρυβο στα δεδομένα εκπαίδευσης.

20. **Σύμφωνα με τα διαγράμματα στο slide 48 (και 53, 54), ποια μετρική χρησιμοποιείται για την "γρήγορη αξιολόγηση της απόδοσης του μοντέλου" και πώς ερμηνεύεται;**
    *   α. Accuracy: υψηλότερη τιμή, καλύτερο μοντέλο.
    *   β. RMSE (Root Mean Squared Error): χαμηλότερη τιμή, καλύτερο μοντέλο.
    *   γ. R-squared: χαμηλότερη τιμή, καλύτερο μοντέλο.
    *   δ. MAE (Mean Absolute Error): υψηλότερη τιμή, καλύτερο μοντέλο.

---

**Απαντήσεις**

1.  **β.** Να προβλέψει την τιμή μιας συνεχούς μεταβλητής απόκρισης (response variable).
2.  **γ.** Ένα τυχαίο σφάλμα ή θόρυβο που δεν σχετίζεται με το X.
3.  **δ.** Όλα τα παραπάνω.
4.  **γ.** Εκτίμηση χρησιμοποιώντας την μέση τιμή όλων των παρατηρημένων τιμών της μεταβλητής απόκρισης.
5.  **β.** Λαμβάνοντας τον μέσο όρο των τιμών των k-πλησιέστερων γειτόνων.
6.  **β.** Οι προβλέψεις τείνουν προς την μέση τιμή όλων των τιμών της μεταβλητής απόκρισης.
7.  **β.** Decision trees, support vector regression, Naïve Bayes regression.
8.  **β.** β1 είναι η κλίση της γραμμής και β0 είναι η τομή με τον άξονα Υ (intercept).
9.  **β.** Mean Squared Error (MSE) (Μέσο Τετραγωνικό Σφάλμα)
10. **γ.** Να κινείται επαναληπτικά προς την κατεύθυνση που μειώνει την συνάρτηση απώλειας, χρησιμοποιώντας την παράγωγο.
11. **γ.** Τον ρυθμό μάθησης (learning rate).
12. **β.** Απαιτεί τον υπολογισμό της συνάρτησης απώλειας για κάθε πιθανό συνδυασμό παραμέτρων, κάτι που είναι υπολογιστικά ασύμφορο.
13. **β.** Από τα δεδομένα εκπαίδευσης (train data).
14. **γ.** Μια λύση κλειστού τύπου για την ελαχιστοποίηση του αθροίσματος των τετραγώνων των υπολοίπων.
15. **γ.** Υψηλή διακύμανση (high variance) και αστάθεια.
16. **γ.** Να συρρικνώσει τους συντελεστές, να ελέγξει την πολυπλοκότητα και να σταθεροποιήσει το μοντέλο.
17. **β.** Η Lasso μηδενίζει κάποιους συντελεστές (επιλογή χαρακτηριστικών), ενώ η Ridge τους συρρικνώνει προς το μηδέν αλλά δεν τους μηδενίζει απαραίτητα.
18. **γ.** Ελαχιστοποιεί μια συνάρτηση απώλειας με όρους ποινής και από την L1 και από την L2 νόρμα.
19. **β.** Το μοντέλο κάνει ισχυρές υποθέσεις για την μορφή της συνάρτησης στόχου, οδηγώντας πιθανώς σε υποπροσαρμογή (underfitting).
20. **β.** RMSE (Root Mean Squared Error): χαμηλότερη τιμή, καλύτερο μοντέλο.
