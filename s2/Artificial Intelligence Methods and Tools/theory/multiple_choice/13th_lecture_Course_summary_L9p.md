# ΣΥΝΟΨΗ

## **ΕΝΟΤΗΤΑ 1: Μονή απάντηση ανά ερώτημα.**

### **Ε1.** Σε ποιες βασικές κατηγορίες χωρίζεται η Μηχανική Μάθηση σύμφωνα με τις διαφάνειες;
α. Linear, Non-linear, Symbolic
β. Supervised, Unsupervised, Reinforcement Learning
γ. Classification, Regression, Clustering
δ. Parametric, Non-parametric, Ensemble

**Σωστή απάντηση:** β. Supervised, Unsupervised, Reinforcement Learning

### **Ε2.** Ποια από τις παρακάτω τεχνικές χαρακτηρίζεται ως **μη επιβλεπόμενη** (Unsupervised Learning) στις διαφάνειες;
α. Logistic Regression
β. Decision Trees
γ. k-means Clustering
δ. Naïve Bayes

**Σωστή απάντηση:** γ. k-means Clustering

### **Ε3.** Ποιος είναι ο κύριος σκοπός μιας **μη γραμμικής** (non-linear) συνάρτησης ενεργοποίησης σε ένα νευρωνικό δίκτυο;
α. Να επιταχύνει την εκπαίδευση του δικτύου.
β. Να επιτρέπει στο δίκτυο να μαθαίνει μη-γραμμικές σχέσεις στα δεδομένα.
γ. Να μειώνει τον αριθμό των παραμέτρων του δικτύου.
δ. Να διασφαλίζει τη σύγκλιση του αλγορίθμου Gradient Descent.

**Σωστή απάντηση:** β. Να επιτρέπει στο δίκτυο να μαθαίνει μη-γραμμικές σχέσεις στα δεδομένα.

### **Ε4.** Ποια από τις παρακάτω συναρτήσεις ενεργοποίησης παρουσιάζεται στις διαφάνειες ως λιγότερο συχνή σε σύγχρονα δίκτυα λόγω του προβλήματος saturation (κορεσμού) στα άκρα;
α. ReLU
β. Tanh
γ. Leaky ReLU
δ. Linear

**Σωστή απάντηση:** β. Tanh (και η Sigmoid παρουσιάζει το ίδιο πρόβλημα)

### **Ε5.** Ποια συνάρτηση ενεργοποίησης χρησιμοποιείται συνήθως στο **τελευταίο επίπεδο** (output layer) ενός νευρωνικού δικτύου για προβλήματα **πολλαπλής ταξινόμησης** (multi-class classification), ώστε η έξοδος να ερμηνεύεται ως πιθανότητα;
α. Sigmoid
β. ReLU
γ. Softmax
δ. Linear

**Σωστή απάντηση:** γ. Softmax

### **Ε6.** Τι μετράει η συνάρτηση απώλειας (Loss Function) κατά την εκπαίδευση ενός νευρωνικού δικτύου;
α. Τον χρόνο εκπαίδευσης.
β. Τη διαφορά (σφάλμα) μεταξύ της πρόβλεψης του μοντέλου και της πραγματικής ετικέτας.
γ. Την πολυπλοκότητα του μοντέλου.
δ. Τον αριθμό των κρυμμένων επιπέδων.

**Σωστή απάντηση:** β. Τη διαφορά (σφάλμα) μεταξύ της πρόβλεψης του μοντέλου και της πραγματικής ετικέτας.

### **Ε7.** Ποιος αλγόριθμος χρησιμοποιείται ευρέως για την **ενημέρωση των βαρών** σε ένα νευρωνικό δίκτυο, βασιζόμενος στον υπολογισμό των κλίσεων (gradients) της συνάρτησης απώλειας;
α. k-means Clustering
β. Principal Component Analysis (PCA)
γ. Backpropagation
δ. Decision Tree Induction

**Σωστή απάντηση:** γ. Backpropagation

### **Ε8.** Γιατί το **Mini-batch Gradient Descent** προτιμάται από το standard (Vanilla) Gradient Descent σε μεγάλα σύνολα δεδομένων;
α. Εγγυάται πάντα την εύρεση του ολικού βέλτιστου (global minimum).
β. Είναι υπολογιστικά πιο αποδοτικό, χρησιμοποιώντας ένα μικρό υποσύνολο δεδομένων για την προσέγγιση της κλίσης.
γ. Δεν επηρεάζεται από προβλήματα όπως vanishing/exploding gradients.
δ. Δεν απαιτεί την επιλογή learning rate.

**Σωστή απάντηση:** β. Είναι υπολογιστικά πιο αποδοτικό, χρησιμοποιώντας ένα μικρό υποσύνολο δεδομένων για την προσέγγιση της κλίσης.

### **Ε9.** Ποιο πρόβλημα αντιμετωπίζει η εκπαίδευση νευρωνικών δικτύων όταν οι κλίσεις (gradients) γίνονται πολύ μικρές καθώς διαδίδονται προς τα πίσω στα επίπεδα;
α. Overfitting
β. Underfitting
γ. Exploding Gradient
δ. Vanishing Gradient

**Σωστή απάντηση:** δ. Vanishing Gradient

### **Ε10.** Ποια από τις παρακάτω τεχνικές χρησιμοποιείται για την αντιμετώπιση της υπερπροσαρμογής (overfitting) σε νευρωνικά δίκτυα, "απενεργοποιώντας" τυχαία μονάδες (και τις συνδέσεις τους) κατά την εκπαίδευση;
α. Batch Normalization
β. Dropout
γ. Early Stopping
δ. Gradient Descent with Momentum

**Σωστή απάντηση:** β. Dropout

### **Ε11.** Ποια τεχνική κανονικοποίησης (regularization) σταματά την εκπαίδευση όταν η απόδοση του μοντέλου σε ένα **validation set** πάψει να βελτιώνεται;
α. Weight Decay
β. Dropout
γ. Early Stopping
δ. Batch Normalization

**Σωστή απάντηση:** γ. Early Stopping

### **Ε12.** Τα Συνελικτικά Νευρωνικά Δίκτυα (CNNs) σχεδιάστηκαν κυρίως για την επεξεργασία ποιου τύπου δεδομένων;
α. Χρονοσειρών
β. Κειμένου
γ. Εικόνων
δ. Δομημένων (tabular) δεδομένων

**Σωστή απάντηση:** γ. Εικόνων

### **Ε13.** Τα Αναδρομικά Νευρωνικά Δίκτυα (RNNs) είναι ιδιαίτερα κατάλληλα για την επεξεργασία ποιου τύπου δεδομένων;
α. Εικόνων
β. Δεδομένων με χωρική δομή
γ. Ακολουθιακών δεδομένων (π.χ. κείμενο, χρονοσειρές)
δ. Δεδομένων που είναι γραμμικά διαχωρίσιμα

**Σωστή απάντηση:** γ. Ακολουθιακών δεδομένων (π.χ. κείμενο, χρονοσειρές)

### **Ε14.** Σύμφωνα με το No-Free-Lunch Theorem, όπως αναφέρεται στις διαφάνειες:
α. Τα νευρωνικά δίκτυα είναι πάντα ο καλύτερος αλγόριθμος.
β. Κανένας μεμονωμένος αλγόριθμος ταξινόμησης δεν είναι ο καλύτερος για **όλα** τα πιθανά προβλήματα.
γ. Η υπερπροσαρμογή (overfitting) είναι αναπόφευκτη.
δ. Η μη επιβλεπόμενη μάθηση είναι πάντα πιο δύσκολη από την επιβλεπόμενη.

**Σωστή απάντηση:** β. Κανένας μεμονωμένος αλγόριθμος ταξινόμησης δεν είναι ο καλύτερος για **όλα** τα πιθανά προβλήματα.

## **ΕΝΟΤΗΤΑ 3: Πολλαπλές απαντήσεις ανά ερώτημα.**

### **Ε15.** Ποιες από τις παρακάτω τεχνικές θεωρούνται μορφές **μη επιβλεπόμενης** (Unsupervised Learning) μάθησης σύμφωνα με τις διαφάνειες;
α. Density Estimation
β. Regression
γ. Clustering
δ. Dimensionality Reduction
ε. Classification

**Σωστές απαντήσεις:** α. Density Estimation, γ. Clustering, δ. Dimensionality Reduction

### **Ε16.** Ποιες από τις παρακάτω συναρτήσεις ενεργοποίησης εισάγουν **μη-γραμμικότητα** σε ένα νευρωνικό δίκτυο, επιτρέποντάς του να μαθαίνει μη-γραμμικές διαχωριστικές επιφάνειες;
α. Linear / Identity
β. ReLU
γ. Tanh
δ. Sigmoid
ε. Softmax

**Σωστές απαντήσεις:** β. ReLU, γ. Tanh, δ. Sigmoid, ε. Softmax (Η Linear / Identity είναι γραμμική)

### **Ε17.** Ποιες από τις παρακάτω τεχνικές αναφέρονται στις διαφάνειες ως μέθοδοι **κανονικοποίησης** (regularization) για την αντιμετώπιση της υπερπροσαρμογής (overfitting);
α. Weight Decay
β. Batch Normalization
γ. Early Stopping
δ. Dropout
ε. Gradient Descent

**Σωστές απαντήσεις:** α. Weight Decay, β. Batch Normalization, γ. Early Stopping, δ. Dropout (Το Gradient Descent είναι αλγόριθμος βελτιστοποίησης, όχι κανονικοποίησης)

### **Ε18.** Ποιες από τις παρακάτω μεθόδους **βελτιστοποίησης** (optimizers), βασιζόμενες στο Gradient Descent, αναφέρονται ονομαστικά στις διαφάνειες;
α. Adam
β. Batch Normalization
γ. RMSprop
δ. Early Stopping
ε. Momentum (as part of GD w/ Momentum)

**Σωστές απαντήσεις:** α. Adam, γ. RMSprop, ε. Momentum (Το Batch Normalization και το Early Stopping είναι τεχνικές κανονικοποίησης)

