# **Ερωτήσεις Πολλαπλής Επιλογής**

1.  **Ποια από τις παρακάτω ιδιότητες ΔΕΝ αποτελεί τυπική ιδιότητα ενός μέτρου απόστασης (distance measure) (slide 3);**
    *   α. D(A,B) = D(B,A) (Συμμετρία)
    *   β. D(A,A) = 1 (Σταθερότητα Αυτο-Ομοιότητας)
    *   γ. D(A,B) = 0 αν και μόνο αν A=B (Θετικότητα/Διαχωρισμός)
    *   δ. D(A,B) ≤ D(A,C) + D(B,C) (Τριγωνική Ανισότητα)

2.  **Σύμφωνα με το slide 8, ποια πληροφορία ΔΕΝ είναι διαθέσιμη στους αλγορίθμους συσταδοποίησης κατά την επεξεργασία των συνθετικών datasets;**
    *   α. Οι τιμές των χαρακτηριστικών (features' values).
    *   β. Οι πραγματικές κλάσεις (classes).
    *   γ. Ο αριθμός των χαρακτηριστικών.
    *   δ. Ο αριθμός των δειγμάτων.

3.  **Ποια είναι η κύρια διάκριση μεταξύ της Partitional Clustering και της Hierarchical Clustering (slide 10);**
    *   α. Η Partitional δημιουργεί επικαλυπτόμενες συστάδες, ενώ η Hierarchical όχι.
    *   β. Η Hierarchical δημιουργεί μια διαίρεση των αντικειμένων σε μη επικαλυπτόμενες υποομάδες, ενώ η Partitional ένα σύνολο ένθετων συστάδων.
    *   γ. Η Partitional δημιουργεί μια διαίρεση των αντικειμένων σε μη επικαλυπτόμενες υποομάδες (συστάδες), ενώ η Hierarchical ένα σύνολο ένθετων συστάδων οργανωμένων ως ιεραρχικό δέντρο.
    *   δ. Η Partitional απαιτεί προκαθορισμένο αριθμό συστάδων, ενώ η Hierarchical ποτέ.

4.  **Ποιο από τα παρακάτω ΔΕΝ είναι απαραίτητο να καθοριστεί εκ των προτέρων στον αλγόριθμο k-means (slide 14);**
    *   α. Ο αριθμός των συστάδων (K).
    *   β. Τα αρχικά κεντροειδή (initial centroids).
    *   γ. Η μέθοδος υπολογισμού της απόστασης.
    *   δ. Το σχήμα των συστάδων.

5.  **Ποιο από τα παρακάτω αποτελεί περιορισμό του αλγορίθμου k-means (slide 16-19);**
    *   α. Αντιμετωπίζει προβλήματα όταν οι συστάδες έχουν διαφορετικά μεγέθη, πυκνότητες ή μη σφαιρικά σχήματα.
    *   β. Είναι πολύ αργός για μικρά σύνολα δεδομένων.
    *   γ. Δεν μπορεί να χειριστεί δεδομένα με αριθμητικές τιμές.
    *   δ. Πάντα βρίσκει το ολικό βέλτιστο.

6.  **Ποιο από τα παρακάτω ΔΕΝ αποτελεί πλεονέκτημα (Pro) του αλγορίθμου Affinity Propagation (AP) (slide 22);**
    *   α. Δεν απαιτεί τον προκαθορισμό του αριθμού των συστάδων.
    *   β. Μπορεί να παράγει συστάδες υψηλής ποιότητας ακόμα και με διαφορετικές πυκνότητες/μεγέθη.
    *   γ. Είναι υπολογιστικά φθηνός, ειδικά για μεγάλα σύνολα δεδομένων.
    *   δ. Μπορεί να χρησιμοποιηθεί για δεδομένα με πολύπλοκες σχέσεις και μη γραμμικές δομές.

7.  **Ποια είναι η βασική ιδέα πίσω από τον αλγόριθμο Mean Shift (slide 23-24);**
    *   α. Η ελαχιστοποίηση της ενδο-συστάδας διακύμανσης.
    *   β. Η εύρεση των πιο πυκνών περιοχών μετακινώντας επαναληπτικά ένα παράθυρο προς το κέντρο μάζας των σημείων εντός του.
    *   γ. Η δημιουργία ενός γραφήματος ομοιότητας και η χρήση ιδιοδιανυσμάτων.
    *   δ. Η διαδοχική συγχώνευση των πλησιέστερων συστάδων.

8.  **Ποιο από τα παρακάτω ΔΕΝ αποτελεί βήμα στην Spectral Clustering (slide 33);**
    *   α. Δημιουργία ενός γραφήματος ομοιότητας (similarity graph).
    *   β. Υπολογισμός των πρώτων k ιδιοδιανυσμάτων του πίνακα Laplace.
    *   γ. Εφαρμογή του k-means στα αρχικά χαρακτηριστικά.
    *   δ. Εφαρμογή του k-means στα χαρακτηριστικά που προκύπτουν από τα ιδιοδιανύσματα.

9.  **Τι είναι ο πίνακας Laplace (L) σε σχέση με τον πίνακα γειτνίασης (A ή W) και τον πίνακα βαθμών (D) ενός γραφήματος (slide 36);**
    *   α. L = D + W
    *   β. L = D * W
    *   γ. L = D - W
    *   δ. L = W - D

10. **Ποια είναι η βασική προσέγγιση της Agglomerative Hierarchical Clustering (slide 40, 43);**
    *   α. Ξεκινά με όλα τα σημεία σε μια συστάδα και τα διαχωρίζει διαδοχικά.
    *   β. Ξεκινά με κάθε σημείο ως ξεχωριστή συστάδα και συγχωνεύει διαδοχικά τις πλησιέστερες συστάδες.
    *   γ. Επιλέγει τυχαία κεντροειδή και αναθέτει σημεία σε αυτά.
    *   δ. Μετακινεί ένα παράθυρο για να βρει πυκνές περιοχές.

11. **Πώς μπορεί ένα δενδρόγραμμα (dendrogram) να βοηθήσει στην ιεραρχική συσταδοποίηση (slide 41-42);**
    *   α. Δείχνει την πυκνότητα κάθε συστάδας.
    *   β. Βοηθά στον προσδιορισμό του "σωστού" αριθμού συστάδων και στον εντοπισμό ακραίων τιμών (outliers).
    *   γ. Υπολογίζει την απόσταση μεταξύ όλων των ζευγών σημείων.
    *   δ. Χρησιμοποιείται μόνο για την απεικόνιση των αποτελεσμάτων του k-means.

12. **Ποια μέθοδος ορισμού της απόστασης μεταξύ συστάδων (inter-cluster distance) βασίζεται στα δύο πιο απομακρυσμένα σημεία στις διαφορετικές συστάδες (slide 57);**
    *   α. MIN (Single Link)
    *   β. MAX (Complete Linkage)
    *   γ. Group Average
    *   δ. Ward's Method

13. **Ποιο είναι ένα ισχυρό σημείο της μεθόδου MIN (Single Link) στην ιεραρχική συσταδοποίηση (slide 55);**
    *   α. Είναι ανθεκτική στον θόρυβο.
    *   β. Μπορεί να χειριστεί μη ελλειπτικά σχήματα.
    *   γ. Πάντα βρίσκει σφαιρικές συστάδες.
    *   δ. Δεν επηρεάζεται από ακραίες τιμές.

14. **Στον αλγόριθμο DBSCAN, τι χαρακτηρίζει ένα "core point" (slide 68);**
    *   α. Είναι ένα σημείο που δεν ανήκει σε καμία συστάδα (noise point).
    *   β. Είναι ένα σημείο που βρίσκεται στην περιφέρεια μιας συστάδας.
    *   γ. Είναι ένα σημείο που έχει τουλάχιστον έναν καθορισμένο αριθμό σημείων (MinPts) εντός μιας καθορισμένης ακτίνας (Eps).
    *   δ. Είναι πάντα το κεντροειδές μιας συστάδας.

15. **Πότε ο αλγόριθμος DBSCAN ΔΕΝ λειτουργεί καλά (slide 73);**
    *   α. Όταν οι συστάδες έχουν διαφορετικά σχήματα και μεγέθη.
    *   β. Όταν υπάρχουν ακραίες τιμές (noise).
    *   γ. Όταν οι συστάδες έχουν σημαντικά διαφορετικές πυκνότητες ή σε δεδομένα υψηλών διαστάσεων.
    *   δ. Όταν ο αριθμός των συστάδων είναι μικρός.

16. **Ποια είναι η κύρια διαφορά μεταξύ DBSCAN και OPTICS (slide 74);**
    *   α. Ο OPTICS δεν απαιτεί την παράμετρο Eps.
    *   β. Ο DBSCAN παράγει ιεραρχικά αποτελέσματα, ενώ ο OPTICS όχι.
    *   γ. Ο OPTICS παράγει ένα ιεραρχικό αποτέλεσμα συσταδοποίησης για μια μεταβλητή ακτίνα γειτονιάς και δεν αναθέτει άμεσα συμμετοχές σε συστάδες.
    *   δ. Ο OPTICS είναι πάντα πιο γρήγορος από τον DBSCAN.

17. **Στον αλγόριθμο OPTICS, τι είναι το "reachability distance" (slide 75);**
    *   α. Η ελάχιστη απόσταση για να θεωρηθεί ένα σημείο p ως core point.
    *   β. Η μικρότερη απόσταση ώστε ένα σημείο q να είναι άμεσα προσβάσιμο από ένα core point p (και όχι μικρότερη από το core distance του p).
    *   γ. Πάντα η Ευκλείδεια απόσταση μεταξύ δύο σημείων.
    *   δ. Η μέγιστη απόσταση μέσα σε μια συστάδα.

18. **Πώς εξάγονται συνήθως οι συστάδες από το reachability plot του OPTICS (slide 78);**
    *   α. Οι κορυφές (peaks) στο διάγραμμα αντιπροσωπεύουν πυκνές συστάδες.
    *   β. Οι κοιλάδες (valleys) στο διάγραμμα αντιπροσωπεύουν πυκνές συστάδες.
    *   γ. Το διάγραμμα δείχνει απευθείας τις συστάδες με διαφορετικά χρώματα.
    *   δ. Μετρώντας τον αριθμό των σημείων σε κάθε περιοχή.

19. **Ποια είναι η κύρια δομή δεδομένων που χρησιμοποιεί ο αλγόριθμος BIRCH (slide 81);**
    *   α. Ένα δενδρόγραμμα.
    *   β. Ένα CF (Clustering Feature) tree.
    *   γ. Έναν πίνακα γειτνίασης.
    *   δ. Μια λίστα γειτόνων για κάθε σημείο.

20. **Ποια είναι μια βασική υπόθεση του αλγορίθμου Gaussian Mixture Models (GMM) (slide 84);**
    *   α. Όλες οι συστάδες έχουν την ίδια πυκνότητα.
    *   β. Τα δεδομένα παράγονται από ένα μείγμα πολλών Γκαουσιανών συνιστωσών, όπου κάθε συνιστώσα αντιπροσωπεύει μια ξεχωριστή συστάδα.
    *   γ. Οι συστάδες πρέπει να έχουν μη σφαιρικά σχήματα.
    *   δ. Δεν απαιτεί τον προκαθορισμό του αριθμού των συσταδών.

---

**Απαντήσεις**

1.  **β.** D(A,A) = 1 (Η σωστή ιδιότητα είναι D(A,A) = 0)
2.  **β.** Οι πραγματικές κλάσεις (classes). (Επίσης τα επίπεδα θορύβου κτλ.)
3.  **γ.** Η Partitional δημιουργεί μια διαίρεση των αντικειμένων σε μη επικαλυπτόμενες υποομάδες (συστάδες), ενώ η Hierarchical ένα σύνολο ένθετων συστάδων οργανωμένων ως ιεραρχικό δέντρο.
4.  **δ.** Το σχήμα των συστάδων.
5.  **α.** Αντιμετωπίζει προβλήματα όταν οι συστάδες έχουν διαφορετικά μεγέθη, πυκνότητες ή μη σφαιρικά σχήματα.
6.  **γ.** Είναι υπολογιστικά φθηνός, ειδικά για μεγάλα σύνολα δεδομένων. (Είναι ακριβός)
7.  **β.** Η εύρεση των πιο πυκνών περιοχών μετακινώντας επαναληπτικά ένα παράθυρο προς το κέντρο μάζας των σημείων εντός του.
8.  **γ.** Εφαρμογή του k-means στα αρχικά χαρακτηριστικά.
9.  **γ.** L = D - W
10. **β.** Ξεκινά με κάθε σημείο ως ξεχωριστή συστάδα και συγχωνεύει διαδοχικά τις πλησιέστερες συστάδες.
11. **β.** Βοηθά στον προσδιορισμό του "σωστού" αριθμού συστάδων και στον εντοπισμό ακραίων τιμών (outliers).
12. **β.** MAX (Complete Linkage)
13. **β.** Μπορεί να χειριστεί μη ελλειπτικά σχήματα.
14. **γ.** Είναι ένα σημείο που έχει τουλάχιστον έναν καθορισμένο αριθμό σημείων (MinPts) εντός μιας καθορισμένης ακτίνας (Eps).
15. **γ.** Όταν οι συστάδες έχουν σημαντικά διαφορετικές πυκνότητες ή σε δεδομένα υψηλών διαστάσεων.
16. **γ.** Ο OPTICS παράγει ένα ιεραρχικό αποτέλεσμα συσταδοποίησης για μια μεταβλητή ακτίνα γειτονιάς και δεν αναθέτει άμεσα συμμετοχές σε συστάδες.
17. **β.** Η μικρότερη απόσταση ώστε ένα σημείο q να είναι άμεσα προσβάσιμο από ένα core point p (και όχι μικρότερη από το core distance του p).
18. **β.** Οι κοιλάδες (valleys) στο διάγραμμα αντιπροσωπεύουν πυκνές συστάδες.
19. **β.** Ένα CF (Clustering Feature) tree.
20. **β.** Τα δεδομένα παράγονται από ένα μείγμα πολλών Γκαουσιανών συνιστωσών, όπου κάθε συνιστώσα αντιπροσωπεύει μια ξεχωριστή συστάδα.

