# **Ερωτήσεις Πολλαπλής Επιλογής**

1.  **Σύμφωνα με το κίνητρο (Motivation) που παρουσιάζεται στο slide 2, πότε ο όρος θορύβου (noise term) τείνει να κυριαρχεί έναντι του "πληροφοριακού όρου" (informative term) στην αναμενόμενη απόσταση μεταξύ σημείων από d-διάστατες Γκαουσιανές κατανομές;**
    *   α. Όταν η διάσταση d είναι μικρή.
    *   β. Όταν η διάσταση d είναι μεγάλη.
    *   γ. Όταν οι μέσες τιμές (μ1, μ2) είναι πολύ κοντά.
    *   δ. Όταν οι διασπορές (σ1², σ2²) είναι πολύ μικρές.

2.  **Ποιο από τα παρακάτω ΔΕΝ αποτελεί χαρακτηριστικό (feature) του "wine dataset" όπως περιγράφεται στο slide 3;**
    *   α. Alcohol & malic\_acid
    *   β. Temperature & humidity
    *   γ. Flavonoids and nonflavanoid\_phenols
    *   δ. color\_intensity & hue

3.  **Ποιες μετρικές αξιολόγησης ΔΕΝ χρησιμοποιούνται για την "Clustering Performance" στο slide 19;**
    *   α. Silhouette Score
    *   β. Accuracy Score (από ταξινομητή)
    *   γ. Calinski-Harabasz (CH) Index
    *   δ. Davies-Bouldin (DB) Index

4.  **Στην Ανάλυση Κύριων Συνιστωσών (PCA), τι αντιπροσωπεύει η "πρώτη κύρια συνιστώσα" (first principal component) (slide 22);**
    *   α. Τον άξονα με τη μικρότερη διακύμανση στα δεδομένα.
    *   β. Έναν τυχαία επιλεγμένο άξονα.
    *   γ. Τον άξονα της μεγαλύτερης διακύμανσης στα δεδομένα.
    *   δ. Τον μέσο όρο όλων των χαρακτηριστικών.

5.  **Πώς η PCA μειώνει τη διαστατικότητα των δεδομένων (slide 23);**
    *   α. Αφαιρώντας τυχαία χαρακτηριστικά.
    *   β. Προβάλλοντας τα δεδομένα πάνω στις κύριες συνιστώσες που εξηγούν τη μεγαλύτερη διακύμανση.
    *   γ. Ομαδοποιώντας τα παρόμοια χαρακτηριστικά.
    *   δ. Εφαρμόζοντας μια μη γραμμική συνάρτηση.

6.  **Ποιος είναι ο κύριος στόχος της Γραμμικής Διακριτικής Ανάλυσης (LDA) (slide 28, 30);**
    *   α. Να μεγιστοποιήσει τη διακύμανση εντός των κλάσεων.
    *   β. Να βρει έναν νέο άξονα που μεγιστοποιεί τον διαχωρισμό μεταξύ των κλάσεων.
    *   γ. Να εξηγήσει τη δομή συνδιακύμανσης των δεδομένων.
    *   δ. Να διατηρήσει την τοπική δομή των δεδομένων.

7.  **Ποια είναι η κύρια διαφορά μεταξύ PCA και LDA (slide 30, 32);**
    *   α. Η PCA είναι επιβλεπόμενη (supervised), ενώ η LDA είναι μη επιβλεπόμενη (unsupervised).
    *   β. Η PCA εστιάζει στη μεγιστοποίηση της συνολικής διακύμανσης, ενώ η LDA εστιάζει στη μεγιστοποίηση του διαχωρισμού μεταξύ των κλάσεων.
    *   γ. Η LDA χρησιμοποιείται μόνο για παλινδρόμηση, ενώ η PCA για ταξινόμηση.
    *   δ. Η PCA είναι μια μη γραμμική τεχνική, ενώ η LDA είναι γραμμική.

8.  **Ποιος είναι ο σκοπός της Ανάλυσης Παραγόντων (Factor Analysis - FA) (slide 32);**
    *   α. Να μεγιστοποιήσει την ακρίβεια ταξινόμησης.
    *   β. Να αναγνωρίσει και να μοντελοποιήσει λανθάνουσες μεταβλητές (παράγοντες) που εξηγούν τις παρατηρούμενες συσχετίσεις μεταξύ ενός συνόλου μεταβλητών.
    *   γ. Να προβάλλει τα δεδομένα σε έναν χώρο χαμηλότερης διάστασης διατηρώντας τις Ευκλείδειες αποστάσεις.
    *   δ. Να αφαιρεί αναδρομικά τα λιγότερο σημαντικά χαρακτηριστικά.

9.  **Ποια τεχνική μείωσης διαστατικότητας είναι σχεδιασμένη να οπτικοποιεί δεδομένα υψηλών διαστάσεων σε 2D ή 3D διατηρώντας την τοπική δομή των δεδομένων (π.χ. συστάδες) (slide 36);**
    *   α. Principal Component Analysis (PCA)
    *   β. t-Distributed Stochastic Neighbor Embedding (t-SNE)
    *   γ. Linear Discriminant Analysis (LDA)
    *   δ. Factor Analysis

10. **Ποια είναι μια βασική διαφορά μεταξύ t-SNE και UMAP (slide 39);**
    *   α. Το t-SNE είναι γραμμικό, ενώ το UMAP είναι μη γραμμικό.
    *   β. Το UMAP είναι συχνά ταχύτερο και χειρίζεται καλύτερα μεγάλα σύνολα δεδομένων, διατηρώντας παράλληλα και κάποια παγκόσμια δομή.
    *   γ. Το t-SNE προσφέρει πιο ερμηνεύσιμες παραμέτρους από το UMAP.
    *   δ. Το UMAP εστιάζει αποκλειστικά στην παγκόσμια δομή.

11. **Ποια τεχνική μη γραμμικής μείωσης διαστατικότητας διατηρεί την παγκόσμια γεωμετρία των δεδομένων καταγράφοντας γεωδαισιακές αποστάσεις (geodesic distances) (slide 42);**
    *   α. t-SNE
    *   β. UMAP
    *   γ. Isomap
    *   δ. Locally Linear Embedding (LLE)

12. **Ποια είναι η κύρια υπόθεση της Locally Linear Embedding (LLE) (slide 45);**
    *   α. Τα δεδομένα βρίσκονται σε μια γραμμική πολλαπλότητα (manifold).
    *   β. Κάθε σημείο δεδομένων και οι γείτονές του βρίσκονται πάνω ή κοντά σε μια τοπικά γραμμική "φέτα" της υποκείμενης πολλαπλότητας.
    *   γ. Οι παγκόσμιες αποστάσεις είναι πιο σημαντικές από τις τοπικές.
    *   δ. Η τεχνική δεν κάνει υποθέσεις για τη δομή των δεδομένων.

13. **Ποιος είναι ο πρωταρχικός στόχος της Multidimensional Scaling (MDS) (slide 49);**
    *   α. Να μεγιστοποιήσει τη διακύμανση μεταξύ των κλάσεων.
    *   β. Να βρει μια αναπαράσταση χαμηλής διάστασης των δεδομένων που διατηρεί τις αποστάσεις μεταξύ των ζευγών σημείων όσο το δυνατόν περισσότερο.
    *   γ. Να αναγνωρίσει λανθάνοντες παράγοντες.
    *   δ. Να διατηρήσει μόνο την τοπική γειτνίαση.

14. **Πώς λειτουργεί η Recursive Feature Elimination (RFE) (slide 53);**
    *   α. Συνδυάζοντας χαρακτηριστικά για να δημιουργήσει νέα, πιο ισχυρά.
    *   β. Αναγνωρίζοντας τα πιο σημαντικά χαρακτηριστικά αφαιρώντας αναδρομικά τα λιγότερο σημαντικά, βασιζόμενη σε ένα μοντέλο.
    *   γ. Προβάλλοντας τα δεδομένα σε έναν χώρο χαμηλότερης διάστασης.
    *   δ. Ομαδοποιώντας τα χαρακτηριστικά με βάση τη συνδιακύμανσή τους.

15. **Ποιο από τα παρακάτω ΔΕΝ αποτελεί τυπικό όφελος της μείωσης διαστατικότητας (slide 56);**
    *   α. Απλοποιεί τα σύνολα δεδομένων, καθιστώντας τα ευκολότερα στην οπτικοποίηση και ανάλυση.
    *   β. Μειώνει το υπολογιστικό κόστος και τους χρόνους εκπαίδευσης.
    *   γ. Αυξάνει πάντα την ακρίβεια ταξινόμησης.
    *   δ. Βοηθά στον μετριασμό της υπερπροσαρμογής (overfitting) αφαιρώντας πλεονάζοντα ή άσχετα χαρακτηριστικά.

16. **Ποια από τις παρακάτω τεχνικές είναι μέθοδος επιλογής χαρακτηριστικών (Feature Selection) και όχι δημιουργίας νέων χαρακτηριστικών (Feature Extraction) (slide 18);**
    *   α. Principal Component Analysis (PCA)
    *   β. t-Distributed Stochastic Neighbor Embedding (t-SNE)
    *   γ. Recursive Feature Elimination (RFE)
    *   δ. Linear Discriminant Analysis (LDA)

17. **Σύμφωνα με το slide 19, ποια τιμή του Silhouette Score υποδηλώνει καλή συσταδοποίηση (well-clustered);**
    *   α. Τιμή κοντά στο -1.
    *   β. Τιμή κοντά στο 0.
    *   γ. Τιμή κοντά στο 1.
    *   δ. Μια πολύ μεγάλη θετική τιμή.

18. **Για τον Davies-Bouldin (DB) Index που χρησιμοποιείται στην αξιολόγηση συσταδοποίησης, τι υποδηλώνουν οι χαμηλότερες τιμές (slide 19);**
    *   α. Καλύτερο διαχωρισμό και συμπαγείς συστάδες.
    *   β. Χειρότερο διαχωρισμό και αραιές συστάδες.
    *   γ. Μεγάλο αριθμό συστάδων.
    *   δ. Μικρό αριθμό συστάδων.

19. **Ποια από τις παρακάτω τεχνικές μείωσης διαστατικότητας είναι γραμμική (Linear Technique) (slide 18);**
    *   α. t-SNE
    *   β. UMAP
    *   γ. Factor Analysis
    *   δ. Isomap

20. **Σύμφωνα με τα συμπεράσματα (slide 57), ποιος παράγοντας ΔΕΝ αναφέρεται ως κριτήριο για την επιλογή της μεθόδου μείωσης διαστατικότητας;**
    *   α. Η πολυπλοκότητα του προβλήματος.
    *   β. Το μέγεθος του συνόλου δεδομένων.
    *   γ. Η ανάγκη για ερμηνευσιμότητα.
    *   δ. Ο προγραμματιστικός χρόνος υλοποίησης της μεθόδου.

---

**Απαντήσεις**

1.  **β.** Όταν η διάσταση d είναι μεγάλη.
2.  **β.** Temperature & humidity
3.  **β.** Accuracy Score (από ταξινομητή) (Αυτή χρησιμοποιείται για Classification Performance)
4.  **γ.** Τον άξονα της μεγαλύτερης διακύμανσης στα δεδομένα.
5.  **β.** Προβάλλοντας τα δεδομένα πάνω στις κύριες συνιστώσες που εξηγούν τη μεγαλύτερη διακύμανση.
6.  **β.** Να βρει έναν νέο άξονα που μεγιστοποιεί τον διαχωρισμό μεταξύ των κλάσεων.
7.  **β.** Η PCA εστιάζει στη μεγιστοποίηση της συνολικής διακύμανσης, ενώ η LDA εστιάζει στη μεγιστοποίηση του διαχωρισμού μεταξύ των κλάσεων.
8.  **β.** Να αναγνωρίσει και να μοντελοποιήσει λανθάνουσες μεταβλητές (παράγοντες) που εξηγούν τις παρατηρούμενες συσχετίσεις μεταξύ ενός συνόλου μεταβλητών.
9.  **β.** t-Distributed Stochastic Neighbor Embedding (t-SNE)
10. **β.** Το UMAP είναι συχνά ταχύτερο και χειρίζεται καλύτερα μεγάλα σύνολα δεδομένων, διατηρώντας παράλληλα και κάποια παγκόσμια δομή.
11. **γ.** Isomap
12. **β.** Κάθε σημείο δεδομένων και οι γείτονές του βρίσκονται πάνω ή κοντά σε μια τοπικά γραμμική "φέτα" της υποκείμενης πολλαπλότητας.
13. **β.** Να βρει μια αναπαράσταση χαμηλής διάστασης των δεδομένων που διατηρεί τις αποστάσεις μεταξύ των ζευγών σημείων όσο το δυνατόν περισσότερο.
14. **β.** Αναγνωρίζοντας τα πιο σημαντικά χαρακτηριστικά αφαιρώντας αναδρομικά τα λιγότερο σημαντικά, βασιζόμενη σε ένα μοντέλο.
15. **γ.** Αυξάνει πάντα την ακρίβεια ταξινόμησης. (Δεν είναι εγγυημένο, μπορεί και να τη μειώσει αν χαθεί σημαντική πληροφορία)
16. **γ.** Recursive Feature Elimination (RFE)
17. **γ.** Τιμή κοντά στο 1.
18. **α.** Καλύτερο διαχωρισμό και συμπαγείς συστάδες.
19. **γ.** Factor Analysis
20. **δ.** Ο προγραμματιστικός χρόνος υλοποίησης της μεθόδου.

