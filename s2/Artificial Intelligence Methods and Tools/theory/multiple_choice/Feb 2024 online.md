Ας δούμε τις απαντήσεις στις ερωτήσεις του διαγωνίσματος Μηχανικής Μάθησης:

**ΕΝΟΤΗΤΑ 1: Σωστό/λάθος. [2 μονάδες]**

*   **Ε1:** α. Σωστό
    *   *Εξήγηση: Ο k-means είναι ευαίσθητος στην αρχικοποίηση των κέντρων και συχνά συγκλίνει σε τοπικό και όχι ολικό βέλτιστο.*
*   **Ε2:** α. Σωστό
    *   *Εξήγηση: Η αύξηση του βάθους επιτρέπει στο δέντρο να προσαρμοστεί καλύτερα στα δεδομένα εκπαίδευσης, μειώνοντας (ή στη χειρότερη περίπτωση διατηρώντας ίδιο) το σφάλμα εκπαίδευσης. Δεν μπορεί να το αυξήσει.*
*   **Ε3:** β. Λάθος
    *   *Εξήγηση: Σε μη παραμετρικά μοντέλα (π.χ. k-NN, Decision Trees χωρίς περιορισμό βάθους), ο αριθμός των "παραμέτρων" ή η πολυπλοκότητα του μοντέλου αυξάνεται με το πλήθος των δεδομένων εκπαίδευσης.*
*   **Ε4:** β. Λάθος
    *   *Εξήγηση: Η καλή απόδοση μόνο στο train set μπορεί να υποδεικνύει υπερπροσαρμογή (overfitting). Ένα καλό μοντέλο πρέπει να γενικεύει καλά σε νέα, αόρατα δεδομένα (test set).*
*   **Ε5:** β. Λάθος
    *   *Εξήγηση: Ο ταξινομητής Β (80% train, 75% test) είναι προτιμότερος γιατί γενικεύει καλύτερα. Ο Α (100% train, 70% test) δείχνει έντονη υπερπροσαρμογή.*
*   **Ε6:** α. Σωστό
    *   *Εξήγηση: Το linear SVM στοχεύει στην εύρεση του υπερεπιπέδου μεγίστου περιθωρίου, το οποίο συχνά οδηγεί σε καλύτερη γενίκευση σε γραμμικά διαχωρίσιμα σύνολα δεδομένων σε σχέση με το Perceptron που βρίσκει απλώς ένα οποιοδήποτε διαχωριστικό υπερεπίπεδο.*
*   **Ε7:** α. Σωστό
    *   *Εξήγηση: Η μείωση διαστατικότητας είναι μια κοινή τεχνική προεπεξεργασίας που μπορεί να βελτιώσει την απόδοση, να μειώσει τον υπολογιστικό χρόνο και να αντιμετωπίσει την "κατάρα της διαστατικότητας" για αλγορίθμους όπως kNN, Decision Trees, και Naive Bayes.*
*   **Ε8:** α. Σωστό
    *   *Εξήγηση: Όσο αυξάνεται το βάθος, το δέντρο μπορεί να μοντελοποιήσει πιο πολύπλοκες σχέσεις στα δεδομένα εκπαίδευσης, οδηγώντας σε υψηλότερη ακρίβεια στο train set (μέχρι το σημείο της τέλειας ταξινόμησης ή της αδυναμίας περαιτέρω διαχωρισμού).*
*   **Ε9:** α. Σωστό
    *   *Εξήγηση: Ο Naïve Bayes βασίζεται στην υπόθεση της υπό συνθήκη ανεξαρτησίας των χαρακτηριστικών. Αν αυτή η υπόθεση παραβιάζεται (τα χαρακτηριστικά *δεν* είναι ανεξάρτητα), η Logistic Regression, που δεν κάνει αυτή την υπόθεση, είναι συχνά προτιμότερη.*

**ΕΝΟΤΗΤΑ 2: Μονή απάντηση ανά ερώτημα. [4 μονάδες]**

*   **Ε10:** β. Μη γραμμικών (non linear)
    *   *Εξήγηση: Τα Regression Trees μπορούν να μοντελοποιήσουν μη γραμμικές σχέσεις στα δεδομένα χωρίζοντας τον χώρο των χαρακτηριστικών σε περιοχές.*
*   **Ε11:** β. Stratification
    *   *Εξήγηση: Η στρωματοποίηση (stratification) εξασφαλίζει ότι η αναλογία των κλάσεων διατηρείται τόσο στο σύνολο εκπαίδευσης όσο και στο σύνολο δοκιμής.*
*   **Ε12:** β. Logistic regression
    *   *Εξήγηση: Η Logistic Regression μοντελοποιεί την πιθανότητα P(Y=1|X) χρησιμοποιώντας τη σιγμοειδή συνάρτηση, δίνοντας έτσι μια υπό συνθήκη πιθανότητα.*
*   **Ε13:** β. Αλλαγή του αλγορίθμου βελτιστοποίησης των παραμέτρων
    *   *Εξήγηση: Η αύξηση δεδομένων, η μείωση πολυπλοκότητας και ο περιορισμός θορύβου είναι τεχνικές αντιμετώπισης του overfitting. Η αλλαγή του αλγορίθμου βελτιστοποίησης (π.χ. από SGD σε Adam) αφορά κυρίως την ταχύτητα και την ποιότητα της σύγκλισης, όχι άμεσα τη μείωση του overfitting.*
*   **Ε14:** δ. Ο τρόπος χρήσης εξαρτάται από το εκάστοτε πρόβλημα
    *   *Εξήγηση: Δεν υπάρχει μία μοναδική προσέγγιση για τις αποκλίνουσες τιμές. Η διαχείρισή τους (αφαίρεση, μετασχηματισμός, ειδική μοντελοποίηση) εξαρτάται από τη φύση του προβλήματος και των δεδομένων.*
*   **Ε15:** α. Τα βάρη (weights) ρυθμίζονται με τη νόρμα l1.
    *   *Εξήγηση: Η τεχνική Lasso (Least Absolute Shrinkage and Selection Operator) προσθέτει έναν όρο ποινής L1 στη συνάρτηση κόστους της γραμμικής παλινδρόμησης.*
*   **Ε16:** β. Hard-margin SVM (linear kernel)
    *   *Εξήγηση: Το Hard-margin SVM απαιτεί τα δεδομένα να είναι γραμμικά διαχωρίσιμα για να βρει ένα διαχωριστικό υπερεπίπεδο χωρίς σφάλματα ταξινόμησης. Οι άλλες τεχνικές μπορούν να λειτουργήσουν και σε μη γραμμικά διαχωρίσιμα προβλήματα (η Logistic Regression θα δώσει πιθανότητες, το Soft-margin SVM επιτρέπει σφάλματα).*
*   **Ε17:** β. Ο(η)
    *   *Εξήγηση: Για την ταξινόμηση ενός νέου σημείου, ο βασικός αλγόριθμος k-NN (χωρίς βελτιστοποιήσεις όπως KD-trees) υπολογίζει την απόσταση προς όλα τα n σημεία του συνόλου εκπαίδευσης (O(n)) και μετά βρίσκει τους k πλησιέστερους (π.χ. με μερική ταξινόμηση ή σωρό, που μπορεί να γίνει σε O(n) ή O(n log k)). Συνολικά, κυριαρχεί το O(n).*

**ΕΝΟΤΗΤΑ 3: Πολλαπλές απαντήσεις ανά ερώτημα. [5 μονάδες]**

*   **Ε18:** β. Linear Regression, γ. Naïve Bayes
    *   *Εξήγηση: Parametric models έχουν σταθερό αριθμό παραμέτρων ανεξάρτητα από το μέγεθος του training set. KNN είναι non-parametric. SVMs με non-linear kernels (π.χ. RBF) είναι non-parametric καθώς ο αριθμός των support vectors μπορεί να αυξηθεί με τα δεδομένα. Linear SVM είναι parametric. Ο Naive Bayes έχει παραμέτρους τις πιθανότητες που εκτιμώνται.*
*   **Ε19:** α. Normalization, β. Outliers' removal, γ. Class-imbalance handling, δ. Dimensionality reduction
    *   *Εξήγηση: Όλες οι αναφερόμενες είναι τυπικές διαδικασίες προεπεξεργασίας δεδομένων.*
*   **Ε20:** α. DBSCAN, β. OPTICS
    *   *Εξήγηση: DBSCAN και OPTICS είναι αλγόριθμοι συσταδοποίησης βασισμένοι στην πυκνότητα. Minibatch k-means είναι centroid-based. Spectral clustering είναι graph-based.*
*   **Ε21:** β. RMSE, γ. ΜΑΕ
    *   *Εξήγηση: RMSE (Root Mean Squared Error) και MAE (Mean Absolute Error) είναι μετρικές αξιολόγησης για προβλήματα παλινδρόμησης. F1-score και AUC-ROC είναι για προβλήματα ταξινόμησης.*
*   **Ε22:** α. Δέντρα απόφασης, β. 15-πλησιέστεροι γείτονες, γ. SVM (Hard-margin), δ. Νευρωνικό δίκτυο (2 hidden layers)
    *   *Εξήγηση: Όλες αυτές οι τεχνικές έχουν τη δυνατότητα (με κατάλληλες παραμέτρους/δομή) να επιτύχουν μηδενικό σφάλμα στο training set αν αυτό είναι γραμμικά διαχωρίσιμο. Τα δέντρα αν μεγαλώσουν αρκετά, το k-NN (ειδικά για k=1), το Hard-margin SVM εξ ορισμού, και ένα αρκετά εκφραστικό νευρωνικό δίκτυο.*
