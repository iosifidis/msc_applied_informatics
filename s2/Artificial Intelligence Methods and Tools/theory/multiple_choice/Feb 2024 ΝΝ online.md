Εντάξει, ας δούμε και τις απαντήσεις για αυτό το διαγώνισμα Νευρωνικών Δικτύων:

**ΕΝΟΤΗΤΑ 1: Σωστό/λάθος. [1.5 μονάδες]**

*   **Ε1:** α. Σωστό
    *   *Εξήγηση: Οι σιγμοειδείς συναρτήσεις είναι μη-γραμμικές και πολλαπλά κρυμμένα επίπεδα επιτρέπουν στο δίκτυο να μάθει πολύπλοκες, μη-γραμμικές επιφάνειες απόφασης.*
*   **Ε2:** β. Λάθος
    *   *Εξήγηση: Η αρχικοποίηση όλων των βαρών με μηδέν θα είχε ως αποτέλεσμα όλοι οι νευρώνες σε ένα επίπεδο να υπολογίζουν την ίδια έξοδο και να λαμβάνουν την ίδια κλίση κατά την οπισθοδιάδοση. Αυτό εμποδίζει το δίκτυο να μάθει αποτελεσματικά (πρόβλημα συμμετρίας).*
*   **Ε3:** α. Σωστό
    *   *Εξήγηση: Ο αλγόριθμος backpropagation χρησιμοποιεί τον κανόνα της αλυσίδας για τον υπολογισμό των παραγώγων της συνάρτησης απώλειας ως προς τα βάρη. Η αποδοτική, βήμα προς βήμα (επίπεδο προς επίπεδο) φύση του υπολογισμού, αποφεύγοντας επαναλαμβανόμενους υπολογισμούς, έχει χαρακτηριστικά δυναμικού προγραμματισμού.*

**ΕΝΟΤΗΤΑ 2: Μονή απάντηση ανά ερώτημα. [4,5 μονάδες]**

*   **Ε4:** γ. εισαγωγή της μη γραμμικότητας
    *   *Εξήγηση: Ο κύριος σκοπός των συναρτήσεων ενεργοποίησης είναι να εισάγουν μη-γραμμικότητα στο μοντέλο, επιτρέποντας στα νευρωνικά δίκτυα να μαθαίνουν πολύπλοκες σχέσεις στα δεδομένα που δεν θα μπορούσαν να μοντελοποιηθούν μόνο με γραμμικούς μετασχηματισμούς.*
*   **Ε5:** β. Tanh
    *   *Εξήγηση: Η Tanh (όπως και η σιγμοειδής) κορέννυται (οι παράγωγοί της πλησιάζουν το μηδέν) για μεγάλες θετικές ή αρνητικές τιμές εισόδου. Αυτό μπορεί να οδηγήσει σε πολύ μικρές κλίσεις κατά την οπισθοδιάδοση, επιβραδύνοντας ή σταματώντας τη μάθηση (vanishing gradient).*
*   **Ε6:** β. Να ενημερώσει τα συναπτικά βάρη (weights)
    *   *Εξήγηση: Ο αλγόριθμος backpropagation υπολογίζει τις κλίσεις (gradients) της συνάρτησης απώλειας ως προς τα βάρη του δικτύου. Αυτές οι κλίσεις χρησιμοποιούνται στη συνέχεια από έναν αλγόριθμο βελτιστοποίησης (π.χ. SGD) για την ενημέρωση των βαρών.*
*   **Ε7:** β. Αναδρομικά Νευρωνικά Δίκτυα (RNN)
    *   *Εξήγηση: Τα Αναδρομικά Νευρωνικά Δίκτυα (RNNs) σχεδιάστηκαν ειδικά για την επεξεργασία ακολουθιακών δεδομένων, καθώς διαθέτουν βρόχους που επιτρέπουν τη διατήρηση πληροφορίας από προηγούμενα βήματα της ακολουθίας.*
*   **Ε8:** β. Autoencoders
    *   *Εξήγηση: Οι Autoencoders είναι ένας τύπος νευρωνικού δικτύου που εκπαιδεύεται να αναπαράγει την είσοδό του στην έξοδο, μαθαίνοντας έτσι μια συμπιεσμένη αναπαράσταση (κωδικοποίηση) των δεδομένων στο ενδιάμεσο κρυφό επίπεδο, συχνά από μη τιτλοφορημένα δεδομένα.*
*   **Ε9:** α. 238
    *   *Εξήγηση: Βαθμωτό γινόμενο εισόδων και βαρών: (4*1) + (10*2) + (5*3) + (20*4) = 4 + 20 + 15 + 80 = 119. Έξοδος γραμμικής συνάρτησης μεταφοράς με κλίση 2: 2 * 119 = 238.*
*   **Ε10:** β. Dropout layers
    *   *Εξήγηση: Η μεγάλη απόκλιση μεταξύ accuracy στο train set (100%) και στο test set (42%) υποδεικνύει έντονη υπερπροσαρμογή (overfitting). Το Dropout είναι μια τεχνική κανονικοποίησης που βοηθά στη μείωση του overfitting απενεργοποιώντας τυχαία νευρώνες κατά την εκπαίδευση.*

**Σελίδα 2**

*   **Ε11:** γ. Κατά την εκπαίδευση, το σφάλμα (loss) για τον discriminator συγκλίνει πάντα στο ίδιο νούμερο
    *   *Εξήγηση: Σε ένα GAN, οι συναρτήσεις απώλειας του generator και του discriminator ανταγωνίζονται. Το ιδανικό είναι να φτάσουν σε μια ισορροπία, αλλά οι τιμές τους κυμαίνονται κατά τη διάρκεια της εκπαίδευσης και δεν συγκλίνουν πάντα σε ένα σταθερό, ίδιο νούμερο για τον discriminator.*
*   **Ε12:** δ. εκτελεί ένα μη-γραμμικό μετασχηματισμό πάνω στις activations σε ένα επιπεδο (layer)
    *   *Εξήγηση: Το Batch Normalization κανονικοποιεί τις ενεργοποιήσεις ενός επιπέδου (τις κάνει να έχουν μέση τιμή 0 και διασπορά 1) και στη συνέχεια τις κλιμακώνει και τις μετατοπίζει με δύο παραμέτρους (γάμμα και βήτα) που μαθαίνονται. Αυτός ο μετασχηματισμός ( y = γ * (x-μ)/σ + β ) είναι ένας αφινικός (γραμμικός) μετασχηματισμός πάνω στις κανονικοποιημένες ενεργοποιήσεις. Η επιλογή (β) είναι επίσης σωστή (επιταχύνει την εκπαίδευση), αλλά η (δ) περιγράφει καλύτερα τη λειτουργία. Ωστόσο, υπάρχει μια λεπτή διάκριση: ο βασικός μετασχηματισμός κανονικοποίησης και μετατόπισης/κλιμάκωσης είναι γραμμικός. Αν η ερώτηση υπονοεί την επίδραση στο συνολικό δίκτυο, το batchnorm βοηθά στη σταθεροποίηση της εκπαίδευσης μη-γραμμικών μοντέλων. Η πιο ακριβής απάντηση για το τι *κάνει* το ίδιο το batchnorm layer είναι ένας μετασχηματισμός που περιλαμβάνει κανονικοποίηση και έναν επακόλουθο γραμμικό μετασχηματισμό (κλιμάκωση και μετατόπιση). Αν η επιλογή (β) "επιταχύνει την εκπαίδευση σε deep networks" θεωρείται πιο γενικά αληθής ως αποτέλεσμα, τότε αυτή θα μπορούσε να είναι η επιδιωκόμενη. Δεδομένου ότι είναι μονή απάντηση, η (β) είναι ένα πολύ γνωστό όφελος. Η (δ) είναι πιο λεπτομερής για τη λειτουργία αλλά η περιγραφή "μη-γραμμικό" είναι ανακριβής για τον ίδιο τον μετασχηματισμό του batchnorm.
    *   *Διόρθωση/Επαναξιολόγηση Ε12:* Ο μετασχηματισμός `y = γ * (x_norm) + β` είναι γραμμικός (αφινικός) ως προς το `x_norm`. Το Batch Normalization *εφαρμόζεται* σε ενεργοποιήσεις που μπορεί να προέρχονται από μη γραμμικές συναρτήσεις, και *βοηθά* στην εκπαίδευση βαθιών δικτύων. Η πιο αναμφισβήτητα αληθής και κύρια συνεισφορά του είναι η **β. επιταχύνει την εκπαίδευση σε deep networks.** Η (δ) είναι λανθασμένη στο "μη-γραμμικό".*
*   **Ε13:** γ. Μείωση μέτρου, διατήρηση προσήμου.
    *   *Εξήγηση: Η παράγωγος της σιγμοειδούς συνάρτησης, σ(x)(1-σ(x)), έχει μέγιστη τιμή 0.25. Όταν η κλίση διαδίδεται προς τα πίσω μέσω μιας σιγμοειδούς, πολλαπλασιάζεται με αυτή την παράγωγο, η οποία είναι πάντα μεταξύ 0 και 0.25. Αυτό μειώνει το μέτρο της κλίσης και διατηρεί το πρόσημό της (καθώς η παράγωγος είναι πάντα μη αρνητική).*
*   **Ε14:** α. f (x) = − min(2, x), γ. f(x) = softmax(x)
    *   *Εξήγηση:
        *   α. f(x) = -min(2,x): Είναι μη-γραμμική. Για x < 2, f(x) = -x. Για x ≥ 2, f(x) = -2. Έχει μια "γωνία".
        *   β. f(x) = 0.9x + 1: Είναι γραμμική (αφινική).
        *   γ. f(x) = softmax(x): Είναι μη-γραμμική και χρησιμοποιείται συνήθως στο επίπεδο εξόδου για προβλήματα πολυκατηγορικής ταξινόμησης.
        *   δ. καμία από τις παραπάνω.*
*   **Ε15:** α. Μεγαλύτερους χρόνους εκπαίδευσης
    *   *Εξήγηση: Η προσθήκη κρυμμένων επιπέδων αυξάνει τον αριθμό των παραμέτρων και των υπολογισμών που απαιτούνται τόσο για την εμπρόσθια διάδοση (forward pass) όσο και για την οπισθοδιάδοση (backward pass), οδηγώντας σχεδόν πάντα σε μεγαλύτερους χρόνους εκπαίδευσης. Οι άλλες επιλογές (χαμηλότερο σφάλμα, καλύτερη γενίκευση) είναι επιθυμητά αποτελέσματα αλλά όχι εγγυημένα και εξαρτώνται από πολλούς παράγοντες.*
*   **Ε16:** α. Data augmentation, β. Dropout, γ. Batch Normalization
    *   *Εξήγηση:
        *   α. Data augmentation: Αυξάνει τεχνητά το μέγεθος και την ποικιλομορφία του συνόλου εκπαίδευσης, βοηθώντας το μοντέλο να γενικεύει καλύτερα.
        *   β. Dropout: Τεχνική κανονικοποίησης που μειώνει την εξάρτηση από συγκεκριμένους νευρώνες.
        *   γ. Batch Normalization: Αν και ο κύριος σκοπός του είναι η σταθεροποίηση και επιτάχυνση της εκπαίδευσης, έχει και μια ελαφρά κανονικοποιητική επίδραση.
        *   δ. Η χρήση Adam αντί για SGD αφορά την βελτιστοποίηση και όχι άμεσα την αντιμετώπιση του overfitting, αν και μπορεί έμμεσα να βοηθήσει στην εύρεση καλύτερων σημείων ελαχίστου.*
*   **Ε17:** β. δεν χρειάζεται κάποιος ειδικός για την δημιουργία feature values
    *   *Εξήγηση: Η εκπαίδευση end-to-end επιτρέπει στο μοντέλο να μάθει τις κατάλληλες αναπαραστάσεις (features) απευθείας από τα ακατέργαστα δεδομένα, μειώνοντας ή εξαλείφοντας την ανάγκη για χειροκίνητη εξαγωγή χαρακτηριστικών από ειδικούς.*
*   **Ε18:** α. Ο generator παράγει μόνο εικόνες Bulbasaur ή Charmander ή Squirtle
    *   *Εξήγηση: Το "mode collapse" συμβαίνει όταν ο generator παράγει ένα πολύ περιορισμένο εύρος δειγμάτων (π.χ., μόνο λίγους τύπους εικόνων) αντί να μάθει την πλήρη κατανομή των πραγματικών δεδομένων. Η παρατήρηση ότι ο generator παράγει μόνο συγκεκριμένους τύπους εικόνων είναι η πιο άμεση διαπίστωση του mode collapse μέσω οπτικοποιήσεων. Οι άλλες επιλογές (μεταβολές στα losses, accuracies) μπορεί να είναι συμπτώματα ή να σχετίζονται, αλλά η (α) είναι η οπτική επιβεβαίωση.*
