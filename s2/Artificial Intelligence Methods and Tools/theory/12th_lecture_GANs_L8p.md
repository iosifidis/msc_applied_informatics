# **Παραγωγικά Ανταγωνιστικά Δίκτυα (Generative Adversarial Networks - GANs)**

### **Εισαγωγή & Διάγραμμα Διάλεξης (Διαφάνειες 1-2)**

*   **Θέμα:** Παραγωγικά Ανταγωνιστικά Δίκτυα (GANs), ένα είδος νευρωνικών δικτύων.
*   **Δομή Διάλεξης:**
    *   **Μέρος 0:** Πολύ γρήγορη εξήγηση.
    *   **Μέρος 1:** Επισκόπηση των GANs.
    *   **Μέρος 2:** Ορισμένες προκλήσεις με τα GANs.
    *   **Μέρος 3:** Εφαρμογές των GANs.

### **Γρήγορη Υπενθύμιση: Τύποι Μηχανικής Μάθησης (Διαφάνεια 3)**

*   **Επιβλεπόμενη Μάθηση (Supervised Learning):**
    *   Δίνονται ανεξάρτητες μεταβλητές **X** και οι αντίστοιχες ετικέτες **Y**.
    *   Στόχος: Να μάθουμε μια συνάρτηση απεικόνισης `f: X → Y` που ελαχιστοποιεί μια προκαθορισμένη συνάρτηση απώλειας.
    *   Εκπαιδεύουμε **διακριτικά μοντέλα (discriminative models)** που στοχεύουν στην εκμάθηση της υπό συνθήκη πιθανότητας `p(Y|X)`.
    *   Παραδείγματα: Ταξινόμηση (classification), παλινδρόμηση (regression).
*   **Μη Επιβλεπόμενη Μάθηση (Unsupervised Learning):**
    *   Δίνονται μόνο οι ανεξάρτητες μεταβλητές **X**.
    *   Στόχος: Να μάθουμε κάποια υποκείμενα μοτίβα των δεδομένων.
    *   Εκπαιδεύουμε **παραγωγικά μοντέλα (generative models)** που στοχεύουν στην καταγραφή της πιθανότητας `p(X)`.
    *   Παραδείγματα: Ομαδοποίηση (clustering), μείωση διαστατικότητας (dimensionality reduction).
    *   **Τα GANs ανήκουν κυρίως σε αυτή την κατηγορία, καθώς μαθαίνουν την κατανομή των δεδομένων για να παράγουν νέα.**

### **Παραγωγικά Μοντέλα (Generative Models) (Διαφάνειες 5-6)**

*   **Γενικός Στόχος:** Ένα παραγωγικό μοντέλο προσπαθεί να μάθει την **υποκείμενη κατανομή των δεδομένων**.
*   **Δυνατότητες:**
    *   Να προβλέψει πόσο πιθανό είναι ένα δεδομένο δείγμα.
    *   Να **παράγει νέα δείγματα** χρησιμοποιώντας την μαθημένη κατανομή δεδομένων.
*   **Δύο Τύποι Παραγωγικών Μοντέλων (Διαφ. 5):**
    1.  **Μοντέλα Ρητής Πυκνότητας (Explicit Density Models):**
        *   Υποθέτουν μια εκ των προτέρων (prior) κατανομή των δεδομένων.
        *   Ορίζουν μια ρητή συνάρτηση πυκνότητας.
        *   Προσπαθούν να μεγιστοποιήσουν την πιθανοφάνεια (likelihood) αυτής της συνάρτησης στα δεδομένα μας.
        *   Παραδείγματα από ταξινόμηση (Διαφ. 6):
            *   **Με Υπολογίσιμη Πυκνότητα (Tractable density):** Π.χ., Fully visible Belief Nets.
            *   **Με Προσεγγιστική Πυκνότητα (Approximate density):**
                *   **Μεταβλητικά (Variational):** Π.χ., Variational Autoencoders (VAEs).
                *   **Αλυσίδες Markov (Markov Chain):** Π.χ., Boltzmann Machines.
    2.  **Μοντέλα Άδηλης Πυκνότητας (Implicit Density Models):**
        *   Ορίζουν μια στοχαστική διαδικασία που παράγει απευθείας δεδομένα.
        *   Δεν ορίζουν ρητά μια συνάρτηση πυκνότητας.
        *   Παραδείγματα από ταξινόμηση (Διαφ. 6):
            *   **Αλυσίδες Markov:** Π.χ., GSN (Generative Stochastic Networks).
            *   **Άμεσα (Direct):** **GANs** (Το σημερινό μας μάθημα).

### **Αρχιτεκτονική των GANs (Διαφάνειες 7-10)**

*   Ένα GAN αποτελείται από **δύο νευρωνικά δίκτυα**:
    1.  **Δίκτυο 1: Γεννήτορας (Generator - G) (Διαφ. 7):**
        *   Λαμβάνει ως είσοδο ένα τυχαίο διάνυσμα σταθερού μήκους **z** (συνήθως από μια απλή κατανομή, π.χ., Γκαουσιανή ή ομοιόμορφη).
        *   Μαθαίνει μια απεικόνιση **G(z)**.
        *   Στόχος: Να παράγει δείγματα που μιμούνται την κατανομή του αρχικού συνόλου δεδομένων (π.χ., να παράγει ρεαλιστικές εικόνες).
    2.  **Πρόσβαση σε Πραγματικά Δεδομένα (Διαφ. 8):** Είναι σημαντικό να κατανοήσουμε ότι έχουμε πρόσβαση σε πραγματικά δεδομένα (π.χ., πραγματικές εικόνες) για την εκπαίδευση.
    3.  **Δίκτυο 2: Διακριτής (Discriminator - D) (Διαφ. 9):**
        *   Λαμβάνει ως είσοδο ένα δείγμα **x** που προέρχεται είτε από το αρχικό σύνολο δεδομένων (πραγματικό) είτε από την έξοδο του Γεννήτορα (ψεύτικο).
        *   Εξάγει ένα μόνο βαθμωτό μέγεθος που αντιπροσωπεύει την **πιθανότητα το x να προήλθε από το αρχικό (πραγματικό) σύνολο δεδομένων**.
*   **Δύο Τύποι Σφαλμάτων (Διαφ. 10):** Πρέπει να παρακολουθούμε ταυτόχρονα δύο τύπους σφαλμάτων:
    1.  **Απώλεια Διακριτή (Discriminator loss):** Πόσο καλά ο Διακριτής διακρίνει τα πραγματικά από τα ψεύτικα.
    2.  **Απώλεια Γεννήτορα (Generator loss):** Πόσο καλά ο Γεννήτορας "ξεγελά" τον Διακριτή.

### **Κατανόηση του Γεννήτορα (Διαφάνειες 11-13)**

*   **Αρχικά (Διαφ. 11):** Ο Γεννήτορας παίρνει ένα τυχαίο διάνυσμα `z` και παράγει μια εικόνα `G(z)`. Επειδή οι παράμετροι είναι τυχαία αρχικοποιημένες, η εικόνα εξόδου δεν μοιάζει καθόλου με τον στόχο (π.χ., το ψηφίο "7").
*   **Κατά την Εκπαίδευση (Διαφ. 12):** Ο Γεννήτορας μαθαίνει να παράγει εικόνες όλο και πιο κοντά στην αρχική κατανομή (που απεικονίζει το "7") για να ξεγελάσει τον Διακριτή. Κάποια στιγμή, παράγει εικόνες πιο παρόμοιες με το "7".
*   **Στο Τέλος (Διαφ. 13):** Η κατανομή εικόνων της εξόδου του Γεννήτορα και η αρχική κατανομή είναι πολύ κοντά μεταξύ τους, και παράγονται συνθετικές εικόνες που απεικονίζουν το "7".

### **Επίσημη Παρουσίαση των GANs (Διαφάνειες 14-22)**

*   **Ποιος είναι Πραγματικός; (Διαφ. 15-19):** Παραδείγματα εικόνων προσώπων, όπου η μία είναι πραγματική και η άλλη παραχθείσα από GAN. Δείχνει την ικανότητα των GANs να παράγουν ρεαλιστικές εικόνες που είναι δύσκολο να διακριθούν από τις πραγματικές.
*   **Κατανόηση Εισόδων (Διαφ. 20):**
    *   Μια εικόνα RGB (π.χ., 100x100 pixels) έχει 3 τιμές (R,G,B) ανά pixel. Συνολικά 100x100x3 = 30000 τιμές pixel.
    *   Αυτή η εικόνα μπορεί να μετατραπεί σε ένα μονοδιάστατο διάνυσμα (flattening).
*   **Το Πρόβλημα της Οπτικοποίησης (Διαφ. 21):**
    *   Κάθε εικόνα είναι ένα σημείο σε έναν διανυσματικό χώρο πολύ υψηλής διάστασης (π.χ., 30000 διαστάσεων).
    *   **Δεν μπορούμε να οπτικοποιήσουμε** αυτόν τον χώρο.
    *   Αυτά τα σημεία (εικόνες) θα πρέπει να σχηματίζουν μια πυκνή περιοχή, η οποία παρέχει πληροφορίες για την κατανομή στον διανυσματικό χώρο.
*   **Η Πρόκληση της Δειγματοληψίας (Διαφ. 22):**
    *   Θέλουμε να δημιουργήσουμε μια εικόνα που δεν υπάρχει στο αρχικό μας σύνολο δεδομένων.
    *   Θεωρητικά, θα μπορούσαμε να κάνουμε τυχαία δειγματοληψία ενός διανυσματικού σημείου από την πιθανοθεωρητική κατανομή που δημιουργείται από το δεδομένο σύνολο.
    *   **Πρόβλημα:** Η δεδομένη κατανομή είναι πολύπλοκη για άμεση δειγματοληψία.
        *   Τεράστιος αριθμός διαστάσεων -> δεν μπορούμε να λάβουμε τη συνάρτηση κατανομής.
        *   Όχι γνωστή συνάρτηση κατανομής -> δεν έχουμε την εξισωτική της μορφή.
    *   **Λύση:** Τεχνικές προσομοίωσης (όπως τα GANs).

### **Μετασχηματισμοί / Η Βασική Ιδέα των GANs (Διαφάνειες 23-27)**

1.  **Αρχικό Σημείο (Διαφ. 23):** Έχουμε πολλά δεδομένα (π.χ., εικόνες) που ακολουθούν μια άγνωστη, πολύπλοκη κατανομή (target distribution).
2.  **Δημιουργία Τυχαίων Τιμών (Διαφ. 24):** Παράγουμε μια τυχαία μεταβλητή από μια απλή, γνωστή κατανομή (π.χ., ομοιόμορφη ή Γκαουσιανή). Αυτές οι τιμές είναι εύκολο να δειγματοληπτηθούν.
3.  **Μετασχηματισμός (Διαφ. 25):** Χρησιμοποιούμε μια **πολύπλοκη συνάρτηση (ένα νευρωνικό δίκτυο - τον Γεννήτορα)** για να μετασχηματίσουμε τις απλές τυχαίες τιμές σε τιμές που ακολουθούν την επιθυμητή πολύπλοκη κατανομή.
4.  **Παραγωγή Νέου Δείγματος (Διαφ. 26):** Κάνοντας δειγματοληψία από την απλή κατανομή και περνώντας την μέσα από τον εκπαιδευμένο Γεννήτορα, παράγουμε ένα νέο δείγμα.
*   **Το Κύριο Πρόβλημα (Διαφ. 27):** Χρειαζόμαστε πολλά δεδομένα για να εκπαιδεύσουμε ένα νευρωνικό δίκτυο (τον Γεννήτορα) ικανό να πάρει τυχαίο θόρυβο ως είσοδο και να δημιουργήσει μια εικόνα ως έξοδο.

### **Πώς Εκπαιδεύουμε Ένα Τέτοιο Δίκτυο; (Παραδοσιακή προσέγγιση, ΟΧΙ GAN) (Διαφάνεια 28)**

*   Αυτή η διαφάνεια περιγράφει μια πιο **παραδοσιακή προσέγγιση για παραγωγικά μοντέλα**, ΟΧΙ τη λογική των GANs.
    1.  Τυχαίες είσοδοι στο δίκτυο, παραγωγή εικόνας.
    2.  Σύγκριση της εξόδου με όλες τις εικόνες εκπαίδευσης για εύρεση της πλησιέστερης (π.χ., με L2 νόρμα).
    3.  Η πλησιέστερη εικόνα εκπαίδευσης ορίζεται ως στόχος `t`.
    4.  Τα βάρη τροποποιούνται με backpropagation βάσει της απώλειας μεταξύ εξόδου και `t`.
*   Αυτό **ΔΕΝ είναι ο τρόπος λειτουργίας των GANs**. Τα GANs χρησιμοποιούν την ανταγωνιστική διαδικασία με τον Διακριτή.

### **GMN: Βήματα Επανάληψης Εκπαίδευσης (Αναφέρεται ως GMN, πιθανόν Generative Matching Network) (Διαφάνεια 29)**

*   Αυτή η διαφάνεια οπτικοποιεί την παραδοσιακή προσέγγιση της διαφάνειας 28. Τα κόκκινα σημεία (παραγόμενα) προσπαθούν να ταιριάξουν τα μπλε σημεία (πραγματικά).

### **Παραγωγική Μοντελοποίηση (Διαφάνεια 30)**

*   Η μοντελοποίηση πολύπλοκων δεδομένων υψηλής διάστασης είναι ανοιχτό πρόβλημα. Τα **βαθιά παραγωγικά μοντέλα** σημειώνουν πρόοδο.
*   **Στόχος:** Μοντελοποίηση `P(x)` (μη επιβλεπόμενη) ή `P(x, y)` (επιβλεπόμενη).
*   Συχνά χρησιμοποιούν λανθάνουσες μεταβλητές `h`, μοντελοποιώντας `P(x, h)` ή `P(x, h, y)`.

### **Εξέλιξη των GANs (Διαφάνεια 31)**

*   **2014:** Γέννηση των GAN [NIPS].
*   **2015:** Laplacian Pyramid GAN [NIPS] (Fermenting - Ζύμωση).
*   **2016:** Άνθηση βελτιώσεων και εφαρμογών: DCGAN, InfoGAN, RNN+GAN, VAE+GAN, CoGAN, AAE, Super-Resolution, Latent-Manipulation, Domain transformation. Υψηλότερη ανάλυση, ευέλικτος χειρισμός.
*   **2017:** Θεωρία: Μειονεκτήματα & Λύσεις. Προβλήματα όπως Mode Missing & Instability.

### **Εφαρμογές των GANs (Διαφάνειες 32-35)**

*   **Συμπλήρωση Εικόνας (Image In-painting) (Διαφ. 32):** Γέμισμα τμημάτων που λείπουν από μια εικόνα. Τα GANs (π.χ., Context Encoder) μπορούν να παράγουν ρεαλιστικά συμπληρώματα.
*   **Επεξεργασία Εικόνας (Image Editing) (Διαφ. 33):** Αλλαγή χαρακτηριστικών σε εικόνες προσώπων (π.χ., προσθήκη γυαλιών, αλλαγή χρώματος μαλλιών, έκφρασης).
*   **Ανάμειξη Εικόνων (Image Blending) (Διαφ. 34):** Ομαλή ενσωμάτωση τμημάτων από διαφορετικές εικόνες.
*   **Εξέλιξη & Παλινδρόμηση Ηλικίας (Age Progression and Regression) (Διαφ. 35):** Προσομοίωση της γήρανσης ή της νεότητας σε πρόσωπα.

### **Λίγη Διαίσθηση (Πίσω στα Autoencoders) (Διαφάνεια 36)**

*   **Autoencoder:**
    *   Ένας **Κωδικοποιητής (Encoder)** μαθαίνει να συμπιέζει μια εικόνα σε έναν λανθάνοντα κώδικα (code / latent vector).
    *   Ένας **Αποκωδικοποιητής (Decoder)** μαθαίνει να ανακατασκευάζει την αρχική εικόνα από τον κώδικα.
    *   Στόχος: Η ανακατασκευασμένη εικόνα να είναι όσο το δυνατόν πιο κοντά στην αρχική.
*   **Ιδέα για Παραγωγή:** Αν μπορούσαμε να παράγουμε τυχαία έναν λανθάνοντα κώδικα που να αντιστοιχεί σε μια "λογική" εικόνα και να τον δώσουμε στον Αποκωδικοποιητή, θα μπορούσαμε να παράγουμε νέες εικόνες;

### **Αρχιτεκτονική GANs (Επίσημα) (Διαφάνεια 37)**

*   **Λανθάνον Τυχαίο Διάνυσμα (Latent random variable z):** Είσοδος τυχαίου θορύβου (π.χ., Γκαουσιανός/Ομοιόμορφος). Μπορεί να θεωρηθεί ως η λανθάνουσα αναπαράσταση της εικόνας.
*   **Γεννήτορας (Generator G):** Ένα νευρωνικό δίκτυο που μετασχηματίζει το `z` σε ένα δείγμα `G(z)` (π.χ., μια εικόνα).
*   **Πραγματικές Εικόνες (Real world images x):** Δείγματα από την πραγματική κατανομή δεδομένων.
*   **Διακριτής (Discriminator D):** Ένα νευρωνικό δίκτυο που παίρνει είτε ένα πραγματικό δείγμα `x` είτε ένα ψεύτικο δείγμα `G(z)` και εξάγει μια πιθανότητα `D(x)` ή `D(G(z))` ότι το δείγμα είναι πραγματικό.
*   **Απώλεια (Loss):** Μετρά πόσο καλά ο Διακριτής ξεχωρίζει τα πραγματικά από τα ψεύτικα, και πόσο καλά ο Γεννήτορας ξεγελά τον Διακριτή.
*   **Τα G και D είναι διαφορίσιμα (differentiable modules).**

### **Διαδικασία Εκπαίδευσης: Βασική Ιδέα (Διαφάνεια 38)**

*   **Παιχνίδι Μηδενικού Αθροίσματος (Minimax Game):**
    *   Ο **Γεννήτορας (G) προσπαθεί να ξεγελάσει τον Διακριτή (D)** (να κάνει τον D να πιστέψει ότι τα ψεύτικα δείγματα είναι πραγματικά).
    *   Ο **Διακριτής (D) προσπαθεί να μην ξεγελαστεί** (να διακρίνει σωστά τα πραγματικά από τα ψεύτικα).
*   Τα μοντέλα εκπαιδεύονται **ταυτόχρονα**:
    *   Καθώς ο G γίνεται καλύτερος, ο D έχει πιο δύσκολο έργο.
    *   Καθώς ο D γίνεται καλύτερος, ο G έχει πιο δύσκολο έργο.
*   **Τελικός Στόχος:** Δεν μας ενδιαφέρει απαραίτητα ο D. Ο ρόλος του είναι να **αναγκάσει τον G να δουλέψει σκληρότερα** και να παράγει καλύτερα (πιο ρεαλιστικά) δείγματα.

### **Εκπαίδευση Διακριτή (Διαφάνεια 39)**

1.  Ο Διακριτής λαμβάνει πραγματικές εικόνες και ψεύτικες εικόνες από τον Γεννήτορα.
2.  Ο Γεννήτορας είναι **"κλειδωμένος" (παγωμένος)** - τα βάρη του δεν ενημερώνονται.
3.  Ο Διακριτής εκπαιδεύεται να εξάγει υψηλή πιθανότητα (π.χ., 1) για τις πραγματικές εικόνες και χαμηλή πιθανότητα (π.χ., 0) για τις ψεύτικες.
4.  Το σφάλμα οπισθοδιαδίδεται (backprop) για να ενημερωθούν τα βάρη του Διακριτή.

### **Εκπαίδευση Γεννήτορα (Διαφάνεια 40)**

1.  Ο Γεννήτορας παράγει ψεύτικες εικόνες.
2.  Ο Διακριτής είναι **"κλειδωμένος" (παγωμένος)** - τα βάρη του δεν ενημερώνονται.
3.  Ο Γεννήτορας εκπαιδεύεται ώστε οι ψεύτικες εικόνες του να ταξινομούνται ως πραγματικές από τον Διακριτή (δηλαδή, ο D να εξάγει υψηλή πιθανότητα).
4.  Το σφάλμα οπισθοδιαδίδεται **μέσω του Διακριτή** (χωρίς να τον ενημερώνει) για να ενημερωθούν τα βάρη του Γεννήτορα.

### **Διατύπωση των GANs (Μαθηματικά) (Διαφάνεια 41)**

*   **Παιχνίδι Minimax:** `min_G max_D V(D, G)`
    *   Ο Διακριτής (D) προσπαθεί να μεγιστοποιήσει την ανταμοιβή του `V(D, G)`.
    *   Ο Γεννήτορας (G) προσπαθεί να ελαχιστοποιήσει την ανταμοιβή του Διακριτή (ή να μεγιστοποιήσει τη δική του απώλεια).
*   **Συνάρτηση Αξίας (Value Function) V(D, G):**
    `V(D, G) = E_{x~p_data(x)}[log D(x)] + E_{z~p_z(z)}[log(1 - D(G(z)))]`
    *   Ο πρώτος όρος (`E_{x~p_data(x)}[log D(x)]`): Ο D θέλει να είναι μεγάλος (κοντά στο 1) για πραγματικά δεδομένα.
    *   Ο δεύτερος όρος (`E_{z~p_z(z)}[log(1 - D(G(z)))]`): Ο D θέλει το `D(G(z))` να είναι μικρό (κοντά στο 0) για ψεύτικα δεδομένα, άρα το `1 - D(G(z))` να είναι μεγάλο.
*   **Ισορροπία Nash (Nash Equilibrium):** Επιτυγχάνεται όταν:
    *   `p_data(x) = p_gen(x)` για όλα τα x (η κατανομή των παραγόμενων δειγμάτων είναι ίδια με την πραγματική).
    *   `D(x) = 1/2` για όλα τα x (ο Διακριτής δεν μπορεί να διακρίνει τα πραγματικά από τα ψεύτικα - πιθανότητα 50%).

### **Αλγόριθμος Εκπαίδευσης (Διαφάνεια 42)**

*   Για έναν αριθμό επαναλήψεων εκπαίδευσης:
    *   **Για k βήματα (Ενημέρωση Διακριτή):**
        1.  Δειγματοληψία mini-batch θορύβου `{z⁽¹⁾, ..., z⁽ᵐ⁾}`.
        2.  Δειγματοληψία mini-batch πραγματικών παραδειγμάτων `{x⁽¹⁾, ..., x⁽ᵐ⁾}`.
        3.  Ενημέρωση του Διακριτή ανεβαίνοντας την στοχαστική κλίση του `V(D,G)`:
            `∇_{θ_d} (1/m) Σ [log D(x⁽ⁱ⁾) + log(1 - D(G(z⁽ⁱ⁾)))]`
    *   **Ενημέρωση Γεννήτορα:**
        1.  Δειγματοληψία νέου mini-batch θορύβου.
        2.  Ενημέρωση του Γεννήτορα κατεβαίνοντας την στοχαστική κλίση του `V(D,G)` (που ισοδυναμεί με το να προσπαθεί να μεγιστοποιήσει το `log D(G(z))`):
            `∇_{θ_g} (1/m) Σ [log(1 - D(G(z⁽ⁱ⁾)))]`

### **Πρόβλημα Εξαφανιζόμενης Κλίσης (Vanishing Gradient) (Διαφάνεια 43)**

*   Όταν ο Διακριτής γίνεται πολύ καλός (πολύ σίγουρος), το `D(G(z))` πλησιάζει το 0.
*   Η κλίση του όρου `log(1 - D(G(z)))` για τον Γεννήτορα γίνεται πολύ μικρή (εξαφανίζεται), καθιστώντας την εκμάθηση του Γεννήτορα αργή ή αδύνατη.
*   **Λύση:** Αντί ο Γεννήτορας να ελαχιστοποιεί το `log(1 - D(G(z)))`, τον εκπαιδεύουμε να **μεγιστοποιεί το `log D(G(z))`**. Αυτό παρέχει ισχυρότερες κλίσεις στην αρχή της εκπαίδευσης. (Αυτό αναφέρεται ως "non-saturating heuristic" στη διαφάνεια 45).

### **Γιατί Λειτουργούν τα GANs (Διαφάνεια 44)**

1.  **Ο G έχει μια εργασία ενισχυτικής μάθησης:** Ξέρει πότε τα πάει καλά (ξεγελά τον D) αλλά δεν του δίνεται ένα επιβλεπόμενο σήμα. Η οπισθοδιάδοση μέσω του D παρέχει στον G ένα (έμμεσο) επιβλεπόμενο σήμα.
2.  **Δεν περιγράφεται το βέλτιστο μέσω μιας μόνο απώλειας:** Η δυναμική του παιχνιδιού οδηγεί στην ισορροπία.
3.  **Ο D σπάνια ξεγελιέται (ειδικά στην αρχή):** Αλλά ο G εξακολουθεί να μαθαίνει γιατί παίρνει μια κλίση που του λέει πώς να αλλάξει για να τα πάει καλύτερα στον επόμενο γύρο.

### **Έχουν Σημασία οι Συναρτήσεις Απώλειας του Γεννήτορα; (Διαφάνεια 45)**

*   Γράφημα που συγκρίνει διαφορετικές συναρτήσεις απώλειας για τον Γεννήτορα:
    *   **Minimax (πράσινη):** `min log(1 - D(G(z)))` - υποφέρει από vanishing gradients.
    *   **Non-saturating heuristic (μπλε):** `max log D(G(z))` - καλύτερες κλίσεις.
    *   **Maximum likelihood cost (κόκκινη):** Μια άλλη εναλλακτική.
*   Όλες οι απώλειες φαίνεται να παράγουν "αιχμηρά" (ρεαλιστικά) δείγματα (Goodfellow, 2014).

### **Παραδείγματα Παραγόμενων Εικόνων (Διαφάνειες 46-47)**

*   **CIFAR (Διαφ. 46):** Πρώιμα αποτελέσματα GANs σε εικόνες χαμηλής ανάλυσης. Θορυβώδεις αλλά αναγνωρίσιμες.
*   **DCGAN: Εικόνες Υπνοδωματίων (Διαφ. 47):** Πολύ πιο ρεαλιστικές εικόνες που παράγονται από Deep Convolutional GANs.

### **Deep Convolutional GANs (DCGANs) (Διαφάνεια 48)**

*   **Βασικές Ιδέες για τη Βελτίωση των GANs με Συνελικτικά Δίκτυα:**
    *   **Αντικατάσταση των πλήρως συνδεδεμένων (FC) κρυφών επιπέδων με Συνελίξεις.**
    *   **Γεννήτορας:** Χρήση **Κλασματικά Βηματιζόμενων Συνελίξεων (Fractional-Strided Convolutions)** (επίσης γνωστές ως transposed convolutions ή deconvolutions) για αύξηση της χωρικής διάστασης (upsampling).
    *   **Χρήση Batch Normalization** μετά από κάθε επίπεδο (εκτός από το τελικό του γεννήτορα και το αρχικό του διακριτή). Σταθεροποιεί την εκπαίδευση.
    *   **Μέσα στον Γεννήτορα:**
        *   Χρήση **ReLU** για τα κρυφά επίπεδα.
        *   Χρήση **Tanh** για το επίπεδο εξόδου (οι τιμές των pixels είναι συνήθως στο [-1, 1]).
    *   **(Όχι στη διαφάνεια, αλλά συχνά):** Χρήση LeakyReLU στον διακριτή.

---

## **Μέρος 2: Προκλήσεις με τα GANs (Διαφάνειες 49-55)**

*   **Προκλήσεις Εκπαίδευσης:**
    *   **Μη-Σύγκλιση (Non-Convergence)**
    *   **Κατάρρευση Τρόπων (Mode-Collapse)**
*   **Προτεινόμενες Λύσεις:**
    *   Επίβλεψη με Ετικέτες (Supervision with Labels)
    *   Mini-Batch GANs
*   **Τροποποίηση των Συναρτήσεων Απώλειας των GANs:**
    *   Διακριτής (EB-GAN - Energy-Based GAN)
    *   Γεννήτορας (InfoGAN)

### **Μη-Σύγκλιση (Non-Convergence) (Διαφάνειες 50-51)**

*   **Μοντέλα Βαθιάς Μάθησης (γενικά):** Ένας παίκτης που προσπαθεί να μεγιστοποιήσει την ανταμοιβή του (ελαχιστοποίηση απώλειας). Ο SGD (με Backpropagation) βρίσκει βέλτιστες παραμέτρους. Έχει εγγυήσεις σύγκλισης (υπό ορισμένες συνθήκες). Πρόβλημα: Με μη-κυρτότητα, μπορεί να συγκλίνει σε τοπικά βέλτιστα.
*   **GANs:** Δύο (ή περισσότεροι) παίκτες. Ο Διακριτής προσπαθεί να μεγιστοποιήσει την ανταμοιβή του. Ο Γεννήτορας προσπαθεί να ελαχιστοποιήσει την ανταμοιβή του Διακριτή.
*   **Πρόβλημα:** Ο SGD **δεν σχεδιάστηκε για να βρει την ισορροπία Nash** ενός παιχνιδιού. Μπορεί να μην συγκλίνουμε καθόλου στην ισορροπία Nash.
*   **Παράδειγμα (Διαφ. 51):** Το παιχνίδι `min_x max_y xy`. Οι κλίσεις οδηγούν σε μια κυκλική τροχιά (`∂/∂x = y`, `∂/∂y = x`). Η λύση της διαφορικής εξίσωσης έχει ημιτονοειδείς όρους. Ακόμα και με μικρό ρυθμό μάθησης, δεν θα συγκλίνει.

### **Κατάρρευση Τρόπων (Mode-Collapse) (Διαφάνεια 52)**

*   Ο Γεννήτορας **αποτυγχάνει να παράγει ποικίλα δείγματα**.
*   **Παράδειγμα:** Αν ο στόχος είναι να παραχθούν 8 διαφορετικές ομάδες σημείων (Target), ο Γεννήτορας μπορεί να μάθει να παράγει μόνο μία (ή πολύ λίγες) από αυτές τις ομάδες τέλεια (Output), αγνοώντας τις υπόλοιπες.
*   Ο Γεννήτορας βρίσκει ένα ή λίγα δείγματα που ξεγελούν καλά τον Διακριτή και παράγει μόνο αυτά.

### **Πώς να Ανταμείψουμε την Ποικιλομορφία Δειγμάτων; (Διαφάνεια 53)**

*   **Στην Κατάρρευση Τρόπων:** Ο Γεννήτορας παράγει καλά δείγματα, αλλά πολύ λίγα από αυτά. Ο Διακριτής δεν μπορεί να τα χαρακτηρίσει ως ψεύτικα (γιατί μοιάζουν καλά).
*   **Για να αντιμετωπιστεί αυτό το πρόβλημα:** Κάνουμε τον Διακριτή να γνωρίζει αυτή την "ακραία περίπτωση".
*   **Πιο Επίσημα (Mini-Batch GANs - Διαφ. 54):**
    *   Ο Διακριτής εξετάζει **ολόκληρη την παρτίδα (batch)** αντί για μεμονωμένα παραδείγματα.
    *   **Εξαγωγή χαρακτηριστικών που καταγράφουν την ποικιλομορφία** στην mini-batch (π.χ., L2 νόρμα της διαφοράς μεταξύ όλων των ζευγών από την παρτίδα).
    *   Τροφοδότηση αυτών των χαρακτηριστικών στον Διακριτή μαζί με την εικόνα.
    *   Οι τιμές των χαρακτηριστικών θα διαφέρουν μεταξύ ποικίλων και μη-ποικίλων παρτίδων. Ο Διακριτής θα βασιστεί σε αυτά τα χαρακτηριστικά για ταξινόμηση.
    *   **Αυτό με τη σειρά του:** Θα αναγκάσει τον Γεννήτορα να ταιριάξει αυτές τις τιμές χαρακτηριστικών με τα πραγματικά δεδομένα, άρα θα παράγει ποικίλες παρτίδες.

**Επίβλεψη με Ετικέτες (Supervision with Labels) (Διαφάνεια 55)**

*   Η πληροφορία των ετικετών των πραγματικών δεδομένων μπορεί να βοηθήσει.
*   Αντί ο Διακριτής να διακρίνει απλώς "Πραγματικό" vs "Ψεύτικο", μπορεί να εκπαιδευτεί να ταξινομεί τα πραγματικά δεδομένα στις σωστές τους κλάσεις (π.χ., Car, Dog, Human) και να χαρακτηρίζει όλα τα ψεύτικα ως "Fake".
*   Εμπειρικά, παράγει πολύ καλύτερα δείγματα. (Αυτό οδηγεί στα Conditional GANs).

---

## **Μέρος 2 (συνέχεια) & Μέρος 3: Εναλλακτικές Προβολές, Τροποποιήσεις Απωλειών, Εφαρμογές & Προηγμένες Επεκτάσεις (Διαφάνειες 56-83)**

### **Εναλλακτική Άποψη των GANs (Διαφάνειες 56-57)**

*   **Αρχική Διατύπωση:** Στρατηγική Διακριτή: `D(x) → 1` (πραγματικό), `D(G(z)) → 0` (ψεύτικο).
    *   `D* = argmax_D V(D,G)`
    *   `G* = argmin_G V(D,G)` (ή `argmax_G -V(D,G)`)
*   **Εναλλακτικά (Διαφ. 56, κάτω):** Αντιστροφή των ετικετών: Ψεύτικο = 1, Πραγματικό = 0.
    *   `V'(D,G) = E_{x~p_data(x)}[log(1 - D(x))] + E_{z~p_z(z)}[log D(G(z))]`
    *   Στρατηγική Διακριτή τώρα: `D(x) → 0`, `D(G(z)) → 1`.
*   **Αν θέλουμε να κωδικοποιήσουμε `D(x) → 0`, `D(G(z)) → 1` (Διαφ. 57):**
    *   Ο Γεννήτορας μπορεί να ελαχιστοποιεί το `E_{x~p_data(x)}[log D(x)] + E_{z~p_z(z)}[log(1 - D(G(z)))]` (αυτό φαίνεται σαν τυπογραφικό λάθος στη διαφάνεια, θα έπρεπε να σχετίζεται με την απώλεια του Γεννήτορα όταν ο Διακριτής προσπαθεί να πετύχει `D(x) → 0` και `D(G(z)) → 1`).
    *   **Hinge Loss:** Αντί για διασταυρούμενη εντροπία, μπορούμε να χρησιμοποιήσουμε άλλη συνάρτηση απώλειας. Με Hinge Loss, ο Διακριτής εξάγει:
        *   Υψηλές τιμές για ψεύτικα δείγματα.
        *   Χαμηλές τιμές για πραγματικά δείγματα.

### **Energy-Based GANs (EB-GANs) (Διαφάνεια 58)**

*   **Τροποποιημένο Παιχνίδι:**
    *   **Γεννήτορας:** Προσπαθεί να παράγει δείγματα με χαμηλές τιμές "ενέργειας".
    *   **Διακριτής:** Προσπαθεί να αντιστοιχίσει υψηλές τιμές "ενέργειας" σε ψεύτικα δείγματα.
*   **Χρήση Autoencoder μέσα στον Διακριτή:**
    *   Ο Διακριτής είναι ένας Autoencoder.
    *   Η "ενέργεια" `D(x)` ορίζεται ως το **Μέσο Τετραγωνικό Σφάλμα Ανακατασκευής (Mean-Squared Reconstruction error)**: `D(x) = ||Dec(Enc(x)) - x||_MSE`.
    *   **Υπόθεση:**
        *   **Υψηλό Σφάλμα Ανακατασκευής για Ψεύτικα δείγματα** (ο Autoencoder δεν έχει εκπαιδευτεί καλά σε αυτά).
        *   **Χαμηλό Σφάλμα Ανακατασκευής για Πραγματικά δείγματα** (ο Autoencoder έχει εκπαιδευτεί να τα ανακατασκευάζει καλά).

### **Παραμετροποίηση Χαρακτηριστικών (Feature Parameterization) & Αποσύνδεση (Disentanglement) (Διαφάνειες 60-61)**

*   **3D Πρόσωπα (Διαφ. 60):** Παραδείγματα αλλαγής χαρακτηριστικών (αζιμούθιο, ύψος, φωτισμός, πλάτος) σε 3D μοντέλα προσώπων. Υπονοεί την επιθυμία για έλεγχο συγκεκριμένων χαρακτηριστικών των παραγόμενων δειγμάτων.
*   **Πώς να Ανταμείψουμε την Αποσύνδεση (Disentanglement); (Διαφ. 61):**
    *   **Αποσύνδεση:** Μεμονωμένες διαστάσεις του λανθάνοντος διανύσματος `z` (ή ενός μέρους του) καταγράφουν ανεξάρτητα βασικά χαρακτηριστικά της εικόνας.
    *   **Διαμερισμός του διανύσματος θορύβου:**
        *   **`z` vector:** Καταγράφει μικρές παραλλαγές στην εικόνα.
        *   **`c` vector:** Καταγράφει τα κύρια χαρακτηριστικά της εικόνας (π.χ., για MNIST: Ψηφίο, Γωνία, Πάχος).
    *   Αν το `c` καταγράφει τις βασικές παραλλαγές, τότε το `c` και το `x_fake` (παραγόμενη εικόνα) θα πρέπει να είναι **ισχυρά συσχετισμένα**.

### **Αμοιβαία Πληροφορία (Mutual Information) & InfoGAN (Διαφάνειες 62-64)**

*   **Αμοιβαία Πληροφορία (MI) (Διαφ. 62):**
    *   Καταγράφει την αμοιβαία εξάρτηση μεταξύ δύο μεταβλητών `X` και `Y`.
    *   `I(X;Y) = Σ_{x,y} p(x,y) log (p(x,y) / (p(x)p(y)))`
    *   `I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)` (όπου Η είναι η εντροπία).
*   **InfoGAN (Διαφ. 63):**
    *   Θέλουμε να **μεγιστοποιήσουμε την αμοιβαία πληροφορία `I`** μεταξύ του λανθάνοντος κώδικα `c` (που ελέγχει τα χαρακτηριστικά) και της παραγόμενης εικόνας `x = G(z, c)`.
    *   Ενσωμάτωση στην συνάρτηση αξίας του παιχνιδιού minimax:
        `min_G max_D V_I(D,G) = V(D,G) - λ I(c; G(z,c))`
        (Ο Γεννήτορας προσπαθεί να ελαχιστοποιήσει το `-I`, δηλαδή να μεγιστοποιήσει το `I`).
*   **Μεταβλητικό Κατώτερο Όριο (Variational Lower Bound) της MI (Διαφ. 64):**
    *   Η MI είναι δύσκολο να υπολογιστεί άμεσα. Χρησιμοποιείται ένα κατώτερο όριο που μπορεί να βελτιστοποιηθεί. Αυτό συχνά περιλαμβάνει την εισαγωγή ενός βοηθητικού δικτύου `Q` που προσπαθεί να προβλέψει το `c` από το `G(z,c)`.

---

## **Μέρος 3: Εφαρμογές & Προηγμένες Επεκτάσεις (Διαφάνειες 65-82)**

*   **Conditional GANs (CGANs) (Διαφ. 66-67):**
    *   Απλή τροποποίηση του αρχικού GAN που **εξαρτά (conditions)** το μοντέλο από **πρόσθετη πληροφορία** (π.χ., ετικέτες κλάσης `c`).
    *   Τόσο ο Γεννήτορας (`G(z,c)`) όσο και ο Διακριτής (`D(x,c)`) λαμβάνουν αυτή την πληροφορία ως είσοδο.
    *   Επιτρέπει καλύτερη πολυτροπική μάθηση (multi-modal learning).
    *   **Παράδειγμα (Διαφ. 67):** Παραγωγή ψηφίων MNIST εξαρτώμενη από την ετικέτα κλάσης τους (π.χ., [1,0,0,0,0,0,0,0,0,0] για το ψηφίο 0).
*   **Χρήση Ετικετών για Βελτίωση Δειγμάτων (Διαφ. 68):**
    *   LAPGAN (Laplacian Pyramid GAN) και CC-LAPGAN (Class-conditional LAPGAN). Δείχνει ότι η χρήση ετικετών (CC-LAPGAN) παράγει οπτικά καλύτερα δείγματα από το απλό LAPGAN ή το αρχικό GAN του Goodfellow.
*   **Μετάφραση Εικόνας-σε-Εικόνα (Image-to-Image Translation) (Διαφ. 69-70):**
    *   Εφαρμογή Pix2Pix (Isola et al.).
    *   Μετατροπή μιας εικόνας από έναν τομέα (domain) σε έναν άλλο (π.χ., ετικέτες σε σκηνή δρόμου -> φωτογραφία, αεροφωτογραφία -> χάρτης, ασπρόμαυρη -> έγχρωμη, σκίτσο τσάντας -> φωτογραφία τσάντας).
    *   **Αρχιτεκτονική:** Βασισμένη σε DCGAN.
    *   Η εκπαίδευση είναι **εξαρτώμενη (conditioned)** στις εικόνες από τον τομέα πηγής.
*   **Σύνθεση Κειμένου-σε-Εικόνα (Text-to-Image Synthesis) (Διαφ. 71-72):**
    *   **Κίνητρο:** Δεδομένης μιας περιγραφής κειμένου, παραγωγή εικόνων που σχετίζονται στενά.
    *   Χρήση Conditional GAN όπου ο Γεννήτορας και ο Διακριτής εξαρτώνται από μια "πυκνή" ενσωμάτωση κειμένου (text embedding).
    *   **Θετικά/Αρνητικά Παραδείγματα Εκπαίδευσης (Διαφ. 72):**
        *   Θετικό: Πραγματική Εικόνα, Σωστό Κείμενο.
        *   Αρνητικά: Πραγματική Εικόνα, Λάθος Κείμενο. Ψεύτικη Εικόνα, Σωστό Κείμενο.
*   **Γήρανση Προσώπου με Conditional GANs (Διαφ. 73-74):**
    *   Χρήση ενός βοηθητικού δικτύου για **Βελτιστοποίηση Διατήρησης Ταυτότητας (Identity Preservation Optimization)** για καλύτερη προσέγγιση του λανθάνοντος κώδικα `z*` για μια εικόνα εισόδου.
    *   Ο λανθάνων κώδικας στη συνέχεια εξαρτάται από μια διακριτή (one-hot) ενσωμάτωση κατηγοριών ηλικίας.
    *   Παραδείγματα εικόνων που δείχνουν την εξέλιξη της ηλικίας.
*   **Προηγμένες Επεκτάσεις GAN (Διαφ. 75):**
    *   Coupled GAN (δεν καλύφθηκε)
    *   **LAPGAN – Laplacian Pyramid of Adversarial Networks (Διαφ. 76-79):**
        *   Βασίζεται στην αναπαράσταση εικόνων με Λαπλασιανή Πυραμίδα.
        *   Παράγει εικόνες υψηλής ανάλυσης χρησιμοποιώντας ένα **ιεραρχικό σύστημα από GANs**.
        *   **Παραγωγή Εικόνας (Διαφ. 77):**
            *   Ένας Γεννήτορας `G_k` παράγει μια εικόνα βάσης χαμηλής ανάλυσης `Ĩ_k` από θόρυβο `z_k`.
            *   Άλλοι Γεννήτορες `G_i` (για `i < k`) παράγουν επαναληπτικά την **εικόνα διαφοράς (difference image) `ĥ`**, εξαρτώμενοι από την προηγούμενη μικρή εικόνα `I` (που είναι μια υπερκλιμακωμένη έκδοση της προηγούμενης παραγόμενης εικόνας).
            *   Αυτή η εικόνα διαφοράς προστίθεται στην υπερκλιμακωμένη έκδοση της προηγούμενης μικρότερης εικόνας.
        *   **Διαδικασία Εκπαίδευσης (Διαφ. 78):** Τα μοντέλα (GANs) σε κάθε επίπεδο της πυραμίδας εκπαιδεύονται ανεξάρτητα για να μάθουν την απαιτούμενη αναπαράσταση.
        *   **Παράδειγμα (Διαφ. 79):** Με πυραμίδα εικόνων, μπορεί επίσης να τροφοδοτηθεί ο γεννήτορας με εικόνα χαμηλής ανάλυσης + κλάση.
    *   **Adversarially Learned Inference (ALI) / BiGAN (Διαφ. 80-82):**
        *   Βασική ιδέα: Να μάθουμε ένα δίκτυο **κωδικοποιητή/συμπερασμού (encoder/inference network `q(z|x)`)** παράλληλα με το δίκτυο γεννήτορα (`p(x|z)`).
        *   Ο Διακριτής προσπαθεί να διακρίνει ζεύγη `(x, z)` που προέρχονται είτε από τον κωδικοποιητή (πραγματικό `x` και το αντίστοιχο `z`) είτε από τον γεννήτορα (πραγματικό `z` και το παραγόμενο `x`).
        *   **Ισορροπία Nash:** `p(x,z) ~ q(x,z)`, που σημαίνει ότι οι οριακές και οι υπό συνθήκη κατανομές ταιριάζουν.
        *   Η μαθημένη λανθάνουσα αναπαράσταση ανακατασκευάζει επιτυχώς την αρχική εικόνα και είναι χρήσιμη για ημι-επιβλεπόμενες εργασίες.

### **Σύνοψη (Διαφάνεια 83)**

*   GANs: Παραγωγικά μοντέλα υλοποιημένα με δύο στοχαστικές νευρωνικές ενότητες: **Γεννήτορας** και **Διακριτής**.
*   **Γεννήτορας:** Προσπαθεί να παράγει δείγματα από τυχαίο θόρυβο ως είσοδο.
*   **Διακριτής:** Προσπαθεί να διακρίνει τα δείγματα από τον Γεννήτορα και τα δείγματα από την πραγματική κατανομή δεδομένων.
*   Και τα δύο δίκτυα εκπαιδεύονται **ανταγωνιστικά (adversarially)** (σε συνδυασμό) για να ξεγελάσουν το άλλο στοιχείο. Σε αυτή τη διαδικασία, και τα δύο μοντέλα γίνονται καλύτερα στις αντίστοιχες εργασίες τους.

### **Γιατί να Χρησιμοποιήσουμε GANs για Παραγωγή; (Διαφάνεια 84)**

*   Μπορούν να εκπαιδευτούν χρησιμοποιώντας **back-propagation**.
*   Μπορούν να παραχθούν **πιο "αιχμηρές" (sharper/more realistic) εικόνες** σε σύγκριση με άλλες μεθόδους όπως οι VAEs.
*   **Ταχύτερη δειγματοληψία** από την κατανομή του μοντέλου: ένα μόνο forward pass παράγει ένα μόνο δείγμα.
