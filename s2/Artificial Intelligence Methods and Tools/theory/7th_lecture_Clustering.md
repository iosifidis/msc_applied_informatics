# **Συσταδοποίηση (Clustering)**

---

## **1. Εισαγωγή στη Συσταδοποίηση (Slides 1-2)**

*   **Τι είναι η Συσταδοποίηση (Clustering);** Είναι μια τεχνική **μη επιβλεπόμενης μάθησης (unsupervised learning)**. Ο στόχος είναι να ομαδοποιήσουμε ένα σύνολο αντικειμένων (σημείων δεδομένων) έτσι ώστε τα αντικείμενα στην ίδια ομάδα (συστάδα - cluster) να είναι πιο **όμοια (similar)** μεταξύ τους απ' ό,τι με τα αντικείμενα σε άλλες ομάδες.
*   **Δεν υπάρχουν προκαθορισμένες ετικέτες κλάσεων** (σε αντίθεση με την ταξινόμηση). Ο αλγόριθμος πρέπει να ανακαλύψει μόνος του τη δομή ομαδοποίησης στα δεδομένα.
*   **Η Έννοια της Ομοιότητας (Similarity - Slide 2):** Είναι δύσκολο να δοθεί ένας αυστηρός ορισμός. Διαισθητικά, καταλαβαίνουμε πότε δύο πράγματα μοιάζουν. Στη συσταδοποίηση, η ομοιότητα ποσοτικοποιείται συνήθως μέσω **μετρικών απόστασης (distance measures)** ή **συναρτήσεων ομοιότητας (similarity functions)**.

---

## **2. Μετρικές Απόστασης (Distance Measures) (Slides 3-4)**

Μια μετρική απόστασης `D(A, B)` μεταξύ δύο σημείων Α και Β πρέπει ιδανικά να έχει τις εξής ιδιότητες:

1.  **Συμμετρία (Symmetry):** `D(A, B) = D(B, A)`
    *   Η απόσταση από το Α στο Β είναι ίδια με την απόσταση από το Β στο Α. (π.χ., δεν θα μπορούσαμε να πούμε "ο Άλεξ μοιάζει στον Μπομπ, αλλά ο Μπομπ δεν μοιάζει καθόλου στον Άλεξ").
2.  **Σταθερότητα Αυτο-Ομοιότητας (Constancy of Self-Similarity):** `D(A, A) = 0`
    *   Η απόσταση ενός σημείου από τον εαυτό του είναι μηδέν. (π.χ., δεν θα λέγαμε "ο Άλεξ μοιάζει περισσότερο στον Μπομπ απ' ό,τι ο Μπομπ στον εαυτό του").
3.  **Θετικότητα / Διαχωρισμός (Positivity / Separation):** `D(A, B) = 0` αν και μόνο αν `A = B`
    *   Η απόσταση είναι μηδέν *μόνο* αν τα σημεία ταυτίζονται. (π.χ., δεν θα υπήρχαν δύο διαφορετικά αντικείμενα που να μην μπορούμε να τα ξεχωρίσουμε με βάση την απόσταση).
4.  **Τριγωνική Ανισότητα (Triangular Inequality):** `D(A, B) ≤ D(A, C) + D(B, C)`
    *   Η "απευθείας" απόσταση μεταξύ Α και Β δεν μπορεί να είναι μεγαλύτερη από την απόσταση μέσω ενός ενδιάμεσου σημείου C. (π.χ., αν ο Άλεξ μοιάζει πολύ στον Μπομπ και πολύ στον Καρλ, δεν γίνεται ο Μπομπ να μην μοιάζει καθόλου στον Καρλ).

---

## **3. Δημιουργία & Προετοιμασία Δεδομένων (Slides 5-8)**

*   Για την επίδειξη των αλγορίθμων, δημιουργούνται τεχνητά (synthetic) σύνολα δεδομένων (dummy datasets).
*   **Παράδειγμα 1 (Blobs):** 3 διακριτές, κάπως σφαιρικές ομάδες σημείων (Slide 5, αριστερά & Slide 6, αριστερά).
*   **Παράδειγμα 2 (Moons):** 2 ομάδες σε σχήμα ημισελήνου, που δεν είναι εύκολα διαχωρίσιμες γραμμικά (Slide 5, δεξιά & Slide 6, δεξιά).
*   **Προσθήκη Θορύβου (Noise - Slide 7):** Στα δεδομένα προστίθεται τυχαίος θόρυβος για να γίνουν πιο ρεαλιστικά.
*   **Δεδομένα για τους Αλγορίθμους (Slide 8):** Οι αλγόριθμοι συσταδοποίησης λαμβάνουν *μόνο* τις τιμές των χαρακτηριστικών (features). **Δεν γνωρίζουν** τον πραγματικό αριθμό ομάδων (κλάσεων), τα επίπεδα θορύβου, ή την πραγματική δομή.

---

## **4. Η Αμφισημία της Έννοιας "Συστάδα" (Ambiguity - Slide 9)**

*   Η έννοια της συστάδας μπορεί να είναι υποκειμενική και εξαρτάται από την οπτική γωνία ή το επίπεδο ανάλυσης. Το ίδιο σύνολο σημείων μπορεί να ερμηνευθεί ότι έχει διαφορετικό αριθμό συστάδων.

---

## **5. Τύποι Συσταδοποίησης (Types of Clustering - Slides 10-12)**

*   **Διαμεριστική (Partitional) Συσταδοποίηση (Slide 10, 11):**
    *   Χωρίζει τα δεδομένα σε ένα *προκαθορισμένο* αριθμό `k` από μη επικαλυπτόμενα υποσύνολα (συστάδες).
    *   Κάθε σημείο ανήκει ακριβώς σε μία συστάδα.
    *   Π.χ., K-Means.
*   **Ιεραρχική (Hierarchical) Συσταδοποίηση (Slide 10, 12):**
    *   Δημιουργεί ένα σύνολο από ένθετες (nested) συστάδες, οργανωμένες σε μια δενδρική δομή (δενδρόγραμμα - dendrogram).
    *   Δεν απαιτεί προκαθορισμένο αριθμό συστάδων. Ο αριθμός μπορεί να επιλεγεί "κόβοντας" το δενδρόγραμμα σε ένα συγκεκριμένο ύψος.
    *   Μπορεί να είναι **συσσωρευτική (agglomerative)** (ξεκινά με κάθε σημείο ως ξεχωριστή συστάδα και τις συγχωνεύει σταδιακά) ή **διαιρετική (divisive)** (ξεκινά με όλα τα σημεία σε μία συστάδα και τις διασπά σταδιακά).

---

## **6. Αλγόριθμοι Συσταδοποίησης (Ένας-Ένας) (Slides 13-93)**

Εξετάζουμε διάφορους αλγόριθμους:

*   **Προσέγγιση 1: k-Means (Slides 14-19)**
    *   **Τύπος:** Διαμεριστικός.
    *   **Απαιτεί:** Να προκαθοριστεί ο αριθμός των συστάδων `K`.
    *   **Ιδέα:**
        1.  Κάθε συστάδα συνδέεται με ένα **κεντροειδές (centroid)** (το μέσο σημείο της συστάδας).
        2.  Κάθε σημείο δεδομένων αντιστοιχίζεται στη συστάδα του οποίου το κεντροειδές είναι το πλησιέστερο.
    *   **Βασικός Αλγόριθμος (Slide 14):**
        1.  *Αρχικοποίηση:* Διάλεξε `K` τυχαία σημεία ως αρχικά κεντροειδή.
        2.  **repeat**
        3.  *Αντιστοίχιση:* Αντιστοίχισε κάθε σημείο στο πλησιέστερο κεντροειδές, σχηματίζοντας `K` συστάδες.
        4.  *Ενημέρωση:* Υπολόγισε ξανά τη θέση του κεντροειδούς για κάθε νέα συστάδα (ως τον μέσο όρο των σημείων της).
        5.  **until** Τα κεντροειδή δεν αλλάζουν θέση (ή αλλάζουν ελάχιστα).
    *   **Αποτελέσματα (Slide 15):** Στα "blobs" λειτουργεί καλά. Στα "moons" αποτυγχάνει να βρει τη σωστή δομή.
    *   **Περιορισμοί (Slides 16-19):**
        *   Δυσκολεύεται όταν οι συστάδες έχουν διαφορετικά **μεγέθη (sizes)**.
        *   Δυσκολεύεται όταν οι συστάδες έχουν διαφορετικές **πυκνότητες (densities)**.
        *   Δυσκολεύεται με **μη σφαιρικά σχήματα (non-globular shapes)** (όπως τα "moons").
        *   Ευαίσθητος σε **ακραίες τιμές (outliers)**. (Μια λύση: αφαίρεση outliers πριν τη συσταδοποίηση).
        *   Η απόδοση εξαρτάται από την αρχική επιλογή κεντροειδών.
*   **Προσέγγιση 2: Affinity Propagation (AP) (Slides 20-22)**
    *   **Ιδέα (Slide 20):** Δεν απαιτεί προκαθορισμό του `K`. Λειτουργεί βρίσκοντας "εκπροσώπους" (exemplars) που είναι αντιπροσωπευτικοί για τις συστάδες. Ανταλλάσσει μηνύματα μεταξύ των σημείων δεδομένων για να αποφασίσει ποια σημεία είναι καλοί εκπρόσωποι και σε ποιον εκπρόσωπο ανήκει κάθε άλλο σημείο.
    *   **Είσοδος:** Πίνακας ομοιοτήτων μεταξύ ζευγών σημείων και "προτιμήσεις" (preferences) για το πόσο πιθανό είναι κάθε σημείο να γίνει εκπρόσωπος.
    *   **Αποτελέσματα (Slide 21):** Στα παραδείγματα, βρίσκει πολλούς μικρούς clusters, δεν φαίνεται να αποδίδει καλά.
    *   **Πλεονεκτήματα/Μειονεκτήματα (Slide 22):**
        *   (+) Δεν χρειάζεται `K`.
        *   (+) Μπορεί να βρει καλά clusters με διαφορετικές πυκνότητες/μεγέθη.
        *   (+) Χρησιμοποιείται για μη-γραμμικές δομές.
        *   (-) Υπολογιστικά ακριβός, ειδικά για μεγάλα datasets.
        *   (-) Ευαίσθητος στην επιλογή της μετρικής ομοιότητας και των preferences.
        *   (-) Μπορεί να δώσει πολλούς εκπροσώπους για μία "πραγματική" συστάδα, δύσκολη ερμηνεία.
*   **Προσέγγιση 3: Mean Shift (Slides 23-32)**
    *   **Ιδέα (Slides 23-24):** Μη-παραμετρικός αλγόριθμος βασισμένος στην εκτίμηση πυκνότητας. Προσπαθεί να βρει τα **κέντρα (modes)** των περιοχών με τη μεγαλύτερη πυκνότητα σημείων. Υποθέτει ότι τα σημεία προέρχονται από μια υποκείμενη κατανομή πιθανότητας (PDF).
    *   **Διαδικασία (Slides 24-30):**
        1.  Για κάθε σημείο, ορίζεται ένα "παράθυρο" (π.χ., κύκλος) γύρω του (Region of interest).
        2.  Υπολογίζεται το "κέντρο μάζας" (Center of mass) των σημείων μέσα στο παράθυρο.
        3.  Το παράθυρο "μετατοπίζεται" (shifts) ώστε το κέντρο του να συμπέσει με το κέντρο μάζας (Mean Shift vector).
        4.  Τα βήματα 2-3 επαναλαμβάνονται μέχρι το παράθυρο να σταματήσει να μετακινείται (να συγκλίνει σε ένα mode).
        5.  Όλα τα σημεία που συγκλίνουν στο ίδιο mode ανήκουν στην ίδια συστάδα.
    *   **Αποτελέσματα (Slide 31):** Στα "blobs" φαίνεται καλό. Στα "moons" πάλι αποτυγχάνει.
    *   **Πλεονεκτήματα/Μειονεκτήματα (Slide 32):**
        *   (+) Γενικής χρήσης, κατάλληλο για πραγματικά δεδομένα.
        *   (+) Χειρίζεται αυθαίρετους χώρους χαρακτηριστικών, δεν υποθέτει σχήμα (π.χ. ελλειπτικό).
        *   (+) Μόνο ΜΙΑ παράμετρος (μέγεθος παραθύρου/bandwidth) χρειάζεται ρύθμιση.
        *   (-) Η επιλογή του μεγέθους παραθύρου δεν είναι τετριμμένη και επηρεάζει πολύ το αποτέλεσμα (λάθος μέγεθος μπορεί να συγχωνεύσει modes ή να δημιουργήσει ψεύτικα).
*   **Προσέγγιση 4: Spectral Clustering (Φασματική Συσταδοποίηση) (Slides 33-39)**
    *   **Ιδέα (Slide 33):** Χρησιμοποιεί τις ιδιότητες ενός **γράφου ομοιότητας (similarity graph)** των δεδομένων για να βρει τις συστάδες. Μετασχηματίζει το πρόβλημα σε ένα πρόβλημα διαμέρισης γράφου.
    *   **Βήματα (Slide 33):**
        1.  **Δημιουργία Γράφου Ομοιότητας (Slides 34-35):** Αναπαριστά τα σημεία ως κόμβους (V) και τις σχέσεις ομοιότητας/απόστασης ως ακμές (E). Συνήθεις τρόποι:
            *   *ε- γειτονιά (ε-neighborhood graph):* Συνδέονται σημεία με απόσταση μικρότερη από `ε`.
            *   *k-πλησιέστεροι γείτονες (k-nearest neighbor graph):* Κάθε σημείο συνδέεται με τους `k` πλησιέστερους γείτονές του.
        2.  **Υπολογισμός Λαπλασιανού Πίνακα (Laplacian Matrix - Slide 36):** Κατασκευάζεται ο πίνακας `L = D - W`, όπου `W` είναι ο πίνακας βαρών (ομοιοτήτων) του γράφου και `D` ο διαγώνιος πίνακας βαθμών των κόμβων.
        3.  **Υπολογισμός Ιδιοτιμών/Ιδιοδιανυσμάτων:** Βρίσκουμε τις `k` πρώτες (μικρότερες) ιδιοτιμές και τα αντίστοιχα ιδιοδιανύσματα του `L`. Οι γραμμές του πίνακα `U` που σχηματίζουν αυτά τα `k` ιδιοδιανύσματα δίνουν μια **νέα αναπαράσταση (feature vector)** χαμηλότερης διάστασης για κάθε αρχικό σημείο δεδομένων (Slide 37).
        4.  **Εκτέλεση k-Means:** Εφαρμόζουμε τον αλγόριθμο k-Means σε αυτές τις *νέες αναπαραστάσεις* (τις γραμμές του `U`) για να πάρουμε τις τελικές `k` συστάδες.
    *   **Αποτελέσματα (Slide 38):** Αποδίδει καλά και στα "blobs" και στα "moons".
    *   **Πλεονεκτήματα/Μειονεκτήματα (Slide 39):**
        *   (+) Μπορεί να βρει μη-κυρτές (non-convex) και ακανόνιστου σχήματος συστάδες.
        *   (+) Αποκαλύπτει υποκείμενες δομές που άλλοι αλγόριθμοι μπορεί να χάσουν.
        *   (+) Σχετικά ανθεκτικό στον θόρυβο.
        *   (-) Ευαίσθητο στην επιλογή υπερ-παραμέτρων (π.χ., αριθμός συστάδων `k`, τρόπος κατασκευής γράφου, μετρική ομοιότητας).
        *   (-) Υπολογιστικά ακριβό, ειδικά ο υπολογισμός ιδιοδιανυσμάτων για μεγάλα datasets.
        *   (-) Δεν κλιμακώνεται (scale) καλά σε πολύ μεγάλα datasets λόγω της ιδιοανάλυσης.
*   **Προσέγγιση 5: Agglomerative Clustering (Συσσωρευτική Συσταδοποίηση) (Slides 40-67)**
    *   **Τύπος:** Ιεραρχική (συγκεκριμένα, συσσωρευτική).
    *   **Ιδέα (Slide 40, 43):**
        1.  Ξεκινά με κάθε σημείο δεδομένων ως μια ξεχωριστή συστάδα.
        2.  Σε κάθε βήμα, βρίσκει το ζεύγος συστάδων που είναι πιο "κοντά" (πιο όμοιο) και το **συγχωνεύει (merge)** σε μία νέα συστάδα.
        3.  Η διαδικασία επαναλαμβάνεται μέχρι να μείνει μόνο μία συστάδα (που περιέχει όλα τα σημεία).
    *   **Δενδρόγραμμα (Dendrogram - Slide 40):** Οπτικοποιεί τη διαδικασία συγχώνευσης. Ο κάθετος άξονας συνήθως δείχνει την απόσταση/ανομοιότητα στην οποία έγινε η συγχώνευση.
    *   **Επιλογή Αριθμού Συστάδων (Slide 40, 41):** Μπορούμε να πάρουμε `k` συστάδες "κόβοντας" το δενδρόγραμμα οριζόντια σε ένα κατάλληλο ύψος. Δύο καλά διαχωρισμένα υποδέντρα υποδηλώνουν δύο βασικές συστάδες.
    *   **Ανίχνευση Outliers (Slide 42):** Μεμονωμένοι κλάδοι που συγχωνεύονται πολύ αργά (σε μεγάλη απόσταση) μπορεί να αντιστοιχούν σε outliers.
    *   **Βασικό Βήμα: Υπολογισμός Απόστασης *Μεταξύ Συστάδων*** (Slides 43, 48-52): Πώς ορίζουμε την απόσταση μεταξύ δύο συστάδων (που μπορεί να έχουν πολλά σημεία); Υπάρχουν διάφοροι τρόποι (linkage criteria):
        *   **MIN (Single Linkage - Slide 49, 53-56):** Η απόσταση είναι η *ελάχιστη* απόσταση μεταξύ ενός σημείου της μιας συστάδας και ενός σημείου της άλλης. (+) Καλό για μη-ελλειπτικά σχήματα. (-) Ευαίσθητο σε θόρυβο.
        *   **MAX (Complete Linkage - Slide 50, 57-60):** Η απόσταση είναι η *μέγιστη* απόσταση μεταξύ ενός σημείου της μιας συστάδας και ενός σημείου της άλλης. (+) Λιγότερο ευαίσθητο σε θόρυβο. (-) Τείνει να "σπάει" μεγάλες συστάδες, μεροληπτεί υπέρ σφαιρικών συστάδων.
        *   **Group Average (Slide 51, 61-63):** Η απόσταση είναι ο *μέσος όρος* όλων των αποστάσεων μεταξύ ζευγών σημείων από τις δύο συστάδες. (Συμβιβασμός μεταξύ MIN και MAX). (-) Μεροληπτεί υπέρ σφαιρικών συστάδων.
        *   **Distance Between Centroids (Slide 52):** Η απόσταση μεταξύ των κεντροειδών των δύο συστάδων.
        *   **Ward's Method (Slide 52, 64):** Η ομοιότητα βασίζεται στην *αύξηση του τετραγωνικού σφάλματος* (συγκεκριμένα, η αύξηση της συνολικής ενδο-συστάδας διακύμανσης) όταν δύο συστάδες συγχωνεύονται. Στόχος είναι η συγχώνευση που προκαλεί τη μικρότερη αύξηση σφάλματος. Λιγότερο ευαίσθητο σε θόρυβο, μεροληπτεί υπέρ σφαιρικών συστάδων, θεωρείται ιεραρχικό ανάλογο του K-means.
    *   **Παραδείγματα Δενδρογραμμάτων (Slides 54, 58, 62):** Δείχνουν πώς η σειρά συγχώνευσης διαφέρει ανάλογα με το κριτήριο linkage.
    *   **Σύγκριση (Slide 65):** Οπτική σύγκριση των φωλιασμένων συστάδων που προκύπτουν από MIN, MAX, Group Average, Ward.
    *   **Αποτελέσματα (Slides 66-67):** Agglomerative με Average Linkage και Ward δουλεύουν καλά στα "blobs", αλλά όχι τόσο καλά στα "moons".
*   **Προσέγγιση 6: DBSCAN (Density-Based Spatial Clustering of Applications with Noise) (Slides 68-73)**
    *   **Τύπος:** Βασισμένος στην πυκνότητα.
    *   **Ιδέα:** Ομαδοποιεί σημεία που είναι πυκνά συνδεδεμένα, χαρακτηρίζοντας ως θόρυβο τα σημεία που βρίσκονται μόνα τους σε αραιές περιοχές.
    *   **Βασικές Έννοιες (Slide 68-69):**
        *   **Eps (ε):** Μια ακτίνα απόστασης.
        *   **MinPts:** Ένας ελάχιστος αριθμός σημείων.
        *   **Core Point (Σημείο Πυρήνα):** Ένα σημείο `p` είναι core point αν υπάρχουν τουλάχιστον `MinPts` σημεία (συμπεριλαμβανομένου του `p`) μέσα στην ακτίνα `Eps` γύρω του. Αυτά είναι τα εσωτερικά σημεία μιας συστάδας.
        *   **Border Point (Συνοριακό Σημείο):** Ένα σημείο που *δεν* είναι core point, αλλά βρίσκεται στην `Eps`-γειτονιά ενός core point.
        *   **Noise Point (Σημείο Θορύβου):** Ένα σημείο που δεν είναι ούτε core point ούτε border point.
    *   **Αλγόριθμος:** Βρίσκει τα core points και τα συνδέει αν είναι κοντά μεταξύ τους (density-connected). Τα border points αντιστοιχίζονται στις συστάδες των core points που τα "φτάνουν". Τα υπόλοιπα είναι θόρυβος.
    *   **Παράδειγμα (Slide 70):** Εφαρμογή σε ένα σύνθετο σχήμα, δείχνοντας core (πράσινα), border (γαλάζια), και noise (κόκκινα) σημεία.
    *   **Αποτελέσματα (Slide 71):** Στα παραδείγματα των "blobs" και "moons", δεν φαίνεται να λειτουργεί καλά με τις default παραμέτρους που δοκιμάστηκαν.
    *   **Πότε Λειτουργεί Καλά (Slide 72):** Μπορεί να χειριστεί συστάδες διαφορετικών σχημάτων και μεγεθών, ανθεκτικό στον θόρυβο.
    *   **Πότε ΔΕΝ Λειτουργεί Καλά (Slide 73):** Δυσκολεύεται με συστάδες που έχουν **μεταβαλλόμενες πυκνότητες (varying densities)** και σε δεδομένα **υψηλής διαστατικότητας (high-dimensional data)** (η έννοια της πυκνότητας γίνεται προβληματική).
*   **Προσέγγιση 7: OPTICS (Ordering Points To Identify Clustering Structure) (Slides 74-80)**
    *   **Ιδέα (Slide 74):** Επέκταση του DBSCAN που προσπαθεί να αντιμετωπίσει το πρόβλημα των μεταβαλλόμενων πυκνοτήτων. Αντί να αντιστοιχίζει σημεία σε συστάδες, δημιουργεί μια **διατεταγμένη λίστα (ordering)** των σημείων που αναπαριστά την πυκνο-βασισμένη δομή συσταδοποίησης.
    *   **Διαφορές από DBSCAN:**
        *   Παράγει μια σειρά (όχι απευθείας αντιστοίχιση σε clusters).
        *   Δημιουργεί ιεραρχικό αποτέλεσμα για "μεταβλητή" ακτίνα γειτονιάς.
        *   Η παράμετρος `Eps` δεν είναι θεωρητικά απαραίτητη (χρησιμοποιείται πρακτικά για απόδοση).
    *   **Νέες Έννοιες (Slide 75):**
        *   **Core Distance:** Η *μικρότερη* ακτίνα `Eps` που θα καθιστούσε ένα σημείο `p` core point.
        *   **Reachability Distance (Απόσταση Προσιτότητας):** Η απόσταση προσιτότητας ενός σημείου `q` *από* ένα άλλο σημείο `p` είναι `max(core_distance(p), distance(p, q))`. Ουσιαστικά, είναι η ελάχιστη απόσταση ώστε το `q` να είναι "προσιτό" από το `p`, με τον περιορισμό ότι το `p` πρέπει να είναι core point.
    *   **Reachability Plot (Γράφημα Προσιτότητας) (Slides 77-78):** Οπτικοποιεί τις αποστάσεις προσιτότητας για τα σημεία στη σειρά που παράγει ο OPTICS. Οι "κοιλάδες" (valleys) στο γράφημα αντιστοιχούν σε πυκνές συστάδες (τα σημεία μέσα στην κοιλάδα έχουν μικρή απόσταση προσιτότητας από τα προηγούμενά τους).
    *   **Εξαγωγή Συστάδων (Slide 78):** Οι συστάδες εξάγονται από το reachability plot, είτε χειροκίνητα (ορίζοντας όρια) είτε αυτόματα, αναζητώντας τις κοιλάδες.
    *   **Παράδειγμα (Slide 79):** Από ένα reachability plot (αριστερά) εξάγονται τρεις συστάδες (δεξιά).
    *   **Αποτελέσματα (Slide 80):** Και πάλι, στα συγκεκριμένα παραδείγματα, δεν φαίνεται να αποδίδει καλά.
*   **Προσέγγιση 8: BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) (Slides 81-83)**
    *   **Ιδέα (Slide 81):** Σχεδιασμένος για **πολύ μεγάλα σύνολα δεδομένων (large datasets)**. Λειτουργεί σταδιακά (incrementally) και ιεραρχικά. Δεν φορτώνει όλα τα δεδομένα στη μνήμη.
    *   **CF Tree (Clustering Feature Tree):** Η καρδιά του BIRCH. Μια δενδρική δομή που αποθηκεύει **συνοπτικές πληροφορίες (summaries)** για τις συστάδες (CFs) αντί για τα ίδια τα σημεία. Κάθε κόμβος φύλλου αντιπροσωπεύει μια μικρή, πυκνή υπο-συστάδα.
    *   **Φάσεις (Slide 81):**
        1.  Σαρώνει τη βάση δεδομένων και χτίζει το αρχικό CF Tree στη μνήμη.
        2.  (Προαιρετικά) Εφαρμόζει έναν άλλο αλγόριθμο συσταδοποίησης (π.χ. K-Means) στους κόμβους-φύλλα του CF Tree.
        3.  Αντιστοιχίζει τα σημεία στις συστάδες με βάση τα CFs (όχι τα αρχικά σημεία).
        4.  (Προαιρετικά) Επιπλέον περάσματα για βελτίωση/διόρθωση.
    *   **Αποτελέσματα (Slide 82):** Δίνει λογικά αποτελέσματα και στα δύο παραδείγματα.
    *   **Πλεονεκτήματα/Μειονεκτήματα (Slide 83):**
        *   (+) Κλιμακώσιμος και αποδοτικός για μεγάλα datasets, λειτουργεί σταδιακά.
        *   (+) Αποδοτικός ως προς τη μνήμη λόγω του CF Tree.
        *   (+) Χειρίζεται θόρυβο και μπορεί να βρει outliers.
        *   (+) Ευέλικτος, μπορεί να συνδυαστεί με άλλους αλγόριθμους.
        *   (-) Ευαίσθητος στην επιλογή παραμέτρων (π.χ., threshold, branching factor του CF Tree).
        *   (-) Περιορίζεται σε σφαιρικά clusters, δυσκολεύεται με ακανόνιστα σχήματα.
        *   (-) Συχνά χρειάζεται βήμα τελειοποίησης (refinement), αυξάνοντας την πολυπλοκότητα.
        *   (-) Όχι ιδανικός για υψηλές διαστάσεις.
*   **Προσέγγιση 9: GMM (Gaussian Mixture Models) (Slides 84-93)**
    *   **Ιδέα (Slide 84):** Ένας **πιθανοτικός (probabilistic)** αλγόριθμος. Υποθέτει ότι τα δεδομένα παράγονται από ένα **μείγμα (mixture)** από πολλές **Γκαουσιανές (Κανονικές) κατανομές (Gaussian components)**, όπου κάθε κατανομή αντιπροσωπεύει μια συστάδα.
    *   **Soft Clustering:** Αντί να αντιστοιχίζει απόλυτα κάθε σημείο σε μία συστάδα (hard clustering), το GMM δίνει σε κάθε σημείο μια **πιθανότητα** να ανήκει σε *κάθε* μία από τις συστάδες (Gaussian components).
    *   **Παράμετροι:** Κάθε Γκαουσιανή συνιστώσα χαρακτηρίζεται από έναν μέσο όρο (mean) και έναν πίνακα συνδιακύμανσης (covariance matrix).
    *   **Εκτίμηση Παραμέτρων:** Συνήθως χρησιμοποιείται ο αλγόριθμος **Expectation-Maximization (EM)** για να εκτιμηθούν οι παράμετροι (μέσοι, συνδιακυμάνσεις, βάρη μείγματος) των Γκαουσιανών συνιστωσών.
    *   **Παράδειγμα EM (Slides 85-91):** Οπτικοποίηση των επαναλήψεων του EM. Ξεκινά με τυχαίες παραμέτρους και σταδιακά προσαρμόζει τις ελλείψεις (που αντιπροσωπεύουν τις Γκαουσιανές) για να ταιριάξουν καλύτερα στα δεδομένα.
    *   **Αποτελέσματα (Slide 92):** Δίνει λογικά αποτελέσματα και στα δύο παραδείγματα.
    *   **Πλεονεκτήματα/Μειονεκτήματα (Slide 93):**
        *   (+) Παρέχει soft clustering (πιθανότητες).
        *   (+) Χειρίζεται καλά επικαλυπτόμενες (overlapping) συστάδες.
        *   (+) Είναι πιθανοτικό μοντέλο με σαφή μαθηματική ερμηνεία και ευελιξία.
        *   (-) Ευαίσθητο στην αρχικοποίηση των παραμέτρων.
        *   (-) Απαιτεί προκαθορισμό του αριθμού των συσταδών (components), όπως το K-Means.
        *   (-) Υπολογιστικά ακριβό λόγω του αλγορίθμου EM.
        *   (-) Υποθέτει Γκαουσιανή κατανομή για τις συστάδες, που μπορεί να μην ταιριάζει πάντα στα πραγματικά δεδομένα.

---

## **7. Συμπεράσματα (Slides 94-96)**

*   **Τυχαία Αντιστοίχιση ID Συστάδων (Slide 94):** Τα χρώματα/αριθμοί που δίνονται στις συστάδες από τους αλγόριθμους είναι *τυχαία*. Η συστάδα "0" σε μια εκτέλεση μπορεί να αντιστοιχεί στη συστάδα "1" σε άλλη εκτέλεση. Αυτό *δεν* είναι πρόβλημα ταξινόμησης με προκαθορισμένες κλάσεις.
*   **Πολλαπλές Μετρικές Απόδοσης (Slide 95):** Υπάρχουν διάφορες μετρικές για την αξιολόγηση της ποιότητας της συσταδοποίησης (π.χ., Silhouette score, CH Index, DBI - θα καλυφθούν σε επόμενο μάθημα). Ωστόσο, διαφορετικές μετρικές μπορεί να δίνουν **αντικρουόμενα** αποτελέσματα για το ποιος αλγόριθμος είναι "καλύτερος".
*   **Γενικά Συμπεράσματα (Slide 96):**
    *   Εξερευνήθηκαν διάφορες τεχνικές συσταδοποίησης με τα δυνατά και αδύνατα σημεία τους.
    *   **Δεν υπάρχει ένας μοναδικός "καλύτερος" αλγόριθμος** για όλες τις περιπτώσεις. Η επιλογή εξαρτάται από τη δομή και την κατανομή των δεδομένων.
    *   Προκλήσεις όπως η ευαισθησία στον θόρυβο, την κλίμακα (scale) των δεδομένων και την επιλογή παραμέτρων παραμένουν σημαντικές.
    
---

## 📚 Extra Πηγές

- [GeeksForGeeks - Clustering Guide](https://www.geeksforgeeks.org/)
- [LazyProgrammer ML Compendium](https://lazyprogrammer.me/mlcompendium/)
- [Spectral Clustering Intro](https://towardsdatascience.com/spectral-clustering-for-beginners-d08b7d25b4d8)
- [DBSCAN vs OPTICS](https://www.atlantbh.com/clustering-algorithms-dbscan-vs-optics/)
