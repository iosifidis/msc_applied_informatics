# Clustering

---

## 🔍 Τι είναι το Clustering;

**Clustering** είναι μία από τις βασικές τεχνικές της μη εποπτευόμενης μάθησης (unsupervised learning). Στόχος του είναι η ομαδοποίηση δεδομένων σε υποσύνολα (clusters) με τέτοιο τρόπο ώστε τα σημεία κάθε ομάδας να είναι παρόμοια μεταξύ τους και διαφορετικά από σημεία άλλων ομάδων.

---

## 📐 Ορισμός Ομοιότητας & Απόστασης

Για να μετρηθεί η ομοιότητα, χρησιμοποιούνται συναρτήσεις απόστασης που πρέπει να πληρούν:
- **Συμμετρία**: D(A,B) = D(B,A)
- **Μηδενική απόσταση με τον εαυτό**: D(A,A) = 0
- **Θετικότητα**: D(A,B) = 0 μόνο αν A = B
- **Τριγωνική ανισότητα**: D(A,B) ≤ D(A,C) + D(B,C)

---

## 📊 Είδη Clustering

1. **Partitional Clustering**: Διαχωρίζει τα δεδομένα σε μη επικαλυπτόμενα clusters (π.χ. K-means).
2. **Hierarchical Clustering**: Δημιουργεί ιεραρχικά δέντρα (dendrograms) από nested clusters.

---

## 🧠 Αλγόριθμοι Clustering

### 1. **K-Means**
- Απλό και δημοφιλές.
- Απαιτεί προκαθορισμένο αριθμό clusters.
- Χρησιμοποιεί ευκλείδεια απόσταση και υπολογίζει κέντρα (centroids).
#### ➖ Προβλήματα:
- Δεν λειτουργεί καλά με clusters διαφορετικών μεγεθών/πυκνοτήτων/μορφών.
- Ευαίσθητο σε outliers.

---

### 2. **Affinity Propagation**
- Δεν απαιτεί προκαθορισμένο αριθμό clusters.
- Βασίζεται σε “exemplars” (αντιπροσωπευτικά σημεία).
#### ➕ Πλεονεκτήματα:
- Καλύτερο σε σύνθετα δεδομένα.
#### ➖ Μειονεκτήματα:
- Πολύπλοκο και αργό για μεγάλα σύνολα.

---

### 3. **Mean Shift**
- Βασίζεται στην εκτίμηση πυκνότητας.
- Εντοπίζει περιοχές υψηλής πυκνότητας.
- Δεν απαιτεί καθορισμό αριθμού clusters.
#### ➖ Μειονεκτήματα:
- Η επιλογή παραμέτρου “bandwidth” είναι κρίσιμη.

---

### 4. **Spectral Clustering**
- Μετασχηματίζει τα δεδομένα σε χαμηλότερες διαστάσεις μέσω του γραφήματος συγγένειας (similarity graph).
- Εκτελεί K-means στις ιδιοδιανύσματα του λαπλασιανού πίνακα.
#### ➕ Πλεονεκτήματα:
- Κατάλληλο για μη σφαιρικά clusters.
- Ανθεκτικό στον θόρυβο.
#### ➖ Μειονεκτήματα:
- Υπολογιστικά απαιτητικό.

---

### 5. **Agglomerative (Ιεραρχικό) Clustering**
- Ξεκινά από κάθε σημείο ως ξεχωριστό cluster και συγχωνεύει βάσει απόστασης.
#### ➕ Πλεονεκτήματα:
- Δεν απαιτεί αριθμό clusters εκ των προτέρων.
- Παράγει δενδρογράμματα.
#### ➖ Μέθοδοι μέτρησης απόστασης μεταξύ clusters:
  - **Single-link (MIN)**: πιο κοντινά σημεία
  - **Complete-link (MAX)**: πιο μακρινά σημεία
  - **Average-link**: μέση απόσταση όλων
  - **Ward’s method**: αύξηση σφάλματος

---

### 6. **DBSCAN**
- Βασίζεται στην πυκνότητα σημείων.
- Κατηγοριοποιεί σε core, border, και noise.
#### ➕ Ικανότητες:
- Ανιχνεύει αυθαίρετα σχήματα.
- Ανθεκτικό σε θόρυβο.
#### ➖ Αδυναμίες:
- Δεν λειτουργεί καλά σε δεδομένα διαφορετικής πυκνότητας.

---

### 7. **OPTICS**
- Παρόμοιο με DBSCAN αλλά πιο ευέλικτο.
- Παράγει reachability plot αντί για τελικά clusters.
- Χρήσιμο σε μεταβαλλόμενη πυκνότητα.

---

### 8. **BIRCH**
- Χρησιμοποιεί δέντρο CF (Clustering Feature) για μεγάλες βάσεις δεδομένων.
- Πολυφασικό clustering (με αρχική σύνοψη + τελικό refinement).
#### ➕ Πολύ αποδοτικό σε μεγάλα σύνολα.
#### ➖ Όχι καλό για υψηλές διαστάσεις.

---

### 9. **Gaussian Mixture Models (GMM)**
- Υποθέτει ότι τα δεδομένα προέρχονται από μείγμα Gaussians.
- Δίνει πιθανότητες συμμετοχής κάθε σημείου σε κάθε cluster (soft clustering).
- Χρησιμοποιεί Expectation-Maximization (EM).
#### ➖ Απαιτεί καθορισμό αριθμού clusters.

---

## 📌 Συμπεράσματα

- **Δεν υπάρχει ένας “καλύτερος” αλγόριθμος**. Η επιλογή εξαρτάται από:
  - Σχήμα και μέγεθος clusters
  - Θόρυβο στα δεδομένα
  - Υπολογιστική πολυπλοκότητα
- Ορισμένοι αλγόριθμοι (π.χ. DBSCAN, Mean Shift) **δεν απαιτούν** αριθμό clusters εκ των προτέρων.
- Άλλοι (π.χ. K-means, GMM) είναι **απλοί αλλά περιορισμένοι**.

---

## 📚 Extra Πηγές

- [GeeksForGeeks - Clustering Guide](https://www.geeksforgeeks.org/)
- [LazyProgrammer ML Compendium](https://lazyprogrammer.me/mlcompendium/)
- [Spectral Clustering Intro](https://towardsdatascience.com/spectral-clustering-for-beginners-d08b7d25b4d8)
- [DBSCAN vs OPTICS](https://www.atlantbh.com/clustering-algorithms-dbscan-vs-optics/)

