# **Εισαγωγή στα Νευρωνικά Δίκτυα**

## **Ενότητα 1: Εισαγωγή και Ιστορική Αναδρομή (Διαφάνειες 1-5)**

*   **Στόχος:** Κατανόηση των βασικών εννοιών των νευρωνικών δικτύων.
*   **Perceptron (Αντίληπτρο):**
    *   Το θεμελιώδες δομικό στοιχείο.
    *   **Ιστορία:**
        *   **1950s (Εποχή του Perceptron):**
            *   1957: The Perceptron (Rosenblatt) – πρώιμο μοντέλο ενός νευρώνα.
            *   1969: Perceptrons (Minsky, Papert) – ανέλυσαν τους περιορισμούς των απλών perceptrons (π.χ. αδυναμία επίλυσης του XOR προβλήματος), οδηγώντας σε μείωση του ενδιαφέροντος (AI winter).
        *   **1980s (Εποχή των Νευρωνικών Δικτύων):**
            *   1986: Backpropagation (Οπισθοδιάδοση) (Hinton) – κρίσιμος αλγόριθμος για την εκπαίδευση πολυεπίπεδων δικτύων, αναζωπυρώνοντας το ενδιαφέρον.
            *   1990s: Εποχή των Γραφικών Μοντέλων.
            *   2000s: Εποχή των Support Vector Machines (SVMs).
        *   **2010s (Εποχή των Βαθιών Δικτύων - Deep Learning):**
            *   **Βαθιά Μάθηση = Γνωστοί Αλγόριθμοι + Υπολογιστική Ισχύς + Μεγάλα Δεδομένα (Big Data)**

## **Ενότητα 2: Το Perceptron Λεπτομερώς (Διαφάνειες 4, 7-28)**

*   **Δομή του Perceptron (Διαφ. 4, 7):**
    *   **Είσοδοι (Inputs - x₁, x₂, ..., xN):** Τα χαρακτηριστικά του δείγματος.
    *   **Βάρη (Weights - w₁, w₂, ..., wN):** Αντιπροσωπεύουν τη σημασία κάθε εισόδου. Ένα βάρος συνδέεται με κάθε είσοδο.
    *   **Σταθμισμένο Άθροισμα (Weighted Sum):** `Σ (wᵢ * xᵢ)`. Μερικές φορές περιλαμβάνει έναν όρο "bias" (πόλωση).
    *   **Συνάρτηση Ενεργοποίησης (Activation Function - f):** Εφαρμόζεται στο σταθμισμένο άθροισμα. Στο κλασικό Perceptron, είναι μια **βηματική συνάρτηση (unit step function / Heaviside step function / sign function)** που δίνει έξοδο 1 αν το άθροισμα υπερβαίνει ένα κατώφλι, αλλιώς 0 (ή -1).
    *   **Έξοδος (Output - y):** Το αποτέλεσμα της συνάρτησης ενεργοποίησης (π.χ. πρόβλεψη κλάσης).

*   **Έμπνευση από τη Βιολογία (Διαφ. 8):**
    *   Τα νευρωνικά δίκτυα/perceptrons είναι **χαλαρά εμπνευσμένα** από τους βιολογικούς νευρώνες.
    *   **ΔΕΝ είναι** ακριβή μοντέλα του πώς λειτουργεί ο εγκέφαλος ή οι νευρώνες.

*   **Αλγόριθμος Εκμάθησης Perceptron (Διαφ. 9-25):**
    1.  **Αρχικοποίηση:** Τα βάρη `w` αρχικοποιούνται (π.χ., σε 0 ή μικρές τυχαίες τιμές).
    2.  **Επανάληψη (για t = 1, ..., T ή μέχρι σύγκλισης):**
        *   **Λήψη εισόδου:** Λαμβάνεται ένα δείγμα εκπαίδευσης `x⁽ᵗ⁾` (δυαδικό διάνυσμα N-διαστάσεων).
        *   **Υπολογισμός πρόβλεψης:** `ŷ⁽ᵗ⁾ = sign(<w⁽ᵗ⁻¹⁾, x⁽ᵗ⁾>)`. Το `sign` του μηδενός θεωρείται +1. (`<a,b>` είναι το εσωτερικό γινόμενο).
        *   **Λήψη αληθινής ετικέτας:** Λαμβάνεται η αληθινή ετικέτα `y⁽ᵗ⁾` (π.χ., από το σύνολο {1, -1}).
        *   **Ενημέρωση βαρών:** Αν `y⁽ᵗ⁾ ≠ ŷ⁽ᵗ⁾` (λάθος πρόβλεψη):
            `wₙ⁽ᵗ⁾ = wₙ⁽ᵗ⁻¹⁾ + y⁽ᵗ⁾ * xₙ⁽ᵗ⁾`
            Αυτό σημαίνει:
            *   Αν η πρόβλεψη ήταν -1 αλλά έπρεπε να είναι +1: αύξησε τα βάρη προς την κατεύθυνση του `x`.
            *   Αν η πρόβλεψη ήταν +1 αλλά έπρεπε να είναι -1: μείωσε τα βάρη (αφαίρεσε το `x`).
        *   Αν η πρόβλεψη είναι σωστή, δεν γίνεται ενημέρωση.
    *   **Γεωμετρική Ερμηνεία:** Το διάνυσμα βαρών `w` ορίζει ένα υπερεπίπεδο. Ο αλγόριθμος προσπαθεί να προσαρμόσει αυτό το υπερεπίπεδο ώστε να διαχωρίζει σωστά τις κλάσεις.

*   **Perceptron με Όρο Πόλωσης (Bias) (Διαφ. 26):**
    *   Το σταθμισμένο άθροισμα γίνεται `Σ (wᵢ * xᵢ) + b`.
    *   Ο όρος πόλωσης `b` επιτρέπει στο υπερεπίπεδο απόφασης να μετατοπιστεί, χωρίς να περνά απαραίτητα από την αρχή των αξόνων.
    *   Μπορεί να υλοποιηθεί προσθέτοντας μια είσοδο `x₀` που είναι πάντα 1, και το αντίστοιχο βάρος `w₀` είναι το `b`.
    *   **Συναρτήσεις Ενεργοποίησης:** Γενικεύεται πέρα από τη βηματική (π.χ., sigmoid, Tanh, ReLU).

*   **Εναλλακτικός Συμβολισμός και "Forward Pass" (Διαφ. 27-28):**
    *   Συχνά, ο κόμβος συνδυάζει την άθροιση και την ενεργοποίηση.
    *   Ο όρος πόλωσης μπορεί να "κρυφτεί" θεωρώντας `xN = 1` και `wN = b`.
    *   **Forward Pass (Προώθηση Εμπρός):** Η διαδικασία υπολογισμού της εξόδου από τις εισόδους.
        *   `a = Σ wᵢxᵢ` (σταθμισμένο άθροισμα, ή `dot(x,w)`)
        *   `y = f(a)` (εφαρμογή συνάρτησης ενεργοποίησης)
        *   Παράδειγμα με sigmoid: `f(a) = 1.0 / (1.0 + exp(-a))` (λογιστική συνάρτηση).

## **Ενότητα 3: Νευρωνικά Δίκτυα (Διαφάνειες 29-58)**

*   **Σύνθεση Νευρώνων (Διαφ. 30-33):**
    *   Συνδυάζοντας πολλαπλά perceptrons (τώρα γενικότερα "νευρώνες" με διάφορες συναρτήσεις ενεργοποίησης) μπορούμε να μοντελοποιήσουμε πολύπλοκες, μη-γραμμικές συναρτήσεις.
    *   Διαφορετικά βάρη οδηγούν σε διαφορετικά σχήματα και θέσεις των συναρτήσεων που μοντελοποιούνται.
    *   Προσθέτοντας επίπεδα (layers) αυξάνεται η πολυπλοκότητα των συναρτήσεων που μπορούν να αναπαρασταθούν.

*   **Δομή Νευρωνικού Δικτύου (Διαφ. 34-41):**
    *   Μια συλλογή από συνδεδεμένους νευρώνες.
    *   Κάθε νευρώνας (εκτός από αυτούς του επιπέδου εισόδου) είναι ουσιαστικά ένα perceptron.
    *   Αριθμός perceptrons στο δίκτυο = άθροισμα νευρώνων στα κρυφά και στο επίπεδο εξόδου.

*   **Ορολογία (Διαφ. 42-46):**
    *   **Επίπεδο Εισόδου (Input Layer):** Λαμβάνει τα αρχικά δεδομένα. Οι κόμβοι εδώ συνήθως δεν εκτελούν υπολογισμούς, απλά μεταδίδουν τις τιμές εισόδου.
    *   **Κρυφό Επίπεδο (Hidden Layer):** Ενδιάμεσα επίπεδα νευρώνων μεταξύ εισόδου και εξόδου. Εδώ γίνονται οι περισσότεροι υπολογισμοί και η εξαγωγή χαρακτηριστικών. Μπορεί να υπάρχουν πολλά κρυφά επίπεδα.
    *   **Επίπεδο Εξόδου (Output Layer):** Παράγει την τελική πρόβλεψη του δικτύου.
    *   **Πλήρως Συνδεδεμένο Επίπεδο (Fully Connected Layer / Dense Layer):** Κάθε νευρώνας ενός επιπέδου συνδέεται με κάθε νευρώνα του προηγούμενου (ή επόμενου) επιπέδου.
    *   **Πολυεπίπεδο Perceptron (Multi-layer Perceptron - MLP):** Ένα νευρωνικό δίκτυο με τουλάχιστον ένα κρυφό επίπεδο.

*   **Παράμετροι προς Εκμάθηση (Διαφ. 47-50):**
    *   **Νευρώνες:** Ο αριθμός των υπολογιστικών μονάδων (συνήθως σε κρυφά και επίπεδο εξόδου).
    *   **Βάρη (Edges):** Κάθε σύνδεση μεταξύ νευρώνων έχει ένα βάρος.
        *   Αριθμός βαρών = (νευρώνες_προηγούμ_επιπέδου * νευρώνες_τρέχοντος_επιπέδου) για κάθε ζεύγος συνδεδεμένων επιπέδων.
    *   **Biases:** Κάθε νευρώνας (στα κρυφά και στο επίπεδο εξόδου) έχει έναν όρο πόλωσης.
    *   **Συνολικές Παράμετροι:** Άθροισμα όλων των βαρών και όλων των biases. Αυτές είναι οι τιμές που το δίκτυο "μαθαίνει" κατά την εκπαίδευση.

*   **Βάθος Δικτύου (Διαφ. 51):**
    *   Παραδοσιακά, η απόδοση έφτανε σε οροφή με 2-3 επίπεδα.
    *   Τα βαθύτερα δίκτυα είχαν προβλήματα εκπαίδευσης και γενίκευσης.
    *   Οι σύγχρονες **αρχιτεκτονικές βαθιών νευρωνικών δικτύων (deep neural networks)** έχουν ξεπεράσει πολλά από αυτά τα προβλήματα.

*   **Νευρωνικά Δίκτυα σε Δράση (Διαφ. 52-58):**
    *   Οπτική επίδειξη του πώς η αύξηση του αριθμού των νευρώνων σε ένα κρυφό επίπεδο ή η προσθήκη κρυφών επιπέδων βελτιώνει την ικανότητα του δικτύου να προσεγγίσει μια συνάρτηση (π.χ., `xsin(x)`).

*   **Θεώρημα Καθολικής Προσέγγισης (Universal Approximation Theorem) (Διαφ. 59):**
    *   **Hornik et al. (1989):** Ένα νευρωνικό δίκτυο με **ένα μόνο κρυφό επίπεδο** και μια γραμμική μονάδα εξόδου μπορεί να προσεγγίσει οποιαδήποτε συνεχή συνάρτηση αυθαίρετα καλά, δεδομένου ότι υπάρχουν αρκετές κρυφές μονάδες.
    *   Αρχικά αποδείχθηκε για τη σιγμοειδή συνάρτηση (Cybenko, 1989), αλλά ισχύει και για πολλές άλλες συναρτήσεις ενεργοποίησης (π.χ., tanh).
    *   **Προσοχή (Caveat):** Μπορεί να απαιτούνται **εκθετικά πολλές** κρυφές μονάδες, καθιστώντας το πρακτικά μη εφικτό σε ορισμένες περιπτώσεις. Τα βαθιά δίκτυα συχνά προσφέρουν πιο αποδοτική αναπαράσταση.

## **Ενότητα 4: Συναρτήσεις Ενεργοποίησης (Διαφ. 60-70)**

*   **McCulloch and Pitts (1943):** Το πρώτο υπολογιστικό μοντέλο ενός νευρώνα, βασισμένο σε λογική κατωφλίου (threshold logic).
*   **Ρόλος της Συνάρτησης Ενεργοποίησης (Διαφ. 62-63):**
    *   Μια μαθηματική συνάρτηση που εφαρμόζεται στην έξοδο κάθε νευρώνα.
    *   Εισάγει **μη-γραμμικότητα** στο μοντέλο. Χωρίς μη-γραμμικές συναρτήσεις ενεργοποίησης, ένα πολυεπίπεδο δίκτυο θα κατέρρεε σε ένα ισοδύναμο μονοεπίπεδο γραμμικό μοντέλο.
    *   Συνήθως εφαρμόζεται ξεχωριστά σε κάθε νευρώνα.

*   **Τυπικές Επιλογές (Διαφ. 63-69):**
    *   **Linear (Γραμμική):** `g(z) = z`
        *   Χωρίς "συνθλιπτική" (squashing) δράση.
        *   Η σύνθεση επιπέδων γραμμικών μονάδων είναι ισοδύναμη με ένα μόνο επίπεδο γραμμικών μονάδων (δεν προστίθεται εκφραστική ισχύς).
        *   Χρήσιμη για γραμμική προβολή της εισόδου σε χαμηλότερη διάσταση ή στο επίπεδο εξόδου για προβλήματα παλινδρόμησης.
    *   **Sigmoid (Σιγμοειδής):** `g(z) = 1 / (1 + e⁻ᶻ)`
        *   "Συνθλίβει" την έξοδο του νευρώνα στο διάστημα [0, 1].
        *   Η έξοδος μπορεί να ερμηνευθεί ως πιθανότητα.
        *   Θετική, οριοθετημένη, αυστηρά αύξουσα.
        *   Μπορεί να οδηγήσει στο πρόβλημα των "εξαφανιζόμενων κλίσεων" (vanishing gradients) κατά την οπισθοδιάδοση, ειδικά σε βαθιά δίκτυα.
    *   **Tanh (Υπερβολική Εφαπτομένη):** `g(z) = (eᶻ - e⁻ᶻ) / (eᶻ + e⁻ᶻ)`
        *   "Συνθλίβει" την έξοδο στο διάστημα [-1, 1].
        *   Σχετίζεται με τη sigmoid: `tanh(z) = 2 * sigmoid(2z) - 1`.
        *   Μπορεί να είναι θετική ή αρνητική, οριοθετημένη, αυστηρά αύξουσα.
        *   Συχνά προτιμάται από τη sigmoid σε κρυφά επίπεδα γιατί είναι μηδενικο-κεντρική, βοηθώντας τη σύγκλιση. Επίσης υποφέρει από vanishing gradients.
    *   **ReLU (Rectified Linear Unit):** `g(z) = max(0, z)`
        *   Λιγότερο επιρρεπής σε vanishing gradients (για z > 0).
        *   Επέτρεψε την εκπαίδευση πολύ βαθιών δικτύων χωρίς μη επιβλεπόμενη προ-εκπαίδευση.
        *   Μη-αρνητική, αύξουσα, αλλά **όχι οριοθετημένη άνωθεν**.
        *   **Μη διαφορίσιμη στο 0** (στην πράξη, η κλίση στο 0 μπορεί να τεθεί 0 ή 1).
        *   Οδηγεί σε **αραιές ενεργοποιήσεις (sparse activations)**, δηλαδή πολλοί νευρώνες έχουν έξοδο 0, κάτι που μπορεί να είναι υπολογιστικά αποδοτικό και βιολογικά πιο εύλογο.
        *   **Πρόβλημα "Dying ReLU" (Διαφ. 68):** Αν ένας νευρώνας ReLU πάρει μια μεγάλη αρνητική είσοδο, η κλίση του γίνεται 0. Αυτό σημαίνει ότι ο νευρώνας σταματά να μαθαίνει και μπορεί να "πεθάνει" μόνιμα, μην ανταποκρινόμενος σε καμία είσοδο.
    *   **Leaky ReLU (Διαφ. 69):** `g(z) = max(αz, z)` όπου `α` είναι μια μικρή θετική σταθερά (π.χ., 0.01).
        *   Προσπάθεια επίλυσης του "dying ReLU" επιτρέποντας μια μικρή, μη μηδενική κλίση για αρνητικές εισόδους.
    *   **Άλλες (Διαφ. 60, 63):** Softmax (συνήθως στο τελευταίο επίπεδο για πολυклаσική κατηγοριοποίηση), Sparsemax, Max-pooling (λειτουργεί σε ομάδες νευρώνων).

*   **Δυνατότητες Βαθύτερων Δικτύων (Διαφ. 70):**
    *   **Θεώρημα (Montufar et al. 2014):** Βαθύτερα δίκτυα (περισσότερα κρυφά επίπεδα) μπορούν να παρέχουν πιο συμπαγείς προσεγγίσεις. Ο αριθμός των γραμμικών περιοχών που χαράσσονται από ένα βαθύ νευρωνικό δίκτυο με ReLU ενεργοποιήσεις αυξάνεται εκθετικά με το βάθος.
    *   Για σταθερό αριθμό παραμέτρων, τα βαθύτερα δίκτυα είναι εκθετικά πιο εκφραστικά.

## **Ενότητα 5: Γιατί Περισσότερα Επίπεδα; Αναπαράσταση και Ιεραρχία (Διαφ. 71-73)**

*   **Η Αναπαράσταση Έχει Σημασία (Representation Matters) (Διαφ. 71):**
    *   Διαφορετικές αναπαραστάσεις των δεδομένων μπορούν να κάνουν ένα πρόβλημα πιο εύκολο ή πιο δύσκολο. (π.χ., Καρτεσιανές vs. Πολικές συντεταγμένες για διαχωρισμό δεδομένων).
    *   Τα βαθιά δίκτυα μαθαίνουν ιεραρχικές αναπαραστάσεις των δεδομένων, όπου κάθε επίπεδο μαθαίνει πιο σύνθετα χαρακτηριστικά βασισμένα στα χαρακτηριστικά του προηγούμενου επιπέδου.

*   **Εκμάθηση Πολλαπλών Στοιχείων / Βάθος = Επαναλαμβανόμενες Συνθέσεις (Διαφ. 72-73):**
    *   **Rule-based systems:** Χειροποίητο πρόγραμμα.
    *   **Classic machine learning:** Χειροποίητα χαρακτηριστικά (features) + αλγόριθμος μάθησης.
    *   **Representation learning:** Το σύστημα μαθαίνει τα χαρακτηριστικά.
    *   **Deep learning:** Το σύστημα μαθαίνει πολλαπλά επίπεδα χαρακτηριστικών (από απλά σε πιο αφηρημένα).
    *   **Παράδειγμα (Διαφ. 73):**
        *   Επίπεδο εισόδου: pixels.
        *   1ο κρυφό επίπεδο: ακμές (edges).
        *   2ο κρυφό επίπεδο: γωνίες και περιγράμματα (corners, contours).
        *   3ο κρυφό επίπεδο: μέρη αντικειμένων (object parts).
        *   Επίπεδο εξόδου: ταυτότητα αντικειμένου (π.χ., CAR, PERSON, ANIMAL).

## **Ενότητα 6: Εκπαίδευση Μοντέλου - Συναρτήσεις Απώλειας (Διαφ. 74-81)**

*   **Τι είναι η Συνάρτηση Απώλειας (Loss Function); (Διαφ. 75):**
    *   Μια μέθοδος αξιολόγησης του πόσο καλά το μοντέλο σας μοντελοποιεί το σύνολο δεδομένων σας.
    *   Μια μαθηματική συνάρτηση των παραμέτρων του αλγορίθμου μηχανικής μάθησης.
    *   **"Δεν μπορείς να βελτιώσεις αυτό που δεν μπορείς να μετρήσεις" [Peter Drucker].**

*   **Τύποι Συναρτήσεων Απώλειας (Διαφ. 76-77):**
    *   **Παλινδρόμηση (Regression):**
        *   MSE (Mean Squared Error)
        *   MAE (Mean Absolute Error)
        *   Huber loss
    *   **Κατηγοριοποίηση (Classification):**
        *   Binary cross-entropy
        *   Categorical cross-entropy
    *   **Άλλες:** KL Divergence (AutoEncoders), Discriminator loss/Minmax GAN loss (GANs), Focal loss (Object detection), Triplet loss (Word embeddings).

*   **Συναρτήσεις Απώλειας για Παλινδρόμηση (Διαφ. 78-79):**
    *   **Mean Squared Error (MSE) / L2 loss (Μέσο Τετραγωνικό Σφάλμα):**
        *   `MSE = (1/n) * Σ (yᵢ - ŷᵢ)²`
        *   **Πλεονεκτήματα:** Εύκολη ερμηνεία, πάντα διαφορίσιμη, ένα μόνο τοπικό ελάχιστο (για γραμμική παλινδρόμηση).
        *   **Μειονεκτήματα:** Η μονάδα σφάλματος είναι στο τετράγωνο (δύσκολη κατανόηση), **όχι ανθεκτική σε ακραίες τιμές (outliers)**.
    *   **Mean Absolute Error (MAE) / L1 loss (Μέσο Απόλυτο Σφάλμα):**
        *   `MAE = (1/n) * Σ |yᵢ - ŷᵢ|`
        *   **Πλεονεκτήματα:** Διαισθητική και εύκολη, η μονάδα σφάλματος είναι ίδια με την έξοδο, **ανθεκτική σε ακραίες τιμές**.
        *   **Μειονεκτήματα:** **Όχι διαφορίσιμη παντού** (στο σημείο όπου yᵢ - ŷᵢ = 0).

*   **Συναρτήσεις Απώλειας για Κατηγοριοποίηση (Διαφ. 80-81):**
    *   **Binary Cross-Entropy / Log loss (Δυαδική Διασταυρούμενη Εντροπία):**
        *   Χρησιμοποιείται για προβλήματα δυαδικής κατηγοριοποίησης (0 ή 1).
        *   `Log Loss = -(1/n) * Σ [yᵢ log(ŷᵢ) + (1-yᵢ) log(1-ŷᵢ)]`
            *   `yᵢ`: αληθινή ετικέτα (0 ή 1)
            *   `ŷᵢ`: προβλεπόμενη πιθανότητα για την κλάση 1
        *   **Πλεονεκτήματα:** Διαφορίσιμη, ερμηνεύσιμη (χαμηλότερη τιμή = καλύτερη πρόβλεψη).
        *   **Μειονεκτήματα:** Ευαίσθητη σε ακραίες τιμές (πολύ σίγουρες λανθασμένες προβλέψεις τιμωρούνται αυστηρά).
    *   **Categorical Cross-Entropy Loss (Κατηγορική Διασταυρούμενη Εντροπία):**
        *   Χρησιμοποιείται όταν ο αριθμός των κλάσεων είναι > 2.
        *   `CCE Loss = -(1/n) * Σᵢ Σₖ yᵢ,ₖ log(ŷᵢ,ₖ)`
            *   `n`: αριθμός δειγμάτων
            *   `K`: αριθμός κλάσεων
            *   `yᵢ,ₖ`: μεταβλητή δείκτης (0 ή 1) αν το δείγμα `i` ανήκει στην κλάση `k`.
            *   `ŷᵢ,ₖ`: προβλεπόμενη πιθανότητα το δείγμα `i` να ανήκει στην κλάση `k`.
        *   Υπολογίζει τον μέσο αρνητικό λογάριθμο της πιθανότητας για όλες τις κλάσεις και τα δείγματα.
        *   Τιμωρεί περισσότερο τις λανθασμένες προβλέψεις που γίνονται με μεγάλη σιγουριά.

## **Ενότητα 7: Διάγνωση Συμπεριφοράς Μοντέλου - Καμπύλες Μάθησης (Διαφ. 82-93)**

*   **Καμπύλες Απώλειας (Loss Curves) (Διαφ. 82):**
    *   Γραφική παράσταση της συνάρτησης απώλειας (και/ή άλλων μετρικών όπως η ακρίβεια) ως προς τις εποχές εκπαίδευσης (epochs).
    *   Συνήθως σχεδιάζονται δύο καμπύλες: μία για το σύνολο εκπαίδευσης (training set) και μία για το σύνολο επικύρωσης (validation set).

*   **Τρεις Κοινές Δυναμικές (Διαφ. 83):**
    1.  **Underfit (Υποπροσαρμογή)**
    2.  **Overfit (Υπερπροσαρμογή)**
    3.  **Good Fit (Καλή Προσαρμογή)**
    *   Υποθέτουμε ότι ελαχιστοποιούμε μια μετρική (μικρότερη τιμή = καλύτερη μάθηση).

*   **Underfit Learning Curves (Καμπύλες Υποπροσαρμογής) (Διαφ. 84-85):**
    *   **Προειδοποίηση 1 (Διαφ. 84):** Το μοντέλο δεν έχει επαρκή "χωρητικότητα" (capacity) για την πολυπλοκότητα του προβλήματος. Η απώλεια είναι υψηλή τόσο στο σύνολο εκπαίδευσης όσο και στο σύνολο επικύρωσης, και οι καμπύλες είναι επίπεδες ή συγκλίνουν σε υψηλή τιμή.
    *   **Προειδοποίηση 2 (Διαφ. 85):** Το μοντέλο είναι ικανό για περαιτέρω μάθηση, αλλά η εκπαίδευση σταμάτησε πρόωρα. Οι καμπύλες απώλειας (τόσο εκπαίδευσης όσο και επικύρωσης) εξακολουθούν να μειώνονται στο τέλος της εκπαίδευσης.

*   **A Good Fitting Curve (Καμπύλη Καλής Προσαρμογής) (Διαφ. 86-87):**
    *   Η απώλεια εκπαίδευσης και επικύρωσης μειώνονται και σταθεροποιούνται.
    *   Υπάρχει ένα **μικρό κενό (generalization gap)** μεταξύ της τελικής απώλειας εκπαίδευσης και επικύρωσης. Η απώλεια εκπαίδευσης είναι σχεδόν πάντα χαμηλότερη.
    *   **Προειδοποίηση (Διαφ. 87):** Η συνέχιση της εκπαίδευσης ενός καλά προσαρμοσμένου μοντέλου πιθανότατα θα οδηγήσει σε υπερπροσαρμογή.

*   **Overfit Learning Curves (Καμπύλες Υπερπροσαρμογής) (Διαφ. 88-89):**
    *   Το μοντέλο έχει μάθει "πολύ καλά" τα δεδομένα εκπαίδευσης, συμπεριλαμβανομένου του θορύβου.
    *   Η απώλεια εκπαίδευσης συνεχίζει να μειώνεται.
    *   Η απώλεια επικύρωσης αρχίζει να **αυξάνεται** μετά από ένα σημείο. Αυτό το σημείο είναι ιδανικό για να σταματήσει η εκπαίδευση (early stopping).
    *   Συχνά συμβαίνει όταν το μοντέλο έχει υπερβολική χωρητικότητα ή εκπαιδεύεται για πάρα πολλές εποχές.

*   **Unrepresentative Train Dataset (Μη Αντιπροσωπευτικό Σύνολο Εκπαίδευσης) (Διαφ. 90):**
    *   Η απώλεια εκπαίδευσης είναι υψηλότερη από την απώλεια επικύρωσης ή οι καμπύλες είναι ασταθείς.
    *   Πιθανή εξήγηση: Το σύνολο εκπαίδευσης έχει πολύ λίγα παραδείγματα ή δεν παρέχει επαρκείς πληροφορίες για την εκμάθηση του προβλήματος, σε σύγκριση με το σύνολο επικύρωσης.

*   **Unrepresentative Validation Dataset (Μη Αντιπροσωπευτικό Σύνολο Επικύρωσης) (Διαφ. 91-92):**
    *   **Εικόνα 1 (Διαφ. 91):** Η καμπύλη απώλειας επικύρωσης είναι πολύ ασταθής (θορυβώδης). Πιθανή εξήγηση: Το σύνολο επικύρωσης έχει πολύ λίγα παραδείγματα για να δώσει μια αξιόπιστη εκτίμηση της γενίκευσης.
    *   **Εικόνα 2 (Διαφ. 92):** Η απώλεια επικύρωσης είναι σταθερά χαμηλότερη από την απώλεια εκπαίδευσης. Πιθανή εξήγηση: Το σύνολο επικύρωσης είναι "ευκολότερο" για το μοντέλο από ό,τι το σύνολο εκπαίδευσης (π.χ., περιέχει λιγότερο θόρυβο ή πιο απλά παραδείγματα).

*   **Σχετικά με τη Γενίκευση (About Generalization) (Διαφ. 93):**
    *   Η καμπύλη "U-shape" (ή "J-shape"): Καθώς η πολυπλοκότητα του μοντέλου αυξάνεται, το σφάλμα εκπαίδευσης (Training Error) μειώνεται. Το σφάλμα πρόβλεψης σε νέα δεδομένα (Prediction Error for New Data / Generalization Error) αρχικά μειώνεται, φτάνει σε ένα ελάχιστο, και μετά αρχίζει να αυξάνεται (σημείο υπερπροσαρμογής).
    *   **Προειδοποίηση: Δεν θέλουμε να προσαρμόσουμε τέλεια τα δεδομένα εκπαίδευσης.**

## **Ενότητα 8: Μελέτη Περίπτωσης - The Wine Dataset (Διαφ. 94-108)**

*   **Το Σύνολο Δεδομένων "Wine" (Διαφ. 95):**
    *   Κλάσεις: 3
    *   Συνολικά Χαρακτηριστικά: 13
    *   Συνολικά Δείγματα: 178
    *   Δείγματα ανά κλάση: [59, 71, 48]
    *   Τύπος Χαρακτηριστικών: Θετικοί και πραγματικοί αριθμοί.

*   **Οπτικοποιήσεις Χαρακτηριστικών (Διαφ. 96-101):**
    *   Ιστογράμματα συχνότητας για κάθε χαρακτηριστικό, διαχωρισμένα ανά κλάση.
    *   Ορισμένα χαρακτηριστικά δείχνουν σημαντική επικάλυψη μεταξύ των κλάσεων (π.χ., "alcohol", "malic_acid").
    *   Άλλα χαρακτηριστικά δείχνουν κάποια διαχωριστική ικανότητα (π.χ., "flavanoids" - μικρή επικάλυψη, "color_intensity", "proline" - έχουν κάποια δυναμική).

*   **Πιο Προχωρημένα Γραφήματα (Διαφ. 102):**
    *   Pair plots για τα πιο υποσχόμενα χαρακτηριστικά (π.χ., Total phenols, Flavonoids, Color intensity).
    *   Φαίνεται ότι σχηματίζονται σχετικά εύκολα διαχωρίσιμες περιοχές όταν εξετάζουμε ζεύγη χαρακτηριστικών.

*   **Δοκιμή 4 Διαφορετικών Αρχιτεκτονικών Feed Forward Δικτύων (Διαφ. 103):**
    *   Όλες χρησιμοποιούν 12 εισόδους (χαρακτηριστικά) και 3 εξόδους (κλάσεις).
    1.  **Αρχιτεκτονική 1:** Είσοδος(12) -> 1ο κρυφό(32) -> Έξοδος(3) [**sigmoid** ενεργοποίηση στα κρυφά]
    2.  **Αρχιτεκτονική 2:** Είσοδος(12) -> 1ο κρυφό(32) -> Έξοδος(3) [**ReLU** ενεργοποίηση στα κρυφά]
    3.  **Αρχιτεκτονική 3:** Είσοδος(12) -> 1ο κρυφό(32) -> 2ο κρυφό(32) -> Έξοδος(3) [**Sigmoid** ενεργοποίηση στα κρυφά]
    4.  **Αρχιτεκτονική 4:** Είσοδος(12) -> 1ο κρυφό(32) -> 2ο κρυφό(32) -> Έξοδος(3) [**ReLU** ενεργοποίηση στα κρυφά]

*   **Αποτελέσματα Αρχιτεκτονικών (Καμπύλες Απώλειας & Ακρίβειας) (Διαφ. 104-107):**
    *   **Αρχιτεκτονική 1 (Sigmoid, 1 κρυφό):** Η ακρίβεια επικύρωσης είναι ασταθής και χαμηλότερη από την ακρίβεια εκπαίδευσης. Γενικά, αργή σύγκλιση.
    *   **Αρχιτεκτονική 2 (ReLU, 1 κρυφό):** Πιο γρήγορη σύγκλιση, καλύτερη και πιο σταθερή ακρίβεια επικύρωσης σε σύγκριση με τη Sigmoid.
    *   **Αρχιτεκτονική 3 (Sigmoid, 2 κρυφά):** Παρόμοια ή ελαφρώς χειρότερη από την Αρχιτεκτονική 1. Η προσθήκη βάθους με Sigmoid δεν φαίνεται να βοηθά ιδιαίτερα εδώ.
    *   **Αρχιτεκτονική 4 (ReLU, 2 κρυφά):** Η καλύτερη απόδοση. Γρήγορη σύγκλιση, υψηλή και σταθερή ακρίβεια επικύρωσης που πλησιάζει την ακρίβεια εκπαίδευσης.

*   **Συμπεράσματα από τις Καμπύλες Απώλειας (Διαφ. 108):**
    *   Η **ReLU** φαίνεται να είναι καλύτερη εναλλακτική της Sigmoid (τουλάχιστον για αυτό το πρόβλημα).
    *   Χρειαζόμαστε **περισσότερες εποχές εκπαίδευσης** (ειδικά για την Αρχιτεκτονική 4, όπου οι καμπύλες δεν έχουν πλήρως ισοπεδωθεί).
    *   Η αρχιτεκτονική που βασίζεται σε **πολλαπλά επίπεδα και ReLU ενεργοποιήσεις** αποδίδει καλύτερα.

## **Ενότητα 9: Πρόσθετοι Πόροι (Διαφ. 109)**

*   **TensorFlow Playground (playground.tensorflow.org):** Ένα εξαιρετικό διαδραστικό εργαλείο για να πειραματιστείτε με απλά νευρωνικά δίκτυα, να αλλάξετε παραμέτρους (ρυθμό μάθησης, συνάρτηση ενεργοποίησης, αριθμό κρυφών επιπέδων/νευρώνων, δεδομένα εισόδου) και να δείτε άμεσα την επίδραση στην απόφαση του δικτύου και στις καμπύλες μάθησης.

[playground.tensorflow.org](https://playground.tensorflow.org)


